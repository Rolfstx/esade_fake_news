WEBVTT
Kind: captions
Language: en

00:00:16.630 --> 00:00:17.930
&gt;&gt; NARRATOR: Tonight--

00:00:17.930 --> 00:00:20.470
&gt;&gt; The race to become an A.I.
superpower is on...

00:00:20.470 --> 00:00:22.670
&gt;&gt; NARRATOR: The politics of
artificial intelligence...

00:00:22.670 --> 00:00:24.530
&gt;&gt; There will be
a Chinese tech sector

00:00:24.530 --> 00:00:26.200
and there will be
a American tech sector.

00:00:26.200 --> 00:00:27.730
&gt;&gt; NARRATOR: The new tech war.

00:00:27.730 --> 00:00:30.200
&gt;&gt; The more data,
the better the A.I. works.

00:00:30.200 --> 00:00:33.930
So in the age of A.I.,
where data is the new oil,

00:00:33.930 --> 00:00:36.170
China is the new Saudi Arabia.

00:00:36.170 --> 00:00:37.930
&gt;&gt; NARRATOR:
The future of work...

00:00:37.930 --> 00:00:40.100
&gt;&gt; When I increase productivity
through automation,

00:00:40.100 --> 00:00:42.070
jobs go away.

00:00:42.070 --> 00:00:46.100
&gt;&gt; I believe about 50% of jobs
will be somewhat

00:00:46.100 --> 00:00:50.000
or extremely threatened by A.I.
in the next 15 years or so.

00:00:50.000 --> 00:00:52.630
&gt;&gt; NARRATOR: A.I. and corporate
surveillance...

00:00:52.630 --> 00:00:55.500
&gt;&gt; We thought that we were
searching Google.

00:00:55.500 --> 00:00:57.930
We had no idea that Google
was searching us.

00:00:57.930 --> 00:01:00.170
&gt;&gt; NARRATOR: And the threat
to democracy.

00:01:00.170 --> 00:01:02.600
&gt;&gt; China is on its way
to building

00:01:02.600 --> 00:01:04.070
a total surveillance state.

00:01:04.070 --> 00:01:06.130
&gt;&gt; NARRATOR: Tonight on
"Frontline"...

00:01:06.130 --> 00:01:09.300
&gt;&gt; It has pervaded so many
elements of everyday life.

00:01:09.300 --> 00:01:11.930
How do we make it transparent
and accountable?

00:01:11.930 --> 00:01:13.770
&gt;&gt; NARRATOR:
..."In the Age of A.I."

00:01:16.530 --> 00:01:21.200
♪ ♪

00:01:34.900 --> 00:01:37.970
♪ ♪

00:01:42.700 --> 00:01:46.330
&gt;&gt; NARRATOR: This is the world's
most complex board game.

00:01:48.070 --> 00:01:51.530
There are more possible moves
in the game of Go

00:01:51.530 --> 00:01:55.800
than there are atoms
in the universe.

00:01:55.800 --> 00:02:01.700
Legend has it that in 2300 BCE,
Emperor Yao devised it

00:02:01.700 --> 00:02:08.000
to teach his son discipline,
concentration, and balance.

00:02:08.000 --> 00:02:12.570
And, over 4,000 years later,
this ancient Chinese game

00:02:12.570 --> 00:02:17.370
would signal the start
of a new industrial age.

00:02:17.370 --> 00:02:18.430
♪ ♪

00:02:25.930 --> 00:02:31.400
It was 2016, in Seoul,
South Korea.

00:02:31.400 --> 00:02:35.170
&gt;&gt; Can machines overtake
human intelligence?

00:02:35.170 --> 00:02:37.730
A breakthrough moment when the
world champion

00:02:37.730 --> 00:02:40.630
of the Asian board game Go
takes on an A.I. program

00:02:40.630 --> 00:02:42.530
developed by Google.

00:02:42.530 --> 00:02:49.430
&gt;&gt; (speaking Korean):

00:02:55.300 --> 00:02:56.770
&gt;&gt; In countries where
it's very popular,

00:02:56.770 --> 00:03:00.570
like China and Japan and,
and South Korea, to them,

00:03:00.570 --> 00:03:02.100
Go is not just a game, right?

00:03:02.100 --> 00:03:04.000
It's, like, how you learn
strategy.

00:03:04.000 --> 00:03:07.500
It has an almost spiritual
component.

00:03:07.500 --> 00:03:09.400
You know, if you talk
to South Koreans, right,

00:03:09.400 --> 00:03:11.500
and Lee Sedol is the world's
greatest Go player,

00:03:11.500 --> 00:03:13.730
he's a national hero
in South Korea.

00:03:13.730 --> 00:03:18.630
They were sure that Lee Sedol
would beat AlphaGo hands down.

00:03:18.630 --> 00:03:23.030
♪ ♪

00:03:23.030 --> 00:03:26.130
&gt;&gt; NARRATOR: Google's AlphaGo
was a computer program that,

00:03:26.130 --> 00:03:28.870
starting with the rules of Go

00:03:28.870 --> 00:03:31.330
and a database
of historical games,

00:03:31.330 --> 00:03:34.630
had been designed
to teach itself.

00:03:34.630 --> 00:03:38.700
&gt;&gt; I was one of the commentators
at the Lee Sedol games.

00:03:38.700 --> 00:03:42.700
And yes, it was watched by tens
of millions of people.

00:03:42.700 --> 00:03:44.300
(man speaking Korean)

00:03:44.300 --> 00:03:46.630
&gt;&gt; NARRATOR: Throughout
Southeast Asia,

00:03:46.630 --> 00:03:48.400
this was seen as
a sports spectacle

00:03:48.400 --> 00:03:49.800
with national pride at stake.

00:03:49.800 --> 00:03:51.030
&gt;&gt; Wow, that was a player guess.

00:03:51.030 --> 00:03:53.530
&gt;&gt; NARRATOR: But much more
was in play.

00:03:53.530 --> 00:03:55.730
This was the public unveiling

00:03:55.730 --> 00:03:57.830
of a form of artificial
intelligence

00:03:57.830 --> 00:04:00.400
called deep learning,

00:04:00.400 --> 00:04:03.400
that mimics the neural networks
of the human brain.

00:04:03.400 --> 00:04:05.430
&gt;&gt; So what happens with machine
learning,

00:04:05.430 --> 00:04:08.300
or artificial intelligence--
initially with AlphaGo--

00:04:08.300 --> 00:04:12.130
is that the machine is fed
all kinds of Go games,

00:04:12.130 --> 00:04:15.530
and then it studies them,
learns from them,

00:04:15.530 --> 00:04:17.830
and figures out its own moves.

00:04:17.830 --> 00:04:19.700
And because it's an A.I.
system--

00:04:19.700 --> 00:04:21.570
it's not just following
instructions,

00:04:21.570 --> 00:04:23.930
it's figuring out its own
instructions--

00:04:23.930 --> 00:04:26.930
it comes up with moves that
humans hadn't thought of before.

00:04:26.930 --> 00:04:31.130
So, it studies games that humans
have played, it knows the rules,

00:04:31.130 --> 00:04:36.130
and then it comes up
with creative moves.

00:04:36.130 --> 00:04:38.030
(woman speaking Korean)

00:04:39.600 --> 00:04:42.370
(speaking Korean):

00:04:42.370 --> 00:04:44.800
&gt;&gt; That's a very...
that's a very surprising move.

00:04:44.800 --> 00:04:47.670
&gt;&gt; I thought it was a mistake.

00:04:47.670 --> 00:04:51.470
&gt;&gt; NARRATOR: Game two, move 37.

00:04:51.470 --> 00:04:54.200
&gt;&gt; That move 37 was a move that
humans could not fathom,

00:04:54.200 --> 00:04:57.070
but yet it ended up being
brilliant

00:04:57.070 --> 00:05:00.470
and woke people up to say,

00:05:00.470 --> 00:05:03.100
"Wow, after thousands
of years of playing,

00:05:03.100 --> 00:05:06.330
we never thought about making
a move like that."

00:05:06.330 --> 00:05:08.370
&gt;&gt; Oh, he resigned.

00:05:08.370 --> 00:05:12.300
It looks like... Lee Sedol has
just resigned, actually.

00:05:12.300 --> 00:05:13.830
&gt;&gt; Yeah!
&gt;&gt; Yes.

00:05:13.830 --> 00:05:15.530
&gt;&gt; NARRATOR: In the end, the
scientists watched

00:05:15.530 --> 00:05:18.200
their algorithms win four
of the games.

00:05:18.200 --> 00:05:20.470
Lee Sedol took one.

00:05:20.470 --> 00:05:22.330
&gt;&gt; What happened with Go,
first and foremost,

00:05:22.330 --> 00:05:25.830
was a huge victory for deep mind
and for A.I., right?

00:05:25.830 --> 00:05:28.170
It wasn't that the computers
beat the humans,

00:05:28.170 --> 00:05:31.970
it was that, you know, one type
of intelligence beat another.

00:05:31.970 --> 00:05:34.230
&gt;&gt; NARRATOR: Artificial
intelligence had proven

00:05:34.230 --> 00:05:36.770
it could marshal a vast amount
of data,

00:05:36.770 --> 00:05:40.300
beyond anything any human
could handle,

00:05:40.300 --> 00:05:44.400
and use it to teach itself how
to predict an outcome.

00:05:44.400 --> 00:05:48.400
The commercial implications
were enormous.

00:05:48.400 --> 00:05:51.670
&gt;&gt; While AlphaGo is a,
is a toy game,

00:05:51.670 --> 00:05:57.530
but its success and its waking
everyone up, I think,

00:05:57.530 --> 00:06:03.770
is, is going to be remembered
as the pivotal moment

00:06:03.770 --> 00:06:07.230
where A.I. became mature

00:06:07.230 --> 00:06:09.100
and everybody jumped
on the bandwagon.

00:06:09.100 --> 00:06:10.570
♪ ♪

00:06:10.570 --> 00:06:14.270
&gt;&gt; NARRATOR: This is about the
consequences of that defeat.

00:06:14.270 --> 00:06:16.270
(man speaking local language)

00:06:16.270 --> 00:06:19.870
How the A.I. algorithms are
ushering in a new age

00:06:19.870 --> 00:06:24.200
of great potential and
prosperity,

00:06:24.200 --> 00:06:29.170
but an age that will also deepen
inequality, challenge democracy,

00:06:29.170 --> 00:06:35.200
and divide the world
into two A.I. superpowers.

00:06:35.200 --> 00:06:39.130
Tonight, five stories about how
artificial intelligence

00:06:39.130 --> 00:06:40.930
is changing our world.

00:06:40.930 --> 00:06:43.930
♪ ♪

00:06:51.800 --> 00:06:56.330
China has decided to chase
the A.I. future.

00:06:56.330 --> 00:06:58.770
&gt;&gt; The difference between
the internet mindset

00:06:58.770 --> 00:07:00.830
and the A.I. mindset...

00:07:00.830 --> 00:07:04.700
&gt;&gt; NARRATOR: A future made and
embraced by a new generation.

00:07:07.000 --> 00:07:10.770
&gt;&gt; Well, it's hard not to feel
the kind of immense energy,

00:07:10.770 --> 00:07:15.570
and also the obvious fact
of the demographics.

00:07:15.570 --> 00:07:18.770
They're mostly very younger
people,

00:07:18.770 --> 00:07:22.830
so that this clearly is
technology which is being

00:07:22.830 --> 00:07:26.030
generated by a whole new
generation.

00:07:26.030 --> 00:07:27.800
&gt;&gt; NARRATOR: Orville Schell
is one of

00:07:27.800 --> 00:07:30.100
America's foremost
China scholars.

00:07:30.100 --> 00:07:31.730
&gt;&gt; (speaking Mandarin)

00:07:31.730 --> 00:07:34.830
&gt;&gt; NARRATOR: He first came here
45 years ago.

00:07:34.830 --> 00:07:38.270
&gt;&gt; When I, when I first came
here, in 1975,

00:07:38.270 --> 00:07:40.770
Chairman Mao was still alive,

00:07:40.770 --> 00:07:43.300
the Cultural Revolution
was coming on,

00:07:43.300 --> 00:07:47.830
and there wasn't a single whiff
of anything

00:07:47.830 --> 00:07:49.170
of what you see here.

00:07:49.170 --> 00:07:50.770
It was unimaginable.

00:07:50.770 --> 00:07:54.500
In fact, in those years,
one very much thought,

00:07:54.500 --> 00:08:00.530
"This is the way China is, this
is the way it's going to be."

00:08:00.530 --> 00:08:02.570
And the fact that it has gone
through

00:08:02.570 --> 00:08:06.330
so many different changes since
is quite extraordinary.

00:08:06.330 --> 00:08:08.270
(man giving instructions)

00:08:08.270 --> 00:08:11.770
&gt;&gt; NARRATOR: This extraordinary
progress goes back

00:08:11.770 --> 00:08:14.370
to that game of Go.

00:08:14.370 --> 00:08:16.830
&gt;&gt; I think that the government
recognized

00:08:16.830 --> 00:08:18.300
that this was a sort of critical
thing for the future,

00:08:18.300 --> 00:08:20.270
and, "We need to catch up
in this," that, you know,

00:08:20.270 --> 00:08:22.900
"We cannot have a foreign
company showing us up

00:08:22.900 --> 00:08:24.300
at our own game.

00:08:24.300 --> 00:08:25.730
And this is going to be
something that is going to be

00:08:25.730 --> 00:08:27.100
critically important
in the future."

00:08:27.100 --> 00:08:29.230
So, you know, we called it the
Sputnik moment for,

00:08:29.230 --> 00:08:31.000
for the Chinese government--

00:08:31.000 --> 00:08:33.970
the Chinese government kind of
woke up.

00:08:33.970 --> 00:08:36.600
&gt;&gt; (translated): As we often say
in China,

00:08:36.600 --> 00:08:39.700
"The beginning is the most
difficult part."

00:08:39.700 --> 00:08:42.630
&gt;&gt; NARRATOR: In 2017, Xi Jinping
announced

00:08:42.630 --> 00:08:44.570
the government's bold new plans

00:08:44.570 --> 00:08:47.570
to an audience
of foreign diplomats.

00:08:47.570 --> 00:08:51.000
China would catch up with the
U.S. in artificial intelligence

00:08:51.000 --> 00:08:55.170
by 2025 and lead the world
by 2030.

00:08:55.170 --> 00:08:57.500
&gt;&gt; (translated): ...and
intensified cooperation

00:08:57.500 --> 00:09:00.270
in frontier areas such as
digital economy,

00:09:00.270 --> 00:09:02.930
artificial intelligence,
nanotechnology,

00:09:02.930 --> 00:09:05.230
and accounting computing.

00:09:05.230 --> 00:09:08.730
♪ ♪

00:09:11.530 --> 00:09:15.200
&gt;&gt; NARRATOR: Today, China leads
the world in e-commerce.

00:09:18.370 --> 00:09:22.070
Drones deliver to rural
villages.

00:09:22.070 --> 00:09:25.070
And a society that bypassed
credit cards

00:09:25.070 --> 00:09:28.030
now shops in stores
without cashiers,

00:09:28.030 --> 00:09:33.200
where the currency
is facial recognition.

00:09:33.200 --> 00:09:36.230
&gt;&gt; No country has ever moved
that fast.

00:09:36.230 --> 00:09:38.730
And in a short two-and-a-half
years,

00:09:38.730 --> 00:09:43.400
China's A.I. implementation
really went from minimal amount

00:09:43.400 --> 00:09:47.230
to probably about
17 or 18 unicorns,

00:09:47.230 --> 00:09:50.000
that is billion-dollar
companies, in A.I. today.

00:09:50.000 --> 00:09:55.130
And that, that progress is,
is hard to believe.

00:09:55.130 --> 00:09:57.830
&gt;&gt; NARRATOR: The progress was
powered by a new generation

00:09:57.830 --> 00:10:01.870
of ambitious young techs pouring
out of Chinese universities,

00:10:01.870 --> 00:10:05.570
competing with each other
for new ideas,

00:10:05.570 --> 00:10:11.630
and financed by a new cadre of
Chinese venture capitalists.

00:10:11.630 --> 00:10:13.600
This is Sinovation,

00:10:13.600 --> 00:10:17.100
created by U.S.-educated A.I.
scientist and businessman

00:10:17.100 --> 00:10:19.000
Kai-Fu Lee.

00:10:19.000 --> 00:10:24.170
&gt;&gt; These unicorns-- we've got
one, two, three, four, five,

00:10:24.170 --> 00:10:27.300
six, in the general A.I. area.

00:10:27.300 --> 00:10:29.630
And unicorn means a
billion-dollar company,

00:10:29.630 --> 00:10:33.870
a company whose valuation
or market capitalization

00:10:33.870 --> 00:10:36.870
is at $1 billion or higher.

00:10:36.870 --> 00:10:42.830
I think we put two unicorns
to show $5 billion or higher.

00:10:42.830 --> 00:10:45.300
&gt;&gt; NARRATOR: Kai-Fu Lee was born
in Taiwan.

00:10:45.300 --> 00:10:48.530
His parents sent him
to high school in Tennessee.

00:10:48.530 --> 00:10:51.270
His PhD thesis
at Carnegie Mellon

00:10:51.270 --> 00:10:53.900
was on computer speech
recognition,

00:10:53.900 --> 00:10:55.570
which took him to Apple.

00:10:55.570 --> 00:10:57.900
&gt;&gt; Well, reality is a step
closer to science fiction,

00:10:57.900 --> 00:11:00.770
with Apple Computers'
new developed program...

00:11:00.770 --> 00:11:03.830
&gt;&gt; NARRATOR: And at 31,
an early measure of fame.

00:11:03.830 --> 00:11:06.100
&gt;&gt; Kai-Fu Lee,
the inventor of Apple's

00:11:06.100 --> 00:11:07.530
speech-recognition technology.

00:11:07.530 --> 00:11:10.400
&gt;&gt; Casper, copy this
to Make Write 2.

00:11:10.400 --> 00:11:12.730
Casper, paste.

00:11:12.730 --> 00:11:15.600
Casper, 72-point italic outline.

00:11:15.600 --> 00:11:18.970
&gt;&gt; NARRATOR: He would move on to
Microsoft research in Asia

00:11:18.970 --> 00:11:21.430
and became the head
of Google China.

00:11:21.430 --> 00:11:26.530
Ten years ago, he started
Sinovation in Beijing,

00:11:26.530 --> 00:11:30.700
and began looking for promising
startups and A.I. talent.

00:11:30.700 --> 00:11:33.500
&gt;&gt; So, the Chinese
entrepreneurial companies

00:11:33.500 --> 00:11:35.500
started as copycats.

00:11:35.500 --> 00:11:39.570
But over the last 15 years,
China has developed its own form

00:11:39.570 --> 00:11:45.100
of entrepreneurship, and that
entrepreneurship is described

00:11:45.100 --> 00:11:50.130
as tenacious, very fast,
winner-take-all,

00:11:50.130 --> 00:11:52.930
and incredible work ethic.

00:11:52.930 --> 00:11:57.430
I would say these few thousand
Chinese top entrepreneurs,

00:11:57.430 --> 00:11:59.230
they could take on any
entrepreneur

00:11:59.230 --> 00:12:01.400
anywhere in the world.

00:12:01.400 --> 00:12:04.170
&gt;&gt; NARRATOR: Entrepreneurs like
Cao Xudong,

00:12:04.170 --> 00:12:10.100
the 33-year-old C.E.O. of
a new startup called Momenta.

00:12:10.100 --> 00:12:12.700
This is a ring road around
Beijing.

00:12:12.700 --> 00:12:15.470
The car is driving itself.

00:12:15.470 --> 00:12:18.730
♪ ♪

00:12:21.230 --> 00:12:24.470
&gt;&gt; You see, another cutting,
another cutting-in.

00:12:24.470 --> 00:12:26.670
&gt;&gt; Another cut-in, yeah, yeah.

00:12:26.670 --> 00:12:29.470
&gt;&gt; NARRATOR: Cao has no doubt
about the inevitability

00:12:29.470 --> 00:12:33.430
of autonomous vehicles.

00:12:33.430 --> 00:12:39.130
&gt;&gt; Just like AlphaGo can beat
the human player in, in Go,

00:12:39.130 --> 00:12:43.570
I think the machine will
definitely surpass

00:12:43.570 --> 00:12:47.370
the human driver, in the end.

00:12:47.370 --> 00:12:48.730
&gt;&gt; NARRATOR: Recently, there
have been cautions

00:12:48.730 --> 00:12:53.530
about how soon autonomous
vehicles will be deployed,

00:12:53.530 --> 00:12:55.700
but Cao and his team are
confident

00:12:55.700 --> 00:12:58.730
they're in for the long haul.

00:12:58.730 --> 00:13:01.030
&gt;&gt; U.S. will be the first
to deploy,

00:13:01.030 --> 00:13:03.830
but China may be the first
to popularize.

00:13:03.830 --> 00:13:05.270
It is 50-50 right now.

00:13:05.270 --> 00:13:07.000
U.S. is ahead in technology.

00:13:07.000 --> 00:13:10.030
China has a larger market,
and the Chinese government

00:13:10.030 --> 00:13:12.870
is helping with infrastructure
efforts--

00:13:12.870 --> 00:13:16.100
for example, building a new city
the size of Chicago

00:13:16.100 --> 00:13:18.670
with autonomous driving enabled,

00:13:18.670 --> 00:13:21.700
and also a new highway that has
sensors built in

00:13:21.700 --> 00:13:24.230
to help autonomous vehicle
be safer.

00:13:24.230 --> 00:13:27.470
&gt;&gt; NARRATOR: Their early
investors included

00:13:27.470 --> 00:13:29.470
Mercedes-Benz.

00:13:29.470 --> 00:13:33.430
&gt;&gt; I feel very lucky and very
inspiring

00:13:33.430 --> 00:13:38.000
and very exciting that we're
living in this era.

00:13:38.000 --> 00:13:40.800
♪ ♪

00:13:40.800 --> 00:13:42.630
&gt;&gt; NARRATOR: Life in China is
largely conducted

00:13:42.630 --> 00:13:45.000
on smartphones.

00:13:45.000 --> 00:13:48.270
A billion people use WeChat,
the equivalent of Facebook,

00:13:48.270 --> 00:13:51.030
Messenger, and PayPal,
and much more,

00:13:51.030 --> 00:13:54.200
combined into just one
super-app.

00:13:54.200 --> 00:13:55.870
And there are many more.

00:13:55.870 --> 00:14:00.070
&gt;&gt; China is the best place
for A.I. implementation today,

00:14:00.070 --> 00:14:04.230
because the vast amount of data
that's available in China.

00:14:04.230 --> 00:14:07.670
China has a lot more users than
any other country,

00:14:07.670 --> 00:14:10.700
three to four times more than
the U.S.

00:14:10.700 --> 00:14:14.900
There are 50 times more mobile
payments than the U.S.

00:14:14.900 --> 00:14:17.300
There are ten times more food
deliveries,

00:14:17.300 --> 00:14:21.030
which serve as data to learn
more about user behavior

00:14:21.030 --> 00:14:22.870
than the U.S.

00:14:22.870 --> 00:14:26.570
300 times more shared bicycle
rides,

00:14:26.570 --> 00:14:30.400
and each shared bicycle ride
has all kinds of sensors

00:14:30.400 --> 00:14:32.730
submitting data up to the cloud.

00:14:32.730 --> 00:14:36.230
We're talking about maybe ten
times more data than the U.S.,

00:14:36.230 --> 00:14:41.230
and A.I. is basically run on
data and fueled by data.

00:14:41.230 --> 00:14:44.400
The more data, the better
the A.I. works,

00:14:44.400 --> 00:14:47.500
more importantly than how
brilliant the researcher is

00:14:47.500 --> 00:14:49.000
working on the problem.

00:14:49.000 --> 00:14:54.100
So, in the age of A.I.,
where data is the new oil,

00:14:54.100 --> 00:14:57.370
China is the new Saudi Arabia.

00:14:57.370 --> 00:14:59.570
&gt;&gt; NARRATOR: And access to all
that data

00:14:59.570 --> 00:15:02.870
means that the deep-learning
algorithm can quickly predict

00:15:02.870 --> 00:15:05.370
behavior, like the
creditworthiness of someone

00:15:05.370 --> 00:15:06.870
wanting a short-term loan.

00:15:06.870 --> 00:15:09.270
&gt;&gt; Here is our application.

00:15:09.270 --> 00:15:13.900
And customer can choose how many
money they want to borrow

00:15:13.900 --> 00:15:16.830
and how long they want
to borrow,

00:15:16.830 --> 00:15:21.130
and they can input
their datas here.

00:15:21.130 --> 00:15:27.530
And after, after that, you can
just borrow very quickly.

00:15:27.530 --> 00:15:31.030
&gt;&gt; NARRATOR: The C.E.O. shows us
how quickly you can get a loan.

00:15:31.030 --> 00:15:33.100
&gt;&gt; It is, it has done.

00:15:33.100 --> 00:15:35.430
&gt;&gt; NARRATOR: It takes an average
of eight seconds.

00:15:35.430 --> 00:15:38.400
&gt;&gt; It has passed to banks.
&gt;&gt; Wow.

00:15:38.400 --> 00:15:40.170
&gt;&gt; NARRATOR:
In the eight seconds,

00:15:40.170 --> 00:15:42.930
the algorithm has assessed
5,000 personal features

00:15:42.930 --> 00:15:44.630
from all your data.

00:15:44.630 --> 00:15:50.500
&gt;&gt; 5,000 features that is
related with the delinquency,

00:15:50.500 --> 00:15:57.800
when maybe the banks only use
few, maybe, maybe ten features

00:15:57.800 --> 00:16:02.170
when they are doing
their risk amendment.

00:16:02.170 --> 00:16:03.630
&gt;&gt; NARRATOR: Processing millions
of transactions,

00:16:03.630 --> 00:16:06.930
it'll dig up features that would
never be apparent

00:16:06.930 --> 00:16:11.870
to a human loan officer,
like how confidently you type

00:16:11.870 --> 00:16:15.600
your loan application,
or, surprisingly,

00:16:15.600 --> 00:16:18.670
if you keep your cell phone
battery charged.

00:16:18.670 --> 00:16:21.300
&gt;&gt; It's very interesting, the
battery of the phone

00:16:21.300 --> 00:16:24.170
is related with their
delinquency rate.

00:16:24.170 --> 00:16:26.530
Someone who has much more
lower battery,

00:16:26.530 --> 00:16:31.400
they get much more dangerous
than others.

00:16:31.400 --> 00:16:34.370
&gt;&gt; It's probably unfathomable
to an American

00:16:34.370 --> 00:16:39.600
how a country can dramatically
evolve itself

00:16:39.600 --> 00:16:43.470
from a copycat laggard to,
all of a sudden,

00:16:43.470 --> 00:16:48.030
to nearly as good as the U.S. in
technology.

00:16:48.030 --> 00:16:50.300
&gt;&gt; NARRATOR: Like this
facial-recognition startup

00:16:50.300 --> 00:16:51.800
he invested in.

00:16:51.800 --> 00:16:56.470
Megvii was started by three
young graduates in 2011.

00:16:56.470 --> 00:17:00.700
It's now a world leader in using
A.I. to identify people.

00:17:03.530 --> 00:17:05.000
&gt;&gt; It's pretty fast.

00:17:05.000 --> 00:17:07.530
For example,
on the mobile device,

00:17:07.530 --> 00:17:10.670
we have timed the
facial-recognition speed.

00:17:10.670 --> 00:17:13.830
It's actually less
than 100 milliseconds.

00:17:13.830 --> 00:17:15.830
So, that's very, very fast.

00:17:15.830 --> 00:17:19.970
So 0.1 second that we can, we
will be able to recognize you,

00:17:19.970 --> 00:17:24.200
even on a mobile device.

00:17:24.200 --> 00:17:26.300
&gt;&gt; NARRATOR: The company claims
the system is better

00:17:26.300 --> 00:17:30.170
than any human at identifying
people in its database.

00:17:30.170 --> 00:17:33.770
And for those who aren't,
it can describe them.

00:17:33.770 --> 00:17:36.530
Like our director--
what he's wearing,

00:17:36.530 --> 00:17:42.230
and a good guess at his age,
missing it by only a few months.

00:17:42.230 --> 00:17:46.970
&gt;&gt; We are the first one to
really take facial recognition

00:17:46.970 --> 00:17:50.570
to commercial quality.

00:17:50.570 --> 00:17:52.070
&gt;&gt; NARRATOR: That's why in
Beijing today,

00:17:52.070 --> 00:17:57.630
you can pay for your KFC
with a smile.

00:17:57.630 --> 00:17:59.070
&gt;&gt; You know, it's not so
surprising,

00:17:59.070 --> 00:18:01.230
we've seen Chinese companies
catching up to the U.S.

00:18:01.230 --> 00:18:02.630
in technology for a long time.

00:18:02.630 --> 00:18:05.200
And so, if particular effort
and attention is paid

00:18:05.200 --> 00:18:07.700
in a specific sector,
it's not so surprising

00:18:07.700 --> 00:18:09.600
that they would surpass
the rest of the world.

00:18:09.600 --> 00:18:12.000
And facial recognition is one of
the, really the first places

00:18:12.000 --> 00:18:15.300
we've seen that start to happen.

00:18:15.300 --> 00:18:18.230
&gt;&gt; NARRATOR: It's a technology
prized by the government,

00:18:18.230 --> 00:18:23.370
like this program in Shenzhen
to discourage jaywalking.

00:18:23.370 --> 00:18:27.400
Offenders are shamed in public--
and with facial recognition,

00:18:27.400 --> 00:18:31.330
can be instantly fined.

00:18:31.330 --> 00:18:34.570
Critics warn that the government
and some private companies

00:18:34.570 --> 00:18:37.170
have been building a national
database

00:18:37.170 --> 00:18:41.330
from dozens of experimental
social-credit programs.

00:18:41.330 --> 00:18:43.500
&gt;&gt; The government wants to
integrate

00:18:43.500 --> 00:18:48.670
all these individual behaviors,
or corporations' records,

00:18:48.670 --> 00:18:55.670
into some kind of metrics and
compute out a single number

00:18:55.670 --> 00:18:59.030
or set of number associated
with a individual,

00:18:59.030 --> 00:19:04.670
a citizen, and using that,
to implement a incentive

00:19:04.670 --> 00:19:06.130
or punishment system.

00:19:06.130 --> 00:19:07.400
&gt;&gt; NARRATOR: A high
social-credit number

00:19:07.400 --> 00:19:11.100
can be rewarded with discounts
on bus fares.

00:19:11.100 --> 00:19:15.800
A low number can lead
to a travel ban.

00:19:15.800 --> 00:19:18.430
Some say it's very popular
with a Chinese public

00:19:18.430 --> 00:19:21.500
that wants to punish
bad behavior.

00:19:21.500 --> 00:19:25.070
Others see a future that rewards
party loyalty

00:19:25.070 --> 00:19:28.570
and silences criticism.

00:19:28.570 --> 00:19:32.970
&gt;&gt; Right now, there is no final
system being implemented.

00:19:32.970 --> 00:19:41.070
And from those experiments, we
already see that the possibility

00:19:41.070 --> 00:19:44.400
of what this social-credit
system can do to individual.

00:19:44.400 --> 00:19:48.400
It's very powerful--
Orwellian-like--

00:19:48.400 --> 00:19:56.170
and it's extremely troublesome
in terms of civil liberty.

00:19:56.170 --> 00:19:58.270
&gt;&gt; NARRATOR: Every evening
in Shanghai,

00:19:58.270 --> 00:20:01.270
ever-present cameras record the
crowds

00:20:01.270 --> 00:20:03.430
as they surge down to the Bund,

00:20:03.430 --> 00:20:07.530
the promenade along the banks
of the Huangpu River.

00:20:07.530 --> 00:20:10.830
Once the great trading houses of
Europe came here to do business

00:20:10.830 --> 00:20:12.570
with the Middle Kingdom.

00:20:12.570 --> 00:20:15.600
In the last century,
they were all shut down

00:20:15.600 --> 00:20:18.200
by Mao's revolution.

00:20:18.200 --> 00:20:20.530
But now, in the age of A.I.,

00:20:20.530 --> 00:20:22.670
people come here to take
in a spectacle

00:20:22.670 --> 00:20:26.100
that reflects China's
remarkable progress.

00:20:26.100 --> 00:20:28.630
(spectators gasp)

00:20:28.630 --> 00:20:32.070
And illuminates the great
political paradox of capitalism

00:20:32.070 --> 00:20:37.200
taken root
in the communist state.

00:20:37.200 --> 00:20:40.800
&gt;&gt; People have called it
market Leninism,

00:20:40.800 --> 00:20:43.570
authoritarian capitalism.

00:20:43.570 --> 00:20:46.970
We are watching a kind
of a Petri dish

00:20:46.970 --> 00:20:54.270
in which an experiment of, you
know, extraordinary importance

00:20:54.270 --> 00:20:55.970
to the world is
being carried out.

00:20:55.970 --> 00:20:59.030
Whether you can combine these
things

00:20:59.030 --> 00:21:02.170
and get something
that's more powerful,

00:21:02.170 --> 00:21:04.870
that's coherent,
that's durable in the world.

00:21:04.870 --> 00:21:07.600
Whether you can bring together
a one-party state

00:21:07.600 --> 00:21:12.370
with an innovative sector,
both economically

00:21:12.370 --> 00:21:14.400
and technologically innovative,

00:21:14.400 --> 00:21:20.700
and that's something we thought
could not coexist.

00:21:20.700 --> 00:21:23.070
&gt;&gt; NARRATOR:
As China reinvents itself,

00:21:23.070 --> 00:21:25.170
it has set its sights
on leading the world

00:21:25.170 --> 00:21:29.170
in artificial intelligence
by 2030.

00:21:29.170 --> 00:21:32.230
But that means taking on the
world's most innovative

00:21:32.230 --> 00:21:34.100
A.I. culture.

00:21:34.100 --> 00:21:37.600
♪ ♪

00:21:46.900 --> 00:21:49.930
On an interstate
in the U.S. Southwest,

00:21:49.930 --> 00:21:52.830
artificial intelligence is at
work solving the problem

00:21:52.830 --> 00:21:56.030
that's become emblematic
of the new age,

00:21:56.030 --> 00:21:58.800
replacing a human driver.

00:21:58.800 --> 00:22:04.370
♪ ♪

00:22:04.370 --> 00:22:08.730
This is the company's C.E.O.,
24-year-old Alex Rodrigues.

00:22:11.630 --> 00:22:13.800
&gt;&gt; The more things we build
successfully,

00:22:13.800 --> 00:22:15.970
the less people ask questions

00:22:15.970 --> 00:22:18.870
about how old you are when you
have working trucks.

00:22:18.870 --> 00:22:21.800
&gt;&gt; NARRATOR: And this is what
he's built.

00:22:21.800 --> 00:22:24.670
Commercial goods are being
driven from California

00:22:24.670 --> 00:22:29.170
to Arizona on Interstate 10.

00:22:29.170 --> 00:22:34.270
There is a driver in the cab,
but he's not driving.

00:22:34.270 --> 00:22:40.630
It's a path set by a C.E.O.
with an unusual CV.

00:22:40.630 --> 00:22:42.930
&gt;&gt; Are we ready, Henry?

00:22:42.930 --> 00:22:47.730
The aim is to score these pucks
into the scoring area.

00:22:47.730 --> 00:22:51.400
So I, I did competitive robotics
starting when I was 11,

00:22:51.400 --> 00:22:53.130
and I took it very, very
seriously.

00:22:53.130 --> 00:22:55.900
To, to give you a sense, I won
the Robotics World Championships

00:22:55.900 --> 00:22:57.830
for the first time
when I was 13.

00:22:57.830 --> 00:22:59.430
I've been to worlds seven times

00:22:59.430 --> 00:23:02.200
between the ages of 13
and 20-ish.

00:23:02.200 --> 00:23:04.330
I eventually founded a team,

00:23:04.330 --> 00:23:07.000
did a lot of work at a
very high competitive level.

00:23:07.000 --> 00:23:08.470
Things looking pretty good.

00:23:08.470 --> 00:23:10.930
&gt;&gt; NARRATOR: This was a
prototype of sorts,

00:23:10.930 --> 00:23:15.130
from which he has built his
multi-million-dollar company.

00:23:15.130 --> 00:23:18.100
&gt;&gt; I hadn't built a robot in a
while, wanted to get back to it,

00:23:18.100 --> 00:23:21.030
and felt that this was by far
the most exciting piece

00:23:21.030 --> 00:23:22.930
of robotics technology that was
up and coming.

00:23:22.930 --> 00:23:25.170
A lot of people told us we
wouldn't be able to build it.

00:23:25.170 --> 00:23:28.570
But knew roughly the techniques
that you would use.

00:23:28.570 --> 00:23:30.370
And I was pretty confident that
if you put them together,

00:23:30.370 --> 00:23:32.330
you would get something
that worked.

00:23:32.330 --> 00:23:35.900
Took the summer off, built in my
parents' garage a golf cart

00:23:35.900 --> 00:23:40.470
that could drive itself.

00:23:40.470 --> 00:23:42.430
&gt;&gt; NARRATOR: That golf cart
got the attention

00:23:42.430 --> 00:23:45.400
of Silicon Valley,
and the first of several rounds

00:23:45.400 --> 00:23:47.570
of venture capital.

00:23:47.570 --> 00:23:50.670
He formed a team and then
decided the business opportunity

00:23:50.670 --> 00:23:53.700
was in self-driving trucks.

00:23:53.700 --> 00:23:56.470
He says there's also
a human benefit.

00:23:56.470 --> 00:23:58.630
&gt;&gt; If we can build a truck
that's ten times safer

00:23:58.630 --> 00:24:02.770
than a human driver, then not
much else actually matters.

00:24:02.770 --> 00:24:05.770
When we talk to regulators,
especially,

00:24:05.770 --> 00:24:08.930
everyone agrees that the only
way that we're going to get

00:24:08.930 --> 00:24:11.770
to zero highway deaths,
which is everyone's objective,

00:24:11.770 --> 00:24:13.800
is to use self-driving.

00:24:13.800 --> 00:24:17.030
And so, I'm sure you've heard
the statistic,

00:24:17.030 --> 00:24:19.230
more than 90% of all crashes

00:24:19.230 --> 00:24:20.870
have a human driver
as the cause.

00:24:20.870 --> 00:24:24.230
So if you want to solve
traffic fatalities,

00:24:24.230 --> 00:24:28.170
which, in my opinion, are the
single biggest tragedy

00:24:28.170 --> 00:24:30.970
that happens year after year
in the United States,

00:24:30.970 --> 00:24:33.800
this is the only solution.

00:24:33.800 --> 00:24:36.230
&gt;&gt; NARRATOR:
It's an ambitious goal,

00:24:36.230 --> 00:24:38.430
but only possible because
of the recent breakthroughs

00:24:38.430 --> 00:24:40.170
in deep learning.

00:24:40.170 --> 00:24:42.300
&gt;&gt; Artificial intelligence is
one of those key pieces

00:24:42.300 --> 00:24:46.530
that has made it possible now
to do driverless vehicles

00:24:46.530 --> 00:24:49.070
where it wasn't possible
ten years ago,

00:24:49.070 --> 00:24:53.870
particularly in the ability
to see and understand scenes.

00:24:53.870 --> 00:24:57.130
A lot of people don't know this,
but it's remarkably hard

00:24:57.130 --> 00:24:58.870
for computers,
until very, very recently,

00:24:58.870 --> 00:25:02.800
to do even the most basic
visual tasks,

00:25:02.800 --> 00:25:04.530
like seeing a picture
of a person

00:25:04.530 --> 00:25:06.070
and knowing that it's a person.

00:25:06.070 --> 00:25:09.270
And we've made gigantic strides
with artificial intelligence

00:25:09.270 --> 00:25:11.530
in being able to see and
understanding tasks,

00:25:11.530 --> 00:25:14.000
and that's obviously fundamental
to being able to understand

00:25:14.000 --> 00:25:15.770
the world around you
with the sensors that,

00:25:15.770 --> 00:25:19.870
that you have available.

00:25:19.870 --> 00:25:21.270
&gt;&gt; NARRATOR: That's now possible

00:25:21.270 --> 00:25:23.970
because of the algorithms
written by Yoshua Bengio

00:25:23.970 --> 00:25:28.070
and a small group of scientists.

00:25:28.070 --> 00:25:30.630
&gt;&gt; There are many aspects
of the world

00:25:30.630 --> 00:25:34.200
which we can't explain
with words.

00:25:34.200 --> 00:25:36.400
And that part of our knowledge
is actually

00:25:36.400 --> 00:25:39.170
probably the majority of it.

00:25:39.170 --> 00:25:41.400
So, like, the stuff we can
communicate verbally

00:25:41.400 --> 00:25:43.370
is the tip of the iceberg.

00:25:43.370 --> 00:25:48.600
And so to get at the bottom of
the iceberg, the solution was,

00:25:48.600 --> 00:25:53.000
the computers have to acquire
that knowledge by themselves

00:25:53.000 --> 00:25:54.500
from data, from examples.

00:25:54.500 --> 00:25:58.400
Just like children learn,
most not from their teachers,

00:25:58.400 --> 00:26:01.370
but from interacting
with the world,

00:26:01.370 --> 00:26:03.500
and playing around, and, and
trying things

00:26:03.500 --> 00:26:05.570
and seeing what works
and what doesn't work.

00:26:05.570 --> 00:26:07.870
&gt;&gt; NARRATOR: This is an early
demonstration.

00:26:07.870 --> 00:26:12.470
In 2013, deep-mind scientists
set a machine-learning program

00:26:12.470 --> 00:26:16.070
on the Atari video game
Breakout.

00:26:16.070 --> 00:26:19.870
The computer was only told
the goal-- to win the game.

00:26:19.870 --> 00:26:24.230
After 100 games, it learned to
use the bat at the bottom

00:26:24.230 --> 00:26:27.770
to hit the ball and break
the bricks at the top.

00:26:27.770 --> 00:26:33.030
After 300, it could do that
better than a human player.

00:26:33.030 --> 00:26:37.970
After 500 games, it came up with
a creative way to win the game--

00:26:37.970 --> 00:26:40.730
by digging a tunnel on the side

00:26:40.730 --> 00:26:42.270
and sending the ball
around the top

00:26:42.270 --> 00:26:44.730
to break many bricks
with one hit.

00:26:44.730 --> 00:26:48.170
That was deep learning.

00:26:48.170 --> 00:26:50.630
&gt;&gt; That's the A.I. program based
on learning,

00:26:50.630 --> 00:26:52.430
really, that has been
so successful

00:26:52.430 --> 00:26:54.870
in the last few years and has...

00:26:54.870 --> 00:26:57.430
It wasn't clear ten years ago
that it would work,

00:26:57.430 --> 00:27:00.600
but it has completely changed
the map

00:27:00.600 --> 00:27:06.570
and is now used in almost
every sector of society.

00:27:06.570 --> 00:27:08.970
&gt;&gt; Even the best and brightest
among us,

00:27:08.970 --> 00:27:11.000
we just don't have enough
compute power

00:27:11.000 --> 00:27:13.530
inside of our heads.

00:27:13.530 --> 00:27:16.000
&gt;&gt; NARRATOR: Amy Webb is a
professor at N.Y.U.

00:27:16.000 --> 00:27:19.970
and founder of the Future Today
Institute.

00:27:19.970 --> 00:27:26.270
&gt;&gt; As A.I. progresses, the great
promise is that they...

00:27:26.270 --> 00:27:30.700
they, these, these machines,
alongside of us,

00:27:30.700 --> 00:27:34.330
are able to think and imagine
and see things

00:27:34.330 --> 00:27:36.730
in ways that we never have
before,

00:27:36.730 --> 00:27:40.370
which means that maybe we have
some kind of new,

00:27:40.370 --> 00:27:45.330
weird, seemingly implausible
solution to climate change.

00:27:45.330 --> 00:27:49.530
Maybe we have some radically
different approach

00:27:49.530 --> 00:27:52.930
to dealing with
incurable cancers.

00:27:52.930 --> 00:27:58.330
The real practical and wonderful
promise is that machines help us

00:27:58.330 --> 00:28:02.170
be more creative, and,
using that creativity,

00:28:02.170 --> 00:28:06.430
we get to terrific solutions.

00:28:06.430 --> 00:28:09.500
&gt;&gt; NARRATOR: Solutions that
could come unexpectedly

00:28:09.500 --> 00:28:11.770
to urgent problems.

00:28:11.770 --> 00:28:13.700
&gt;&gt; It's going to change
the face of breast cancer.

00:28:13.700 --> 00:28:16.870
Right now, 40,000 women
in the U.S. alone

00:28:16.870 --> 00:28:19.670
die from breast cancer
every single year.

00:28:19.670 --> 00:28:21.870
&gt;&gt; NARRATOR: Dr. Connie Lehman
is head

00:28:21.870 --> 00:28:23.230
of the breast imaging center

00:28:23.230 --> 00:28:26.470
at Massachusetts General
Hospital in Boston.

00:28:26.470 --> 00:28:28.930
&gt;&gt; We've become so complacent
about it,

00:28:28.930 --> 00:28:31.070
we almost don't think it can
really be changed.

00:28:31.070 --> 00:28:33.470
We, we somehow think we should
put all of our energy

00:28:33.470 --> 00:28:36.670
into chemotherapies
to save women

00:28:36.670 --> 00:28:38.370
with metastatic breast cancer,

00:28:38.370 --> 00:28:41.430
and yet, you know, when we find
it early, we cure it,

00:28:41.430 --> 00:28:44.730
and we cure it without having
the ravages to the body

00:28:44.730 --> 00:28:46.830
when we diagnose it late.

00:28:46.830 --> 00:28:51.700
This shows the progression of a
small, small spot from one year

00:28:51.700 --> 00:28:54.530
to the next,
and then to the diagnosis

00:28:54.530 --> 00:28:57.730
of the small cancer here.

00:28:57.730 --> 00:28:59.830
&gt;&gt; NARRATOR: This is what
happened when a woman

00:28:59.830 --> 00:29:02.100
who had been diagnosed
with breast cancer

00:29:02.100 --> 00:29:04.170
started to ask questions

00:29:04.170 --> 00:29:07.370
about why it couldn't have been
diagnosed earlier.

00:29:07.370 --> 00:29:10.170
&gt;&gt; It really brings a lot of
anxiety,

00:29:10.170 --> 00:29:12.270
and you're asking the questions,
you know,

00:29:12.270 --> 00:29:13.530
"Am I going to survive?

00:29:13.530 --> 00:29:15.200
What's going to happen
to my son?"

00:29:15.200 --> 00:29:19.230
And I start asking
other questions.

00:29:19.230 --> 00:29:21.700
&gt;&gt; NARRATOR: She was used to
asking questions.

00:29:21.700 --> 00:29:24.970
At M.I.T.'s
artificial-intelligence lab,

00:29:24.970 --> 00:29:27.970
Professor Regina Barzilay uses
deep learning

00:29:27.970 --> 00:29:31.100
to teach the computer to
understand language,

00:29:31.100 --> 00:29:34.270
as well as read text and data.

00:29:34.270 --> 00:29:37.600
&gt;&gt; I was really surprised
that the very basic question

00:29:37.600 --> 00:29:39.970
that I ask my physicians,

00:29:39.970 --> 00:29:43.330
which were really excellent
physicians here at MGH,

00:29:43.330 --> 00:29:47.030
they couldn't give me answers
that I was looking for.

00:29:47.030 --> 00:29:50.770
&gt;&gt; NARRATOR: She was convinced
that if you analyze enough data,

00:29:50.770 --> 00:29:53.530
from mammograms
to diagnostic notes,

00:29:53.530 --> 00:29:56.800
the computer could predict
early-stage conditions.

00:29:56.800 --> 00:30:02.830
&gt;&gt; If we fast-forward from 2012
to '13 to 2014,

00:30:02.830 --> 00:30:05.900
we then see when Regina
was diagnosed,

00:30:05.900 --> 00:30:10.170
because of this spot on her
mammogram.

00:30:10.170 --> 00:30:14.830
Is it possible, with more
elegant computer applications,

00:30:14.830 --> 00:30:19.100
that we might have identified
this spot the year before,

00:30:19.100 --> 00:30:21.200
or even back here?

00:30:21.200 --> 00:30:22.830
&gt;&gt; So, those are standard
prediction problems

00:30:22.830 --> 00:30:26.870
in machine learning-- there is
nothing special about them.

00:30:26.870 --> 00:30:29.900
And to my big surprise,
none of the technologies

00:30:29.900 --> 00:30:33.130
that we are developing
at M.I.T.,

00:30:33.130 --> 00:30:38.730
even in the most simple form,
doesn't penetrate the hospital.

00:30:38.730 --> 00:30:41.670
&gt;&gt; NARRATOR: Regina and Connie
began the slow process

00:30:41.670 --> 00:30:45.070
of getting access to thousands
of mammograms and records

00:30:45.070 --> 00:30:46.770
from MGH's breast-imaging
program.

00:30:49.470 --> 00:30:53.130
&gt;&gt; So, our first foray was just
to take all of the patients

00:30:53.130 --> 00:30:56.130
we had at MGH during
a period of time,

00:30:56.130 --> 00:30:58.530
who had had breast surgery
for a certain type

00:30:58.530 --> 00:31:00.430
of high-risk lesion.

00:31:00.430 --> 00:31:03.700
And we found that most of them
didn't really need the surgery.

00:31:03.700 --> 00:31:05.070
They didn't have cancer.

00:31:05.070 --> 00:31:07.670
But about ten percent
did have cancer.

00:31:07.670 --> 00:31:10.570
With Regina's techniques
in deep learning

00:31:10.570 --> 00:31:13.370
and machine learning, we were
able to predict the women

00:31:13.370 --> 00:31:15.600
that truly needed the surgery
and separate out

00:31:15.600 --> 00:31:19.470
those that really could avoid
the unnecessary surgery.

00:31:19.470 --> 00:31:23.030
&gt;&gt; What machine can do, it can
take hundreds of thousands

00:31:23.030 --> 00:31:25.730
of images where the outcome
is known

00:31:25.730 --> 00:31:30.700
and learn, based on how, you
know, pixels are distributed,

00:31:30.700 --> 00:31:35.170
what are the very unique
patterns that correlate highly

00:31:35.170 --> 00:31:38.370
with future occurrence
of the disease.

00:31:38.370 --> 00:31:40.900
So, instead of using human
capacity

00:31:40.900 --> 00:31:44.770
to kind of recognize pattern,
formalize pattern--

00:31:44.770 --> 00:31:48.700
which is inherently limited
by our cognitive capacity

00:31:48.700 --> 00:31:50.800
and how much we can see
and remember--

00:31:50.800 --> 00:31:53.700
we're providing machine with a
lot of data

00:31:53.700 --> 00:31:57.630
and make it learn
this prediction.

00:31:57.630 --> 00:32:02.370
&gt;&gt; So, we are using technology
not only to be better

00:32:02.370 --> 00:32:04.770
at assessing the breast density,

00:32:04.770 --> 00:32:07.200
but to get more to the point of
what we're trying to predict.

00:32:07.200 --> 00:32:10.930
"Does this woman have
a cancer now,

00:32:10.930 --> 00:32:13.170
and will she develop a cancer
in five years? "

00:32:13.170 --> 00:32:16.770
And that's, again, where
the artificial intelligence,

00:32:16.770 --> 00:32:18.700
machine and deep learning can
really help us

00:32:18.700 --> 00:32:20.770
and our patients.

00:32:20.770 --> 00:32:22.830
&gt;&gt; NARRATOR: In the age of A.I.,

00:32:22.830 --> 00:32:26.330
the algorithms are transporting
us into a universe

00:32:26.330 --> 00:32:29.970
of vast potential and
transforming almost every aspect

00:32:29.970 --> 00:32:34.200
of human endeavor and
experience.

00:32:34.200 --> 00:32:38.000
Andrew McAfee is a research
scientist at M.I.T.

00:32:38.000 --> 00:32:42.000
who co-authored
"The Second Machine Age."

00:32:42.000 --> 00:32:45.070
&gt;&gt; The great compliment that a
songwriter gives another one is,

00:32:45.070 --> 00:32:46.600
"Gosh, I wish I had written
that one."

00:32:46.600 --> 00:32:49.100
The great compliment a geek
gives another one is,

00:32:49.100 --> 00:32:50.900
"Wow, I wish I had drawn
that graph."

00:32:50.900 --> 00:32:53.630
So, I wish I had drawn
this graph.

00:32:53.630 --> 00:32:55.500
&gt;&gt; NARRATOR:
The graph uses a formula

00:32:55.500 --> 00:32:59.400
to show human development and
growth since 2000 BCE.

00:32:59.400 --> 00:33:01.570
&gt;&gt; The state of human
civilization

00:33:01.570 --> 00:33:04.970
is not very advanced, and it's
not getting better

00:33:04.970 --> 00:33:07.130
very quickly at all,
and this is true for thousands

00:33:07.130 --> 00:33:08.970
and thousands of years.

00:33:08.970 --> 00:33:12.470
When we, when we formed empires
and empires got overturned,

00:33:12.470 --> 00:33:16.530
when we tried democracy,
when we invented zero

00:33:16.530 --> 00:33:19.630
and mathematics and fundamental
discoveries about the universe,

00:33:19.630 --> 00:33:21.400
big deal.

00:33:21.400 --> 00:33:23.300
It just, the numbers don't
change very much.

00:33:23.300 --> 00:33:26.900
What's weird is that the numbers
change essentially in the blink

00:33:26.900 --> 00:33:28.370
of an eye at one point in time.

00:33:28.370 --> 00:33:32.030
And it goes from really
horizontal, unchanging,

00:33:32.030 --> 00:33:36.600
uninteresting, to, holy Toledo,
crazy vertical.

00:33:36.600 --> 00:33:39.200
And then the question is,
what on Earth happened

00:33:39.200 --> 00:33:40.570
to cause that change?

00:33:40.570 --> 00:33:42.770
And the answer
is the Industrial Revolution.

00:33:42.770 --> 00:33:44.800
There were other things that
happened,

00:33:44.800 --> 00:33:46.830
but really what fundamentally
happened is

00:33:46.830 --> 00:33:49.530
we overcame the limitations
of our muscle power.

00:33:49.530 --> 00:33:52.400
Something equally interesting is
happening right now.

00:33:52.400 --> 00:33:55.330
We are overcoming the
limitations of our minds.

00:33:55.330 --> 00:33:56.930
We're not getting rid of them,

00:33:56.930 --> 00:33:58.970
we're not making them
unnecessary,

00:33:58.970 --> 00:34:02.500
but, holy cow, can we leverage
them and amplify them now.

00:34:02.500 --> 00:34:04.170
You have to be a huge pessimist

00:34:04.170 --> 00:34:06.730
not to find that profoundly
good news.

00:34:06.730 --> 00:34:09.370
&gt;&gt; I really do think the world
has entered a new era.

00:34:09.370 --> 00:34:12.830
Artificial intelligence holds so
much promise,

00:34:12.830 --> 00:34:15.730
but it's going to reshape every
aspect of the economy,

00:34:15.730 --> 00:34:17.370
so many aspects of our lives.

00:34:17.370 --> 00:34:20.770
Because A.I. is a little bit
like electricity.

00:34:20.770 --> 00:34:22.670
Everybody's going to use it.

00:34:22.670 --> 00:34:26.400
Every company is going to be
incorporating A.I.,

00:34:26.400 --> 00:34:28.300
integrating it into
what they do,

00:34:28.300 --> 00:34:29.630
governments are going to be
using it,

00:34:29.630 --> 00:34:33.600
nonprofit organizations are
going to be using it.

00:34:33.600 --> 00:34:37.200
It's going to create all kinds
of benefits

00:34:37.200 --> 00:34:41.070
in ways large and small,
and challenges for us, as well.

00:34:41.070 --> 00:34:44.730
&gt;&gt; NARRATOR: The challenges,
the benefits--

00:34:44.730 --> 00:34:47.000
the autonomous truck
represents both

00:34:47.000 --> 00:34:50.070
as it maneuvers
into the marketplace.

00:34:50.070 --> 00:34:53.070
The engineers are confident
that, in spite of questions

00:34:53.070 --> 00:34:55.370
about when this will happen,

00:34:55.370 --> 00:34:57.330
they can get it working safely
sooner

00:34:57.330 --> 00:34:58.770
than most people realize.

00:34:58.770 --> 00:35:02.130
&gt;&gt; I think that you will see the
first vehicles operating

00:35:02.130 --> 00:35:05.570
with no one inside them moving
freight in the next few years,

00:35:05.570 --> 00:35:07.700
and then you're going to see
that expanding to more freight,

00:35:07.700 --> 00:35:11.030
more geographies,
more weather over time as,

00:35:11.030 --> 00:35:12.530
as that capability builds up.

00:35:12.530 --> 00:35:16.600
We're talking, like,
less than half a decade.

00:35:16.600 --> 00:35:19.370
&gt;&gt; NARRATOR: He already has a
Fortune 500 company

00:35:19.370 --> 00:35:23.830
as a client, shipping appliances
across the Southwest.

00:35:23.830 --> 00:35:27.330
He says the sales pitch
is straightforward.

00:35:27.330 --> 00:35:30.070
&gt;&gt; They spend hundreds of
millions of dollars a year

00:35:30.070 --> 00:35:31.670
shipping parts around
the country.

00:35:31.670 --> 00:35:34.100
We can bring that cost in half.

00:35:34.100 --> 00:35:36.930
And they're really excited to be
able to start working with us,

00:35:36.930 --> 00:35:39.800
both because of the potential,

00:35:39.800 --> 00:35:42.100
the potential savings from
deploying self-driving,

00:35:42.100 --> 00:35:44.470
and also because of all the
operational efficiencies

00:35:44.470 --> 00:35:47.830
that they see, the biggest one
being able to operate

00:35:47.830 --> 00:35:49.800
24 hours a day.

00:35:49.800 --> 00:35:51.970
So, right now, human drivers are
limited to 11 hours

00:35:51.970 --> 00:35:55.470
by federal law,
and a driverless truck

00:35:55.470 --> 00:35:57.000
obviously wouldn't have
that limitation.

00:35:57.000 --> 00:36:02.530
♪ ♪

00:36:02.530 --> 00:36:05.330
&gt;&gt; NARRATOR: The idea of a
driverless truck comes up often

00:36:05.330 --> 00:36:11.430
in discussions about artificial
intelligence.

00:36:11.430 --> 00:36:14.800
Steve Viscelli is a sociologist
who drove a truck

00:36:14.800 --> 00:36:20.330
while researching his book "The
Big Rig" about the industry.

00:36:20.330 --> 00:36:23.000
&gt;&gt; This is one of the most
remarkable stories

00:36:23.000 --> 00:36:25.830
in, in U.S. labor history,
I think,

00:36:25.830 --> 00:36:30.400
is, you know, the decline of,
of unionized trucking.

00:36:30.400 --> 00:36:33.600
The industry was deregulated
in 1980,

00:36:33.600 --> 00:36:37.400
and at that time, you know,
truck drivers were earning

00:36:37.400 --> 00:36:41.400
the equivalent of over
$100,000 in today's dollars.

00:36:41.400 --> 00:36:45.500
And today the typical truck
driver will earn

00:36:45.500 --> 00:36:50.370
a little over $40,000 a year.

00:36:50.370 --> 00:36:52.630
And I think it's
an important part

00:36:52.630 --> 00:36:54.230
of the automation story, right?

00:36:54.230 --> 00:36:56.900
Why are they so afraid of
automation?

00:36:56.900 --> 00:37:00.670
Because we've had four decades
of rising inequality in wages.

00:37:00.670 --> 00:37:03.330
And if anybody is going to take
it on the chin

00:37:03.330 --> 00:37:05.330
from automation
in the trucking industry,

00:37:05.330 --> 00:37:07.630
the, the first in line is going
to be the driver,

00:37:07.630 --> 00:37:12.300
without a doubt.

00:37:12.300 --> 00:37:14.730
&gt;&gt; NARRATOR: For his research,
Viscelli tracked down truckers

00:37:14.730 --> 00:37:17.600
and their families,
like Shawn and Hope Cumbee

00:37:17.600 --> 00:37:19.530
of Beaverton, Michigan.
&gt;&gt; Hi.

00:37:19.530 --> 00:37:20.870
&gt;&gt; Hey, Hope,
I'm Steve Viscelli.

00:37:20.870 --> 00:37:21.870
&gt;&gt; Hi, Steve, nice to meet you.
Come on in.

00:37:21.870 --> 00:37:24.800
&gt;&gt; Great to meet you, too,
thanks.

00:37:24.800 --> 00:37:26.430
&gt;&gt; NARRATOR: And their son
Charlie.

00:37:26.430 --> 00:37:31.730
&gt;&gt; This is Daddy, me,
Daddy, and Mommy.

00:37:31.730 --> 00:37:34.230
&gt;&gt; NARRATOR: But Daddy's not
here.

00:37:34.230 --> 00:37:38.900
Shawn Cumbee's truck has broken
down in Tennessee.

00:37:38.900 --> 00:37:43.470
Hope, who drove a truck herself,
knows the business well.

00:37:43.470 --> 00:37:46.870
&gt;&gt; We made $150,000, right,
in a year.

00:37:46.870 --> 00:37:48.070
That sounds great, right?

00:37:48.070 --> 00:37:50.400
That's, like, good money.

00:37:50.400 --> 00:37:53.870
We paid $100,000 in fuel, okay?

00:37:53.870 --> 00:37:57.030
So, right there,
now I made $50,000.

00:37:57.030 --> 00:37:59.030
But I didn't really, because,
you know,

00:37:59.030 --> 00:38:00.600
you get an oil change every
month,

00:38:00.600 --> 00:38:02.200
so that's $300 a month.

00:38:02.200 --> 00:38:04.170
You still have to do
all the maintenance.

00:38:04.170 --> 00:38:06.500
We had a motor blow out, right?

00:38:06.500 --> 00:38:09.170
$13,000. Right?

00:38:09.170 --> 00:38:11.800
I know, I mean, I choke up a
little just thinking about it,

00:38:11.800 --> 00:38:13.770
because it was...

00:38:13.770 --> 00:38:17.470
And it was 13,000, and we were
off work for two weeks.

00:38:17.470 --> 00:38:19.670
So, by the end of the year,
with that $150,000,

00:38:19.670 --> 00:38:22.670
by the end of the year,
we'd made about 20...

00:38:22.670 --> 00:38:26.030
About $22,000.

00:38:26.030 --> 00:38:28.400
&gt;&gt; NARRATOR: In a truck stop
in Tennessee,

00:38:28.400 --> 00:38:31.500
Shawn has been sidelined
waiting for a new part.

00:38:31.500 --> 00:38:35.300
The garage owner is letting him
stay in the truck to save money.

00:38:37.870 --> 00:38:39.770
&gt;&gt; Hi, baby.

00:38:39.770 --> 00:38:41.330
&gt;&gt; (on phone): Hey, how's it
going?

00:38:41.330 --> 00:38:42.730
&gt;&gt; It's going.
Chunky-butt!

00:38:42.730 --> 00:38:44.600
&gt;&gt; Hi, Daddy!
&gt;&gt; Hi, Chunky-butt.

00:38:44.600 --> 00:38:47.300
What're you doing?
&gt;&gt; (talking inaudibly)

00:38:47.300 --> 00:38:49.600
&gt;&gt; Believe it or not,
I do it because I love it.

00:38:49.600 --> 00:38:51.330
I mean, you know,
it's in the blood.

00:38:51.330 --> 00:38:52.900
Third-generation driver.

00:38:52.900 --> 00:38:55.230
And my granddaddy told me a long
time ago,

00:38:55.230 --> 00:38:58.630
when I was probably
11, 12 years old, probably,

00:38:58.630 --> 00:39:01.500
he said, "The world meets nobody
halfway.

00:39:01.500 --> 00:39:02.930
Nobody."

00:39:02.930 --> 00:39:07.030
He said, "If you want it,
you have to earn it."

00:39:07.030 --> 00:39:09.870
And that's what I do every day.

00:39:09.870 --> 00:39:11.330
I live by that creed.

00:39:11.330 --> 00:39:16.100
And I've lived by that
since it was told to me.

00:39:16.100 --> 00:39:18.300
&gt;&gt; So, if you're down for a week
in a truck,

00:39:18.300 --> 00:39:19.870
you still have to pay your
bills.

00:39:19.870 --> 00:39:22.100
I have enough money in my
checking account at all times

00:39:22.100 --> 00:39:23.470
to pay a month's worth of bills.

00:39:23.470 --> 00:39:25.070
That does not include my food.

00:39:25.070 --> 00:39:27.630
That doesn't include field trips
for my son's school.

00:39:27.630 --> 00:39:31.700
My son and I just went to our
yearly doctor appointment.

00:39:31.700 --> 00:39:36.270
I took, I took money out of my
son's piggy bank to pay for it,

00:39:36.270 --> 00:39:40.600
because it's not...
it's not scheduled in.

00:39:40.600 --> 00:39:43.430
It's, it's not something that
you can, you know, afford.

00:39:43.430 --> 00:39:45.500
I mean, like, when...

00:39:45.500 --> 00:39:46.900
(sighs): Sorry.

00:39:46.900 --> 00:39:48.970
&gt;&gt; It's okay.

00:39:48.970 --> 00:39:52.600
♪ ♪

00:39:57.230 --> 00:39:59.170
Have you guys ever talked about
self-driving trucks?

00:39:59.170 --> 00:40:00.500
Is he...

00:40:00.500 --> 00:40:03.130
&gt;&gt; (laughing): So, kind of.

00:40:03.130 --> 00:40:05.830
Um, I asked him once, you know.

00:40:05.830 --> 00:40:07.230
And he laughed so hard.

00:40:07.230 --> 00:40:10.330
He said, "No way will they
ever have a truck

00:40:10.330 --> 00:40:12.970
that can drive itself."

00:40:12.970 --> 00:40:15.230
&gt;&gt; It's kind of interesting when
you think about it, you know,

00:40:15.230 --> 00:40:17.730
they're putting all this new
technology into things,

00:40:17.730 --> 00:40:19.570
but, you know,
it's still man-made.

00:40:19.570 --> 00:40:22.970
And man, you know,
does make mistakes.

00:40:22.970 --> 00:40:26.170
I really don't see it being
a problem with the industry,

00:40:26.170 --> 00:40:28.770
'cause, one, you still got to
have a driver in it,

00:40:28.770 --> 00:40:30.330
because I don't see it
doing city.

00:40:30.330 --> 00:40:32.600
I don't see it doing,
you know, main things.

00:40:32.600 --> 00:40:34.700
I don't see it backing into
a dock.

00:40:34.700 --> 00:40:37.870
I don't see the automation part,
you know, doing...

00:40:37.870 --> 00:40:39.900
maybe the box-trailer side,
you know, I can see that,

00:40:39.900 --> 00:40:41.400
but not stuff like I do.

00:40:41.400 --> 00:40:44.830
So, I ain't really worried about
the automation of trucks.

00:40:44.830 --> 00:40:46.230
&gt;&gt; How near of a future is it?

00:40:46.230 --> 00:40:49.300
&gt;&gt; Yeah, self-driving, um...

00:40:49.300 --> 00:40:52.600
So, some, you know, some
companies are already operating.

00:40:52.600 --> 00:40:56.170
Embark, for instance, is one
that has been doing

00:40:56.170 --> 00:40:59.030
driverless trucks
on the interstate.

00:40:59.030 --> 00:41:01.930
And what's called exit-to-exit
self-driving.

00:41:01.930 --> 00:41:04.830
And they're currently running
real freight.

00:41:04.830 --> 00:41:07.530
&gt;&gt; Really?
&gt;&gt; Yeah, on I-10.

00:41:07.530 --> 00:41:10.530
♪ ♪

00:41:10.530 --> 00:41:15.170
&gt;&gt; (on P.A.): Shower guest 100,
your shower is now ready.

00:41:15.170 --> 00:41:18.430
&gt;&gt; NARRATOR: Over time, it has
become harder and harder

00:41:18.430 --> 00:41:21.230
for veteran independent drivers
like the Cumbees

00:41:21.230 --> 00:41:23.070
to make a living.

00:41:23.070 --> 00:41:25.070
They've been replaced by
younger,

00:41:25.070 --> 00:41:28.200
less experienced drivers.

00:41:28.200 --> 00:41:32.630
&gt;&gt; So, the, the trucking
industry's $740 billion a year,

00:41:32.630 --> 00:41:34.770
and, again, in, in many
of these operations,

00:41:34.770 --> 00:41:37.470
labor's a third of that cost.

00:41:37.470 --> 00:41:40.500
By my estimate, I, you know,
I think we're in the range

00:41:40.500 --> 00:41:42.970
of 300,000 or so jobs
in the foreseeable future

00:41:42.970 --> 00:41:47.930
that could be automated to some
significant extent.

00:41:47.930 --> 00:41:50.630
♪ ♪

00:41:50.630 --> 00:41:53.530
&gt;&gt; (groans)

00:41:53.530 --> 00:41:57.070
♪ ♪

00:42:03.000 --> 00:42:06.130
&gt;&gt; NARRATOR: The A.I. future
was built with great optimism

00:42:06.130 --> 00:42:09.100
out here in the West.

00:42:09.100 --> 00:42:12.630
In 2018, many of the people
who invented it

00:42:12.630 --> 00:42:16.170
gathered in San Francisco to
celebrate the 25th anniversary

00:42:16.170 --> 00:42:18.700
of the industry magazine.

00:42:18.700 --> 00:42:22.300
&gt;&gt; Howdy, welcome to WIRED25.

00:42:22.300 --> 00:42:24.200
&gt;&gt; NARRATOR: It is a
celebration, for sure,

00:42:24.200 --> 00:42:27.070
but there's also a growing sense
of caution

00:42:27.070 --> 00:42:28.670
and even skepticism.

00:42:31.130 --> 00:42:33.330
&gt;&gt; We're having a really good
weekend here.

00:42:33.330 --> 00:42:37.030
&gt;&gt; NARRATOR: Nick Thompson is
editor-in-chief of "Wired."

00:42:37.030 --> 00:42:40.030
&gt;&gt; When it started,
it was very much a magazine

00:42:40.030 --> 00:42:44.100
about what's coming and why you
should be excited about it.

00:42:44.100 --> 00:42:47.730
Optimism was the defining
feature of "Wired"

00:42:47.730 --> 00:42:49.400
for many, many years.

00:42:49.400 --> 00:42:53.130
Or, as our slogan used to be,
"Change Is Good."

00:42:53.130 --> 00:42:55.070
And over time,
it shifted a little bit.

00:42:55.070 --> 00:42:59.170
And now it's more,
"We love technology,

00:42:59.170 --> 00:43:00.630
but let's look at some
of the big issues,

00:43:00.630 --> 00:43:03.400
and let's look at some of them
critically,

00:43:03.400 --> 00:43:05.730
and let's look at the way
algorithms are changing

00:43:05.730 --> 00:43:07.930
the way we behave,
for good and for ill."

00:43:07.930 --> 00:43:12.030
So, the whole nature of "Wired"
has gone from a champion

00:43:12.030 --> 00:43:14.830
of technological change to more
of a observer

00:43:14.830 --> 00:43:16.700
of technological change.

00:43:16.700 --> 00:43:18.570
&gt;&gt; So, um, before we start...

00:43:18.570 --> 00:43:20.530
&gt;&gt; NARRATOR: There
are 25 speakers,

00:43:20.530 --> 00:43:23.700
all named as icons
of the last 25 years

00:43:23.700 --> 00:43:25.500
of technological progress.

00:43:25.500 --> 00:43:27.770
&gt;&gt; So, why is Apple so
secretive?

00:43:27.770 --> 00:43:29.470
&gt;&gt; (chuckling)

00:43:29.470 --> 00:43:31.630
&gt;&gt; NARRATOR: Jony Ive, who
designed Apple's iPhone.

00:43:31.630 --> 00:43:34.300
&gt;&gt; It would be bizarre
not to be.

00:43:34.300 --> 00:43:36.670
&gt;&gt; There's this question of,
like,

00:43:36.670 --> 00:43:39.000
what are we doing here in this
life, in this reality?

00:43:39.000 --> 00:43:43.170
&gt;&gt; NARRATOR: Jaron Lanier, who
pioneered virtual reality.

00:43:43.170 --> 00:43:46.500
And Jeff Bezos,
the founder of Amazon.

00:43:46.500 --> 00:43:47.870
&gt;&gt; Amazon was a garage startup.

00:43:47.870 --> 00:43:49.370
Now it's a very large company.

00:43:49.370 --> 00:43:50.570
Two kids in a dorm...

00:43:50.570 --> 00:43:52.070
&gt;&gt; NARRATOR: His message is,

00:43:52.070 --> 00:43:54.730
"All will be well
in the new world."

00:43:54.730 --> 00:43:58.470
&gt;&gt; I guess, first of all, I
remain incredibly optimistic

00:43:58.470 --> 00:43:59.630
about technology,

00:43:59.630 --> 00:44:01.830
and technologies always
are two-sided.

00:44:01.830 --> 00:44:03.230
But that's not new.

00:44:03.230 --> 00:44:05.400
That's always been the case.

00:44:05.400 --> 00:44:07.830
And, and we will figure it out.

00:44:07.830 --> 00:44:10.570
The last thing we would ever
want to do is stop the progress

00:44:10.570 --> 00:44:16.630
of new technologies,
even when they are dual-use.

00:44:16.630 --> 00:44:19.800
&gt;&gt; NARRATOR: But, says Thompson,
beneath the surface,

00:44:19.800 --> 00:44:22.530
there's a worry most of them
don't like to talk about.

00:44:22.530 --> 00:44:26.630
&gt;&gt; There are some people in
Silicon Valley who believe that,

00:44:26.630 --> 00:44:29.900
"You just have to trust
the technology.

00:44:29.900 --> 00:44:32.870
Throughout history, there's been
a complicated relationship

00:44:32.870 --> 00:44:34.470
between humans and machines,

00:44:34.470 --> 00:44:36.770
we've always worried about
machines,

00:44:36.770 --> 00:44:38.130
and it's always been fine.

00:44:38.130 --> 00:44:41.000
And we don't know how A.I. will
change the labor force,

00:44:41.000 --> 00:44:42.300
but it will be okay."

00:44:42.300 --> 00:44:44.070
So, that argument exists.

00:44:44.070 --> 00:44:45.700
There's another argument,

00:44:45.700 --> 00:44:48.170
which is what I think most of
them believe deep down,

00:44:48.170 --> 00:44:51.100
which is, "This is different.

00:44:51.100 --> 00:44:52.930
We're going to have labor-force
disruption

00:44:52.930 --> 00:44:55.030
like we've never seen before.

00:44:55.030 --> 00:44:59.370
And if that happens,
will they blame us?"

00:44:59.370 --> 00:45:02.600
&gt;&gt; NARRATOR: There is, however,
one of the WIRED25 icons

00:45:02.600 --> 00:45:05.800
willing to take on the issue.

00:45:05.800 --> 00:45:09.470
Onstage, Kai-Fu Lee dispenses
with one common fear.

00:45:09.470 --> 00:45:11.670
&gt;&gt; Well, I think there are so
many myths out there.

00:45:11.670 --> 00:45:14.530
I think one, one myth is that

00:45:14.530 --> 00:45:17.570
because A.I. is so good at a
single task,

00:45:17.570 --> 00:45:21.600
that one day we'll wake up, and
we'll all be enslaved

00:45:21.600 --> 00:45:24.100
or forced to plug our brains
to the A.I.

00:45:24.100 --> 00:45:28.800
But it is nowhere close
to displacing humans.

00:45:28.800 --> 00:45:32.130
&gt;&gt; NARRATOR: But in interviews
around the event and beyond,

00:45:32.130 --> 00:45:37.430
he takes a decidedly contrarian
position on A.I. and job loss.

00:45:37.430 --> 00:45:41.270
&gt;&gt; The A.I. giants want to paint
the rosier picture

00:45:41.270 --> 00:45:43.500
because they're happily
making money.

00:45:43.500 --> 00:45:47.330
So, I think they prefer not to
talk about the negative side.

00:45:47.330 --> 00:45:53.070
I believe about 50% of jobs
will be

00:45:53.070 --> 00:45:56.900
somewhat or extremely
threatened by A.I.

00:45:56.900 --> 00:46:00.500
in the next 15 years or so.

00:46:00.500 --> 00:46:02.570
&gt;&gt; NARRATOR: Kai-Fu Lee also
makes a great deal

00:46:02.570 --> 00:46:04.900
of money from A.I.

00:46:04.900 --> 00:46:06.800
What separates him from most of
his colleagues

00:46:06.800 --> 00:46:09.930
is that he's frank
about its downside.

00:46:09.930 --> 00:46:13.900
&gt;&gt; Yes, yes, we, we've made
about 40 investments in A.I.

00:46:13.900 --> 00:46:16.930
I think, based on these 40
investments,

00:46:16.930 --> 00:46:20.000
most of them are not impacting
human jobs.

00:46:20.000 --> 00:46:21.970
They're creating value,
making high margins,

00:46:21.970 --> 00:46:24.300
inventing a new model.

00:46:24.300 --> 00:46:27.730
But I could list seven or eight

00:46:27.730 --> 00:46:32.670
that would lead to a very clear
displacement of human jobs.

00:46:32.670 --> 00:46:34.370
&gt;&gt; NARRATOR: He says that A.I.
is coming,

00:46:34.370 --> 00:46:36.470
whether we like it or not.

00:46:36.470 --> 00:46:38.300
And he wants to warn society

00:46:38.300 --> 00:46:41.030
about what he sees as
inevitable.

00:46:41.030 --> 00:46:43.600
&gt;&gt; You have a view which I think
is different than many others,

00:46:43.600 --> 00:46:48.670
which is that A.I. is not going
to take blue-collar jobs

00:46:48.670 --> 00:46:51.230
so quickly, but is actually
going to take white-collar jobs.

00:46:51.230 --> 00:46:53.770
&gt;&gt; Yeah.
Well, both will happen.

00:46:53.770 --> 00:46:57.000
A.I. will be, at the same time,
a replacement for blue-collar,

00:46:57.000 --> 00:47:00.630
white-collar jobs, and be
a great symbiotic tool

00:47:00.630 --> 00:47:03.630
for doctors, lawyers, and you,
for example.

00:47:03.630 --> 00:47:05.700
But the white-collar jobs are
easier to take,

00:47:05.700 --> 00:47:10.030
because they're a pure
quantitative analytical process.

00:47:10.030 --> 00:47:15.370
Let's say reporters, traders,
telemarketing,

00:47:15.370 --> 00:47:17.270
telesales, customer service...

00:47:17.270 --> 00:47:18.730
&gt;&gt; Analysts?

00:47:18.730 --> 00:47:23.170
&gt;&gt; Analysts, yes, these can all
be replaced just by a software.

00:47:23.170 --> 00:47:26.330
To do blue-collar, some of the
work requires, you know,

00:47:26.330 --> 00:47:30.030
hand-eye coordination, things
that machines are not yet

00:47:30.030 --> 00:47:32.300
good enough to do.

00:47:32.300 --> 00:47:36.400
&gt;&gt; Today, there are many people
who are ringing the alarm,

00:47:36.400 --> 00:47:37.600
"Oh, my God, what are we going
to do?

00:47:37.600 --> 00:47:39.830
Half the jobs are going away."

00:47:39.830 --> 00:47:43.430
I believe that's true, but
here's the missing fact.

00:47:43.430 --> 00:47:46.400
I've done the research on this,
and if you go back 20, 30,

00:47:46.400 --> 00:47:50.930
or 40 years ago, you will find
that 50% of the jobs

00:47:50.930 --> 00:47:54.400
that people performed back then
are gone today.

00:47:54.400 --> 00:47:56.900
You know, where are all the
telephone operators,

00:47:56.900 --> 00:48:00.600
bowling-pin setters,
elevator operators?

00:48:00.600 --> 00:48:04.270
You used to have seas of
secretaries in corporations

00:48:04.270 --> 00:48:06.070
that have now been eliminated--
travel agents.

00:48:06.070 --> 00:48:08.770
You can just go through field
after field after field.

00:48:08.770 --> 00:48:12.100
That same pattern has recurred
many times throughout history,

00:48:12.100 --> 00:48:14.230
with each new wave
of automation.

00:48:14.230 --> 00:48:20.270
&gt;&gt; But I would argue that
history is only trustable

00:48:20.270 --> 00:48:24.670
if it is multiple repetitions
of similar events,

00:48:24.670 --> 00:48:28.670
not once-in-a-blue-moon
occurrence.

00:48:28.670 --> 00:48:33.070
So, over the history of many
tech inventions,

00:48:33.070 --> 00:48:34.770
most are small things.

00:48:34.770 --> 00:48:41.330
Only maybe three are at the
magnitude of A.I. revolution--

00:48:41.330 --> 00:48:44.730
the steam, steam engine,
electricity,

00:48:44.730 --> 00:48:46.570
and the computer revolution.

00:48:46.570 --> 00:48:48.970
I'd say everything else
is too small.

00:48:48.970 --> 00:48:52.670
And the reason I think it might
be something brand-new

00:48:52.670 --> 00:48:58.930
is that A.I. is fundamentally
replacing our cognitive process

00:48:58.930 --> 00:49:03.670
in doing a job in its
significant entirety,

00:49:03.670 --> 00:49:06.400
and it can do it dramatically
better.

00:49:06.400 --> 00:49:08.570
&gt;&gt; NARRATOR: This argument
about job loss

00:49:08.570 --> 00:49:11.470
in the age of A.I. was ignited
six years ago

00:49:11.470 --> 00:49:15.830
amid the gargoyles and spires
of Oxford University.

00:49:15.830 --> 00:49:19.970
Two researchers had been poring
through U.S. labor statistics,

00:49:19.970 --> 00:49:25.270
identifying jobs that could be
vulnerable to A.I. automation.

00:49:25.270 --> 00:49:27.300
&gt;&gt; Well, vulnerable to
automation,

00:49:27.300 --> 00:49:30.730
in the context that we discussed
five years ago now,

00:49:30.730 --> 00:49:34.430
essentially meant that those
jobs are potentially automatable

00:49:34.430 --> 00:49:36.900
over an unspecified number of
years.

00:49:36.900 --> 00:49:41.530
And the figure we came up with
was 47%.

00:49:41.530 --> 00:49:43.330
&gt;&gt; NARRATOR: 47%.

00:49:43.330 --> 00:49:46.470
That number quickly traveled
the world in headlines

00:49:46.470 --> 00:49:47.830
and news bulletins.

00:49:47.830 --> 00:49:51.030
But authors Carl Frey
and Michael Osborne

00:49:51.030 --> 00:49:52.770
offered a caution.

00:49:52.770 --> 00:49:57.670
They can't predict how many jobs
will be lost, or how quickly.

00:49:57.670 --> 00:50:02.430
But Frey believes that there are
lessons in history.

00:50:02.430 --> 00:50:04.830
&gt;&gt; And what worries me the most
is that there is actually

00:50:04.830 --> 00:50:08.830
one episode that looks quite
familiar to today,

00:50:08.830 --> 00:50:12.270
which is the British
Industrial Revolution,

00:50:12.270 --> 00:50:16.400
where wages didn't grow
for nine decades,

00:50:16.400 --> 00:50:20.530
and a lot of people actually
saw living standards decline

00:50:20.530 --> 00:50:23.870
as technology progressed.

00:50:23.870 --> 00:50:25.630
♪ ♪

00:50:25.630 --> 00:50:28.370
&gt;&gt; NARRATOR: Saginaw, Michigan,
knows about decline

00:50:28.370 --> 00:50:31.170
in living standards.

00:50:31.170 --> 00:50:34.900
Harry Cripps, an auto worker
and a local union president,

00:50:34.900 --> 00:50:40.730
has witnessed what 40 years of
automation can do to a town.

00:50:40.730 --> 00:50:43.470
&gt;&gt; You know, we're one of the
cities in the country that,

00:50:43.470 --> 00:50:47.170
I think we were left behind in
this recovery.

00:50:47.170 --> 00:50:51.670
And I just... I don't know how
we get on the bandwagon now.

00:50:54.770 --> 00:50:57.030
&gt;&gt; NARRATOR: Once, this was the
U.A.W. hall

00:50:57.030 --> 00:50:59.230
for one local union.

00:50:59.230 --> 00:51:03.670
Now, with falling membership,
it's shared by five locals.

00:51:03.670 --> 00:51:05.730
&gt;&gt; Rudy didn't get his shift.

00:51:05.730 --> 00:51:07.330
&gt;&gt; NARRATOR: This day,
it's the center

00:51:07.330 --> 00:51:09.570
for a Christmas food drive.

00:51:09.570 --> 00:51:12.030
Even in a growth economy,

00:51:12.030 --> 00:51:14.830
unemployment here is near
six percent.

00:51:14.830 --> 00:51:18.930
Poverty in Saginaw is over 30%.

00:51:21.830 --> 00:51:25.130
&gt;&gt; Our factory has about
1.9 million square feet.

00:51:25.130 --> 00:51:29.100
Back in the '70s, that 1.9
million square feet

00:51:29.100 --> 00:51:32.330
had about 7,500 U.A.W.
automotive workers

00:51:32.330 --> 00:51:34.300
making middle-class wage with
decent benefits

00:51:34.300 --> 00:51:36.770
and able to send their kids to
college and do all the things

00:51:36.770 --> 00:51:39.000
that the middle-class family
should be able to do.

00:51:39.000 --> 00:51:42.270
Our factory today, with
automation,

00:51:42.270 --> 00:51:46.300
would probably be about
700 United Auto Workers.

00:51:46.300 --> 00:51:50.130
That's a dramatic change.

00:51:50.130 --> 00:51:52.230
Lot of union brothers used
to work there, buddy.

00:51:52.230 --> 00:51:55.130
&gt;&gt; The TRW plant, that was
unfortunate.

00:51:55.130 --> 00:51:57.830
&gt;&gt; Delphi... looks like they're
starting to tear it down now.

00:51:57.830 --> 00:51:59.300
Wow.

00:51:59.300 --> 00:52:02.770
Automations is, is definitely
taking away a lot of jobs.

00:52:02.770 --> 00:52:05.530
Robots, I don't know how they
buy cars,

00:52:05.530 --> 00:52:07.300
I don't know how
they buy sandwiches,

00:52:07.300 --> 00:52:09.100
I don't know how they go to the
grocery store.

00:52:09.100 --> 00:52:11.430
They definitely don't pay taxes,
which serves the infrastructure.

00:52:11.430 --> 00:52:15.300
So, you don't have the sheriffs
and the police and the firemen,

00:52:15.300 --> 00:52:18.830
and anybody else that supports
the city is gone,

00:52:18.830 --> 00:52:19.900
'cause there's no tax base.

00:52:19.900 --> 00:52:23.770
Robots don't pay taxes.

00:52:23.770 --> 00:52:25.900
&gt;&gt; NARRATOR: The average
personal income in Saginaw

00:52:25.900 --> 00:52:29.570
is $16,000 a year.

00:52:29.570 --> 00:52:32.600
&gt;&gt; A lot of the families that I
work with here in the community,

00:52:32.600 --> 00:52:33.830
both parents are working.

00:52:33.830 --> 00:52:35.470
They're working two jobs.

00:52:35.470 --> 00:52:38.370
Mainly, it's the wages,
you know,

00:52:38.370 --> 00:52:43.270
people not making a decent wage
to be able to support a family.

00:52:43.270 --> 00:52:46.930
Like, back in the day, my dad
even worked at the plant.

00:52:46.930 --> 00:52:49.300
My mom stayed home,
raised the children.

00:52:49.300 --> 00:52:52.000
And that give us the opportunity
to put food on the table,

00:52:52.000 --> 00:52:53.370
and things of that nature.

00:52:53.370 --> 00:52:56.000
And, and them times are gone.

00:52:56.000 --> 00:52:57.930
&gt;&gt; If you look at this graph of
what's been happening

00:52:57.930 --> 00:52:59.670
to America since the end
of World War II,

00:52:59.670 --> 00:53:03.000
you see a line for our
productivity,

00:53:03.000 --> 00:53:05.730
and our productivity
gets better over time.

00:53:05.730 --> 00:53:08.830
It used to be the case
that our pay, our income,

00:53:08.830 --> 00:53:12.700
would increase in lockstep with
those productivity increases.

00:53:12.700 --> 00:53:17.570
The weird part about this graph
is how the income has decoupled,

00:53:17.570 --> 00:53:21.900
is not going up the same way
that productivity is anymore.

00:53:21.900 --> 00:53:24.170
&gt;&gt; NARRATOR: As automation has
taken over,

00:53:24.170 --> 00:53:27.770
workers are either laid off or
left with less-skilled jobs

00:53:27.770 --> 00:53:31.400
for less pay,
while productivity goes up.

00:53:31.400 --> 00:53:33.100
&gt;&gt; There are still plenty
of factories in America.

00:53:33.100 --> 00:53:35.430
We are a manufacturing
powerhouse,

00:53:35.430 --> 00:53:37.670
but if you go walk around
an American factory,

00:53:37.670 --> 00:53:40.070
you do not see long lines
of people

00:53:40.070 --> 00:53:42.470
doing repetitive manual labor.

00:53:42.470 --> 00:53:44.600
You see a whole lot
of automation.

00:53:44.600 --> 00:53:46.230
If you go upstairs in that
factory

00:53:46.230 --> 00:53:47.830
and look at the payroll
department,

00:53:47.830 --> 00:53:51.130
you see one or two people
looking into a screen all day.

00:53:51.130 --> 00:53:53.800
So, the activity is still there,

00:53:53.800 --> 00:53:56.000
but the number of jobs
is very, very low,

00:53:56.000 --> 00:53:58.330
because of automation
and tech progress.

00:53:58.330 --> 00:54:01.130
Now, dealing with
that challenge,

00:54:01.130 --> 00:54:02.900
and figuring out what
the next generation

00:54:02.900 --> 00:54:05.700
of the American middle class
should be doing,

00:54:05.700 --> 00:54:07.700
is a really important challenge,

00:54:07.700 --> 00:54:10.530
because I am pretty confident
that we are never again

00:54:10.530 --> 00:54:13.330
going to have this large,
stable, prosperous

00:54:13.330 --> 00:54:15.730
middle class doing routine work.

00:54:15.730 --> 00:54:19.430
♪ ♪

00:54:19.430 --> 00:54:21.970
&gt;&gt; NARRATOR: Evidence of how
A.I. is likely to bring

00:54:21.970 --> 00:54:25.530
accelerated change to the U.S.
workforce can be found

00:54:25.530 --> 00:54:27.970
not far from Saginaw.

00:54:27.970 --> 00:54:29.600
This is the U.S. headquarters

00:54:29.600 --> 00:54:34.070
for one of the world's largest
builders of industrial robots,

00:54:34.070 --> 00:54:38.030
a Japanese-owned company called
Fanuc Robotics.

00:54:38.030 --> 00:54:41.230
&gt;&gt; We've been producing robots
for well over 35 years.

00:54:41.230 --> 00:54:42.770
And you can imagine,
over the years,

00:54:42.770 --> 00:54:45.330
they've changed quite a bit.

00:54:45.330 --> 00:54:48.230
We're utilizing the artificial
intelligence

00:54:48.230 --> 00:54:49.800
to really make the robots
easier to use

00:54:49.800 --> 00:54:54.400
and be able to handle a broader
spectrum of opportunities.

00:54:54.400 --> 00:54:57.770
We see a huge growth potential
in robotics.

00:54:57.770 --> 00:55:00.330
And we see that growth potential
as being, really,

00:55:00.330 --> 00:55:03.230
there's 90% of the market left.

00:55:03.230 --> 00:55:05.230
&gt;&gt; NARRATOR: The industry says
optimistically

00:55:05.230 --> 00:55:09.270
that with that growth,
they can create more jobs.

00:55:09.270 --> 00:55:11.630
&gt;&gt; Even if there were five
people on a job,

00:55:11.630 --> 00:55:12.870
and we reduced that down to two
people,

00:55:12.870 --> 00:55:15.800
because we automated
some level of it,

00:55:15.800 --> 00:55:18.570
we might produce two times more
parts than we did before,

00:55:18.570 --> 00:55:20.170
because we automated it.

00:55:20.170 --> 00:55:26.430
So now, there might be the need
for two more fork-truck drivers,

00:55:26.430 --> 00:55:29.900
or two more quality-inspection
personnel.

00:55:29.900 --> 00:55:31.870
So, although we reduce
some of the people,

00:55:31.870 --> 00:55:36.100
we grow in other areas as we
produce more things.

00:55:36.100 --> 00:55:41.070
&gt;&gt; When I increase productivity
through automation, I lose jobs.

00:55:41.070 --> 00:55:42.370
Jobs go away.

00:55:42.370 --> 00:55:45.170
And I don't care what the robot
manufacturers say,

00:55:45.170 --> 00:55:47.830
you aren't replacing those ten
production people

00:55:47.830 --> 00:55:51.570
that that robot is now doing
that job, with ten people.

00:55:51.570 --> 00:55:54.830
You can increase productivity to
a level to stay competitive

00:55:54.830 --> 00:55:58.970
with the global market-- that's
what they're trying to do.

00:55:58.970 --> 00:56:00.530
♪ ♪

00:56:00.530 --> 00:56:02.900
&gt;&gt; NARRATOR:
In the popular telling,

00:56:02.900 --> 00:56:06.800
blame for widespread job loss
has been aimed overseas,

00:56:06.800 --> 00:56:08.900
at what's called offshoring.

00:56:08.900 --> 00:56:11.200
&gt;&gt; We want to keep
our factories here,

00:56:11.200 --> 00:56:13.100
we want to keep
our manufacturing here.

00:56:13.100 --> 00:56:17.470
We don't want them moving
to China, to Mexico, to Japan,

00:56:17.470 --> 00:56:21.630
to India, to Vietnam.

00:56:21.630 --> 00:56:23.770
&gt;&gt; NARRATOR: But it turns out
most of the job loss

00:56:23.770 --> 00:56:26.370
isn't because of offshoring.

00:56:26.370 --> 00:56:27.700
&gt;&gt; There's been offshoring.

00:56:27.700 --> 00:56:32.300
And I think offshoring is
responsible for maybe 20%

00:56:32.300 --> 00:56:34.000
of the jobs that have been lost.

00:56:34.000 --> 00:56:36.270
I would say most of the jobs
that have been lost,

00:56:36.270 --> 00:56:38.830
despite what most Americans
thinks, was due to automation

00:56:38.830 --> 00:56:41.830
or productivity growth.

00:56:41.830 --> 00:56:43.570
&gt;&gt; NARRATOR:
Mike Hicks is an economist

00:56:43.570 --> 00:56:46.600
at Ball State University
in Muncie, Indiana.

00:56:46.600 --> 00:56:50.300
He and sociologist Emily Wornell
have been documenting

00:56:50.300 --> 00:56:52.670
employment trends
in Middle America.

00:56:52.670 --> 00:56:57.130
Hicks says that automation has
been a mostly silent job killer,

00:56:57.130 --> 00:56:59.200
lowering the standard of living.

00:56:59.200 --> 00:57:02.400
&gt;&gt; So, in the last 15 years, the
standard of living has dropped

00:57:02.400 --> 00:57:04.600
by 15, ten to 15 percent.

00:57:04.600 --> 00:57:07.100
So, that's unusual
in a developed world.

00:57:07.100 --> 00:57:08.600
A one-year decline
is a recession.

00:57:08.600 --> 00:57:12.470
A 15-year decline gives
an entirely different sense

00:57:12.470 --> 00:57:14.830
about the prospects
of a community.

00:57:14.830 --> 00:57:18.500
And so that is common
from the Canadian border

00:57:18.500 --> 00:57:20.970
to the Gulf of Mexico

00:57:20.970 --> 00:57:23.300
in the middle swath
of the United States.

00:57:23.300 --> 00:57:26.130
&gt;&gt; This is something we're gonna
do for you guys.

00:57:26.130 --> 00:57:30.730
These were left over from our
suggestion drive that we did,

00:57:30.730 --> 00:57:32.200
and we're going to give them
each two.

00:57:32.200 --> 00:57:33.300
&gt;&gt; That is awesome.
&gt;&gt; I mean,

00:57:33.300 --> 00:57:35.070
that is going to go a long ways,
right?

00:57:35.070 --> 00:57:37.070
I mean, that'll really help that
family out during the holidays.

00:57:37.070 --> 00:57:39.800
&gt;&gt; Yes, well, with the kids home
from school,

00:57:39.800 --> 00:57:41.430
the families have three meals
a day that they got

00:57:41.430 --> 00:57:43.170
to put on the table.

00:57:43.170 --> 00:57:45.130
So, it's going to make a big
difference.

00:57:45.130 --> 00:57:47.130
So, thank you, guys.
&gt;&gt; You're welcome.

00:57:47.130 --> 00:57:48.830
&gt;&gt; This is wonderful.
&gt;&gt; Let them know Merry Christmas

00:57:48.830 --> 00:57:50.370
on behalf of us here
at the local, okay?

00:57:50.370 --> 00:57:52.930
&gt;&gt; Absolutely, you guys are
just, just amazing, thank you.

00:57:52.930 --> 00:57:56.270
And please, tell, tell all the
workers how grateful

00:57:56.270 --> 00:57:57.900
these families will be.
&gt;&gt; We will.

00:57:57.900 --> 00:58:00.870
&gt;&gt; I mean, this is not a small
problem.

00:58:00.870 --> 00:58:02.700
The need is so great.

00:58:02.700 --> 00:58:05.830
And I can tell you
that it's all races,

00:58:05.830 --> 00:58:08.070
it's all income classes

00:58:08.070 --> 00:58:09.700
that you might think someone
might be from.

00:58:09.700 --> 00:58:11.900
But I can tell you that when you
see it,

00:58:11.900 --> 00:58:15.000
and you deliver this type
of gift to somebody

00:58:15.000 --> 00:58:18.600
who is in need, just the
gratitude that they show you

00:58:18.600 --> 00:58:22.470
is incredible.

00:58:22.470 --> 00:58:26.470
&gt;&gt; We actually know that people
are at greater risk of mortality

00:58:26.470 --> 00:58:30.130
for over 20 years after they
lose their job due to,

00:58:30.130 --> 00:58:32.670
due to no fault of their own, so
something like automation

00:58:32.670 --> 00:58:34.770
or offshoring.

00:58:34.770 --> 00:58:36.970
They're at higher risk
for cardiovascular disease,

00:58:36.970 --> 00:58:42.500
they're at higher risk
for depression and suicide.

00:58:42.500 --> 00:58:44.630
But then with the
intergenerational impacts,

00:58:44.630 --> 00:58:48.230
we also see their children
are more likely--

00:58:48.230 --> 00:58:50.300
children of parents who have
lost their job

00:58:50.300 --> 00:58:53.670
due to automation-- are more
likely to repeat a grade,

00:58:53.670 --> 00:58:55.570
they're more likely to drop out
of school,

00:58:55.570 --> 00:58:57.700
they're more likely to be
suspended from school,

00:58:57.700 --> 00:58:59.470
and they have lower educational
attainment

00:58:59.470 --> 00:59:03.200
over their entire lifetimes.

00:59:03.200 --> 00:59:06.200
&gt;&gt; It's the future of this,
not the past, that scares me.

00:59:06.200 --> 00:59:08.700
Because I think we're in the
early decades

00:59:08.700 --> 00:59:11.170
of what is a multi-decade
adjustment period.

00:59:11.170 --> 00:59:14.000
♪ ♪

00:59:14.000 --> 00:59:18.170
&gt;&gt; NARRATOR: The world is being
re-imagined.

00:59:18.170 --> 00:59:20.370
This is a supermarket.

00:59:20.370 --> 00:59:24.800
Robots, guided by A.I., pack
everything from soap powder

00:59:24.800 --> 00:59:29.530
to cantaloupes for online
consumers.

00:59:29.530 --> 00:59:31.600
Machines that pick groceries,

00:59:31.600 --> 00:59:35.170
machines that can also read
reports, learn routines,

00:59:35.170 --> 00:59:38.730
and comprehend are reaching deep
into factories,

00:59:38.730 --> 00:59:41.870
stores, and offices.

00:59:41.870 --> 00:59:43.800
At a college in Goshen, Indiana,

00:59:43.800 --> 00:59:47.030
a group of local business and
political leaders come together

00:59:47.030 --> 00:59:52.830
to try to understand the impact
of A.I. and the new machines.

00:59:52.830 --> 00:59:54.870
Molly Kinder studies
the future of work

00:59:54.870 --> 00:59:56.470
at a Washington think tank.

00:59:56.470 --> 00:59:58.970
&gt;&gt; How many people have gone
into a fast-food restaurant

00:59:58.970 --> 01:00:01.370
and done a self-ordering?

01:00:01.370 --> 01:00:02.530
Anyone, yes?

01:00:02.530 --> 01:00:04.400
Panera, for instance,
is doing this.

01:00:04.400 --> 01:00:08.270
Cashier was my first job,
and in, in, where I live,

01:00:08.270 --> 01:00:10.830
in Washington, DC, it's actually
the number-one occupation

01:00:10.830 --> 01:00:12.300
for the greater DC region.

01:00:12.300 --> 01:00:14.670
There are millions of people who
work in cashier positions.

01:00:14.670 --> 01:00:17.000
This is not a futuristic
challenge,

01:00:17.000 --> 01:00:19.800
this is something that's
happening sooner than we think.

01:00:19.800 --> 01:00:24.770
In the popular discussions about
robots and automation and work,

01:00:24.770 --> 01:00:28.600
almost every image is of a man
on a factory floor

01:00:28.600 --> 01:00:29.770
or a truck driver.

01:00:29.770 --> 01:00:32.900
And yet, in our data, when we
looked,

01:00:32.900 --> 01:00:35.900
women disproportionately hold
the jobs that today

01:00:35.900 --> 01:00:37.900
are at highest risk
of automation.

01:00:37.900 --> 01:00:40.800
And that's not really being
talked about,

01:00:40.800 --> 01:00:43.700
and that's in part because women
are over-represented

01:00:43.700 --> 01:00:45.570
in some of these marginalized
occupations,

01:00:45.570 --> 01:00:48.230
like a cashier
or a fast-food worker.

01:00:48.230 --> 01:00:53.670
And also in a large numbers
in clerical jobs in offices--

01:00:53.670 --> 01:00:57.400
HR departments,
payroll, finance,

01:00:57.400 --> 01:01:00.900
a lot of that is more routine
processing information,

01:01:00.900 --> 01:01:03.530
processing paper,
transferring data.

01:01:03.530 --> 01:01:08.000
That has huge potential for
automation.

01:01:08.000 --> 01:01:11.000
A.I. is going to do
some of that, software,

01:01:11.000 --> 01:01:12.900
robots are going to do
some of that.

01:01:12.900 --> 01:01:14.830
So how many people are still
working

01:01:14.830 --> 01:01:16.300
as switchboard operators?

01:01:16.300 --> 01:01:18.170
Probably none in this country.

01:01:18.170 --> 01:01:20.470
&gt;&gt; NARRATOR: The workplace of
the future will demand

01:01:20.470 --> 01:01:24.230
different skills, and gaining
them, says Molly Kinder,

01:01:24.230 --> 01:01:26.300
will depend on who
can afford them.

01:01:26.300 --> 01:01:28.570
&gt;&gt; I mean it's not a good
situation in the United States.

01:01:28.570 --> 01:01:30.330
There's been some excellent
research that says

01:01:30.330 --> 01:01:32.800
that half of Americans
couldn't afford

01:01:32.800 --> 01:01:35.300
a $400 unexpected expense.

01:01:35.300 --> 01:01:38.630
And if you want to get to a
$1,000, there's even less.

01:01:38.630 --> 01:01:41.270
So imagine you're going to go
out without a month's pay,

01:01:41.270 --> 01:01:43.330
two months' pay, a year.

01:01:43.330 --> 01:01:47.030
Imagine you want to put savings
toward a course

01:01:47.030 --> 01:01:49.670
to, to redevelop your career.

01:01:49.670 --> 01:01:52.330
People can't afford to take time
off of work.

01:01:52.330 --> 01:01:56.600
They don't have a cushion, so
this lack of economic stability,

01:01:56.600 --> 01:01:59.500
married with the disruptions in
people's careers,

01:01:59.500 --> 01:02:01.230
is a really toxic mix.

01:02:01.230 --> 01:02:03.630
&gt;&gt; (blowing whistle)

01:02:03.630 --> 01:02:05.530
&gt;&gt; NARRATOR: The new machines
will penetrate every sector

01:02:05.530 --> 01:02:08.600
of the economy:
from insurance companies

01:02:08.600 --> 01:02:11.130
to human resource departments;

01:02:11.130 --> 01:02:14.030
from law firms to the trading
floors of Wall Street.

01:02:14.030 --> 01:02:15.470
&gt;&gt; Wall Street's
going through it,

01:02:15.470 --> 01:02:16.970
but every industry is going
through it.

01:02:16.970 --> 01:02:19.630
Every company is looking at all
of the disruptive technologies,

01:02:19.630 --> 01:02:23.630
could be robotics or drones
or blockchain.

01:02:23.630 --> 01:02:27.130
And whatever it is, every
company's using everything

01:02:27.130 --> 01:02:29.570
that's developed, everything
that's disruptive,

01:02:29.570 --> 01:02:32.370
in thinking about, "How do
I apply that to my business

01:02:32.370 --> 01:02:35.000
to make myself more efficient?"

01:02:35.000 --> 01:02:37.400
And what efficiency means is,
mostly,

01:02:37.400 --> 01:02:40.670
"How do I do this
with fewer workers?"

01:02:43.900 --> 01:02:47.330
And I do think that when we look
at some of the studies

01:02:47.330 --> 01:02:50.700
about opportunity
in this country,

01:02:50.700 --> 01:02:53.030
and the inequality
of opportunity,

01:02:53.030 --> 01:02:55.830
the likelihood that you won't be
able to advance

01:02:55.830 --> 01:02:59.300
from where your parents were, I
think that's, that's,

01:02:59.300 --> 01:03:02.000
is very serious and gets
to the heart of the way

01:03:02.000 --> 01:03:06.430
we like to think of America as
the land of opportunity.

01:03:06.430 --> 01:03:08.970
&gt;&gt; NARRATOR: Inequality has been
rising in America.

01:03:08.970 --> 01:03:13.270
It used to be the top 1%
of earners-- here in red--

01:03:13.270 --> 01:03:16.670
owned a relatively small portion
of the country's wealth.

01:03:16.670 --> 01:03:20.100
Middle and lower earners--
in blue-- had the largest share.

01:03:20.100 --> 01:03:24.970
Then, 15 years ago,
the lines crossed.

01:03:24.970 --> 01:03:29.500
And inequality has been
increasing ever since.

01:03:29.500 --> 01:03:31.830
&gt;&gt; There's many factors that are
driving inequality today,

01:03:31.830 --> 01:03:33.330
and unfortunately,
artificial intelligence--

01:03:33.330 --> 01:03:38.270
without being thoughtful
about it--

01:03:38.270 --> 01:03:41.430
is a driver for increased
inequality

01:03:41.430 --> 01:03:43.900
because it's a form of
automation,

01:03:43.900 --> 01:03:47.000
and automation is the
substitution of capital

01:03:47.000 --> 01:03:49.070
for labor.

01:03:49.070 --> 01:03:52.800
And when you do that,
the people with the capital win.

01:03:52.800 --> 01:03:55.800
So Karl Marx was right,

01:03:55.800 --> 01:03:58.100
it's a struggle between capital
and labor,

01:03:58.100 --> 01:03:59.600
and with artificial
intelligence,

01:03:59.600 --> 01:04:02.830
we're putting our finger on the
scale on the side of capital,

01:04:02.830 --> 01:04:05.770
and how we wish to distribute
the benefits,

01:04:05.770 --> 01:04:07.230
the economic benefits,

01:04:07.230 --> 01:04:09.130
that that will create is going
to be a major

01:04:09.130 --> 01:04:13.430
moral consideration for society
over the next several decades.

01:04:13.430 --> 01:04:19.370
&gt;&gt; This is really an outgrowth
of the increasing gaps

01:04:19.370 --> 01:04:23.600
of haves and have-nots--
the wealthy getting wealthier,

01:04:23.600 --> 01:04:24.870
the poor getting poorer.

01:04:24.870 --> 01:04:28.400
It may not be specifically
related to A.I.,

01:04:28.400 --> 01:04:30.770
but as... but A.I. will
exacerbate that.

01:04:30.770 --> 01:04:36.430
And that, I think, will tear
the society apart,

01:04:36.430 --> 01:04:38.930
because the rich will have just
too much,

01:04:38.930 --> 01:04:44.200
and those who are have-nots will
have perhaps very little way

01:04:44.200 --> 01:04:46.700
of digging themselves
out of the hole.

01:04:46.700 --> 01:04:50.800
And with A.I. making its impact,
it, it'll be worse, I think.

01:04:50.800 --> 01:04:56.170
♪ ♪

01:04:56.170 --> 01:05:01.870
(crowd cheering and applauding)

01:05:01.870 --> 01:05:05.000
&gt;&gt; (speaking on P.A.)

01:05:05.000 --> 01:05:08.830
I'm here today for one main
reason.

01:05:08.830 --> 01:05:12.630
To say thank you to Ohio.

01:05:12.630 --> 01:05:17.400
(crowd cheering and applauding)

01:05:17.400 --> 01:05:20.300
&gt;&gt; I think the Trump vote
was a protest.

01:05:20.300 --> 01:05:22.030
I mean that for whatever reason,

01:05:22.030 --> 01:05:25.000
whatever the hot button was
that, you know,

01:05:25.000 --> 01:05:28.800
that really hit home with these
Americans who voted for him

01:05:28.800 --> 01:05:30.800
were, it was a protest vote.

01:05:30.800 --> 01:05:34.530
They didn't like the direction
things were going.

01:05:34.530 --> 01:05:38.270
(crowd booing and shouting)

01:05:39.170 --> 01:05:40.700
I'm scared.

01:05:40.700 --> 01:05:42.900
I'm gonna be quite honest with
you, I worry about the future

01:05:42.900 --> 01:05:47.100
of not just this country,
but the, the entire globe.

01:05:47.100 --> 01:05:51.100
If we continue to go in an
automated system,

01:05:51.100 --> 01:05:52.730
what are we going to do?

01:05:52.730 --> 01:05:54.870
Now I've got a group of people
at the top

01:05:54.870 --> 01:05:57.170
that are making all the money
and I don't have anybody

01:05:57.170 --> 01:06:00.070
in the middle
that can support a family.

01:06:00.070 --> 01:06:05.330
So do we have to go to the point
where we crash to come back?

01:06:05.330 --> 01:06:06.630
And in this case,

01:06:06.630 --> 01:06:08.030
the automation's already gonna
be there,

01:06:08.030 --> 01:06:09.630
so I don't know how
you come back.

01:06:09.630 --> 01:06:11.730
I'm really worried
about where this,

01:06:11.730 --> 01:06:13.700
where this leads us
in the future.

01:06:13.700 --> 01:06:17.000
♪ ♪

01:06:27.200 --> 01:06:28.730
&gt;&gt; NARRATOR: The future is
largely being shaped

01:06:28.730 --> 01:06:32.370
by a few hugely successful
tech companies.

01:06:32.370 --> 01:06:35.700
They're constantly buying up
successful smaller companies

01:06:35.700 --> 01:06:37.900
and recruiting talent.

01:06:37.900 --> 01:06:39.830
Between the U.S. and China,

01:06:39.830 --> 01:06:42.800
they employ a great majority of
the leading A.I. researchers

01:06:42.800 --> 01:06:46.070
and scientists.

01:06:46.070 --> 01:06:48.370
In the course of amassing
such power,

01:06:48.370 --> 01:06:51.930
they've also become among the
richest companies in the world.

01:06:51.930 --> 01:06:58.130
&gt;&gt; A.I. really is the ultimate
tool of wealth creation.

01:06:58.130 --> 01:07:03.730
Think about the massive data
that, you know, Facebook has

01:07:03.730 --> 01:07:08.270
on user preferences, and how
it can very smartly target

01:07:08.270 --> 01:07:10.400
an ad that you might buy
something

01:07:10.400 --> 01:07:16.430
and get a much bigger cut that
a smaller company couldn't do.

01:07:16.430 --> 01:07:18.970
Same with Google,
same with Amazon.

01:07:18.970 --> 01:07:23.300
So it's... A.I. is a set of
tools

01:07:23.300 --> 01:07:26.500
that helps you maximize an
objective function,

01:07:26.500 --> 01:07:32.200
and that objective function
initially will simply be,

01:07:32.200 --> 01:07:34.200
make more money.

01:07:34.200 --> 01:07:36.400
&gt;&gt; NARRATOR: And it is how these
companies make that money,

01:07:36.400 --> 01:07:41.230
and how their algorithms reach
deeper and deeper into our work,

01:07:41.230 --> 01:07:42.870
our daily lives,
and our democracy,

01:07:42.870 --> 01:07:47.870
that makes many people
increasingly uncomfortable.

01:07:47.870 --> 01:07:52.330
Pedro Domingos wrote the book
"The Master Algorithm."

01:07:52.330 --> 01:07:55.470
&gt;&gt; Everywhere you go,
you generate a cloud of data.

01:07:55.470 --> 01:07:58.500
You're trailing data, everything
that you do is producing data.

01:07:58.500 --> 01:07:59.900
And then there are computers
looking at that data

01:07:59.900 --> 01:08:02.970
that are learning, and these
computers are essentially

01:08:02.970 --> 01:08:05.100
trying to serve you better.

01:08:05.100 --> 01:08:07.270
They're trying to personalize
things to you.

01:08:07.270 --> 01:08:08.900
They're trying to adapt
the world to you.

01:08:08.900 --> 01:08:10.800
So on the one hand,
this is great,

01:08:10.800 --> 01:08:12.430
because the world will get
adapted to you

01:08:12.430 --> 01:08:15.900
without you even having to
explicitly adapt it.

01:08:15.900 --> 01:08:18.600
There's also a danger, because
the entities in the companies

01:08:18.600 --> 01:08:20.100
that are in control of those
algorithms

01:08:20.100 --> 01:08:22.000
don't necessarily have the same
goals as you,

01:08:22.000 --> 01:08:24.800
and this is where I think people
need to be aware that,

01:08:24.800 --> 01:08:30.000
what's going on, so they can
have more control over it.

01:08:30.000 --> 01:08:31.630
&gt;&gt; You know, we came into this
new world thinking

01:08:31.630 --> 01:08:35.530
that we were users
of social media.

01:08:35.530 --> 01:08:37.800
It didn't occur to us
that social media

01:08:37.800 --> 01:08:40.430
was actually using us.

01:08:40.430 --> 01:08:43.830
We thought that we were
searching Google.

01:08:43.830 --> 01:08:48.700
We had no idea that Google
was searching us.

01:08:48.700 --> 01:08:50.800
&gt;&gt; NARRATOR: Shoshana Zuboff
is a Harvard Business School

01:08:50.800 --> 01:08:52.870
professor emerita.

01:08:52.870 --> 01:08:55.930
In 1988, she wrote a definitive
book called

01:08:55.930 --> 01:08:58.370
"In the Age of
the Smart Machine."

01:08:58.370 --> 01:09:01.970
For the last seven years,
she has worked on a new book,

01:09:01.970 --> 01:09:04.730
making the case that we have now
entered a new phase

01:09:04.730 --> 01:09:09.970
of the economy, which she calls
"surveillance capitalism."

01:09:09.970 --> 01:09:16.330
&gt;&gt; So, famously, industrial
capitalism claimed nature.

01:09:16.330 --> 01:09:20.300
Innocent rivers, and meadows,
and forests, and so forth,

01:09:20.300 --> 01:09:25.130
for the market dynamic to be
reborn as real estate,

01:09:25.130 --> 01:09:27.970
as land that could be sold
and purchased.

01:09:27.970 --> 01:09:32.230
Industrial capitalism claimed
work for the market dynamic

01:09:32.230 --> 01:09:35.170
to reborn, to be reborn as labor

01:09:35.170 --> 01:09:38.700
that could be sold
and purchased.

01:09:38.700 --> 01:09:40.830
Now, here comes surveillance
capitalism,

01:09:40.830 --> 01:09:47.700
following this pattern, but with
a dark and startling twist.

01:09:47.700 --> 01:09:51.700
What surveillance capitalism
claims is private,

01:09:51.700 --> 01:09:53.930
human experience.

01:09:53.930 --> 01:09:58.970
Private, human experience is
claimed as a free source

01:09:58.970 --> 01:10:05.800
of raw material, fabricated into
predictions of human behavior.

01:10:05.800 --> 01:10:09.370
And it turns out that there are
a lot of businesses

01:10:09.370 --> 01:10:14.270
that really want to know what
we will do now, soon, and later.

01:10:17.800 --> 01:10:19.430
&gt;&gt; NARRATOR: Like most people,

01:10:19.430 --> 01:10:21.470
Alastair Mactaggart
had know idea

01:10:21.470 --> 01:10:23.600
about this new surveillance
business,

01:10:23.600 --> 01:10:27.370
until one evening in 2015.

01:10:27.370 --> 01:10:30.230
&gt;&gt; I had a conversation with a
fellow who's an engineer,

01:10:30.230 --> 01:10:33.870
and I was just talking to him
one night at a,

01:10:33.870 --> 01:10:35.270
you know, a dinner,
at a cocktail party.

01:10:35.270 --> 01:10:37.470
And I... there had been
something in the press that day

01:10:37.470 --> 01:10:39.900
about privacy in the paper,
and I remember asking him--

01:10:39.900 --> 01:10:41.870
he worked for Google-- "What's
the big deal about all,

01:10:41.870 --> 01:10:44.270
why are people so worked up
about it?"

01:10:44.270 --> 01:10:45.670
And I thought it was gonna be
one of those conversations,

01:10:45.670 --> 01:10:49.330
like, with, you know, if you
ever ask an airline pilot,

01:10:49.330 --> 01:10:50.570
"Should I be worried about
flying?"

01:10:50.570 --> 01:10:52.070
and they say,
"Oh, the most dangerous part

01:10:52.070 --> 01:10:55.030
is coming to the airport,
you know, in the car."

01:10:55.030 --> 01:10:57.730
And he said, "Oh, you'd be
horrified

01:10:57.730 --> 01:10:59.900
if you knew how much we knew
about you."

01:10:59.900 --> 01:11:02.100
And I remember that kind of
stuck in my head,

01:11:02.100 --> 01:11:04.530
because it was not
what I expected.

01:11:04.530 --> 01:11:08.400
&gt;&gt; NARRATOR: That question
would change his life.

01:11:08.400 --> 01:11:09.930
A successful California real
estate developer,

01:11:09.930 --> 01:11:15.730
Mactaggart began researching
the new business model.

01:11:15.730 --> 01:11:17.870
&gt;&gt; What I've learned since is
that their entire business

01:11:17.870 --> 01:11:20.630
is learning as much about you
as they can.

01:11:20.630 --> 01:11:21.970
Everything about your thoughts,
and your desires,

01:11:21.970 --> 01:11:25.730
and your dreams,
and who your friends are,

01:11:25.730 --> 01:11:27.700
and what you're thinking, what
your private thoughts are.

01:11:27.700 --> 01:11:29.770
And with that,
that's true power.

01:11:29.770 --> 01:11:33.430
And so, I think...
I didn't know that at the time.

01:11:33.430 --> 01:11:35.470
That their entire business
is basically mining

01:11:35.470 --> 01:11:37.200
the data of your life.

01:11:37.200 --> 01:11:39.070
♪ ♪

01:11:39.070 --> 01:11:43.200
&gt;&gt; NARRATOR: Shoshana Zuboff had
been doing her own research.

01:11:43.200 --> 01:11:45.970
&gt;&gt; You know, I'd been reading
and reading and reading.

01:11:45.970 --> 01:11:48.370
From patents, to transcripts
of earnings calls,

01:11:48.370 --> 01:11:50.430
research reports.

01:11:50.430 --> 01:11:52.130
And, you know,
just literally everything,

01:11:52.130 --> 01:11:56.730
for years and years and years.

01:11:56.730 --> 01:11:57.930
&gt;&gt; NARRATOR: Her studies
included the early days

01:11:57.930 --> 01:12:00.400
of Google, started in 1998

01:12:00.400 --> 01:12:02.570
by two young Stanford grad
students,

01:12:02.570 --> 01:12:06.330
Sergey Brin and Larry Page.

01:12:06.330 --> 01:12:10.000
In the beginning, they had no
clear business model.

01:12:10.000 --> 01:12:13.900
Their unofficial motto was,
"Don't Be Evil."

01:12:13.900 --> 01:12:16.270
&gt;&gt; Right from the start,
the founders,

01:12:16.270 --> 01:12:20.170
Larry Page and Sergey Brin,
they had been very public

01:12:20.170 --> 01:12:26.200
about their antipathy
toward advertising.

01:12:26.200 --> 01:12:31.300
Advertising would distort
the internet

01:12:31.300 --> 01:12:37.800
and it would distort and
disfigure the, the purity

01:12:37.800 --> 01:12:41.700
of any search engine,
including their own.

01:12:41.700 --> 01:12:43.070
&gt;&gt; Once in love with e-commerce,

01:12:43.070 --> 01:12:46.300
Wall Street has turned its back
on the dotcoms.

01:12:46.300 --> 01:12:49.400
&gt;&gt; NARRATOR: Then came the
dotcom crash of the early 2000s.

01:12:49.400 --> 01:12:51.400
&gt;&gt; ...has left hundreds of
unprofitable internet companies

01:12:51.400 --> 01:12:54.830
begging for love and money.

01:12:54.830 --> 01:12:56.730
&gt;&gt; NARRATOR: While Google had
rapidly become the default

01:12:56.730 --> 01:12:58.830
search engine for tens of
millions of users,

01:12:58.830 --> 01:13:04.070
their investors were pressuring
them to make more money.

01:13:04.070 --> 01:13:06.100
Without a new business model,

01:13:06.100 --> 01:13:10.530
the founders knew that the young
company was in danger.

01:13:10.530 --> 01:13:14.470
&gt;&gt; In this state of emergency,
the founders decided,

01:13:14.470 --> 01:13:19.300
"We've simply got to find a way
to save this company."

01:13:19.300 --> 01:13:25.630
And so, parallel to this were
another set of discoveries,

01:13:25.630 --> 01:13:30.970
where it turns out that whenever
we search or whenever we browse,

01:13:30.970 --> 01:13:35.030
we're leaving behind traces--
digital traces--

01:13:35.030 --> 01:13:37.230
of our behavior.

01:13:37.230 --> 01:13:39.330
And those traces,
back in these days,

01:13:39.330 --> 01:13:43.500
were called digital exhaust.

01:13:43.500 --> 01:13:45.400
&gt;&gt; NARRATOR: They realized how
valuable this data could be

01:13:45.400 --> 01:13:47.700
by applying machine learning
algorithms

01:13:47.700 --> 01:13:52.570
to predict users' interests.

01:13:52.570 --> 01:13:54.670
&gt;&gt; What happened was,
they decided to turn

01:13:54.670 --> 01:13:57.630
to those data logs
in a systematic way,

01:13:57.630 --> 01:14:01.670
and to begin to use these
surplus data

01:14:01.670 --> 01:14:06.970
as a way to come up with
fine-grained predictions

01:14:06.970 --> 01:14:11.330
of what a user would click on,
what kind of ad

01:14:11.330 --> 01:14:14.230
a user would click on.

01:14:14.230 --> 01:14:18.700
And inside Google, they started
seeing these revenues

01:14:18.700 --> 01:14:22.830
pile up at a startling rate.

01:14:22.830 --> 01:14:26.000
They realized that they had to
keep it secret.

01:14:26.000 --> 01:14:28.930
They didn't want anyone to know
how much money they were making,

01:14:28.930 --> 01:14:31.500
or how they were making it.

01:14:31.500 --> 01:14:35.700
Because users had no idea that
these extra-behavioral data

01:14:35.700 --> 01:14:39.070
that told so much about them,
you know, was just out there,

01:14:39.070 --> 01:14:43.700
and now it was being used
to predict their future.

01:14:43.700 --> 01:14:46.100
&gt;&gt; NARRATOR: When Google's
I.P.O. took place

01:14:46.100 --> 01:14:47.170
just a few years later,

01:14:47.170 --> 01:14:49.700
the company had a market
capitalization

01:14:49.700 --> 01:14:53.230
of around $23 billion.

01:14:53.230 --> 01:14:56.000
Google's stock was now as
valuable as General Motors.

01:14:56.000 --> 01:14:59.100
♪ ♪

01:14:59.100 --> 01:15:02.330
&gt;&gt; And it was only when Google
went public in 2004

01:15:02.330 --> 01:15:05.600
that the numbers were released.

01:15:05.600 --> 01:15:10.600
And it's at that point that we
learn that between the year 2000

01:15:10.600 --> 01:15:14.500
and the year 2004, Google's
revenue line increased

01:15:14.500 --> 01:15:20.400
by 3,590%.

01:15:20.400 --> 01:15:22.470
&gt;&gt; Let's talk a little about
information, and search,

01:15:22.470 --> 01:15:24.630
and how people consume it.

01:15:24.630 --> 01:15:27.230
&gt;&gt; NARRATOR: By 2010, the C.E.O.
of Google, Eric Schmidt,

01:15:27.230 --> 01:15:29.570
would tell "The Atlantic"
magazine...

01:15:29.570 --> 01:15:33.230
&gt;&gt; ...is, we don't need you to
type at all.

01:15:33.230 --> 01:15:35.930
Because we know where you are,
with your permission,

01:15:35.930 --> 01:15:39.630
we know where you've been,
with your permission.

01:15:39.630 --> 01:15:41.700
We can more or less guess what
you're thinking about.

01:15:41.700 --> 01:15:44.200
(audience laughing)
Now, is that over the line?

01:15:44.200 --> 01:15:45.800
&gt;&gt; NARRATOR: Eric Schmidt
and Google declined

01:15:45.800 --> 01:15:49.370
to be interviewed
for this program.

01:15:49.370 --> 01:15:52.670
Google's new business model for
predicting users' profiles

01:15:52.670 --> 01:15:58.100
had migrated to other companies,
particularly Facebook.

01:15:58.100 --> 01:16:00.130
Roger McNamee was an early
investor

01:16:00.130 --> 01:16:02.330
and adviser to Facebook.

01:16:02.330 --> 01:16:05.900
He's now a critic, and wrote
a book about the company.

01:16:05.900 --> 01:16:08.930
He says he's concerned about how
widely companies like Facebook

01:16:08.930 --> 01:16:11.770
and Google have been casting
the net for data.

01:16:11.770 --> 01:16:13.530
&gt;&gt; And then they realized,
"Wait a minute,

01:16:13.530 --> 01:16:16.470
there's all this data in
the economy we don't have."

01:16:16.470 --> 01:16:18.700
So they went to credit card
processors,

01:16:18.700 --> 01:16:20.430
and credit rating services,

01:16:20.430 --> 01:16:23.030
and said, "We want
to buy your data."

01:16:23.030 --> 01:16:25.100
They go to health and wellness
apps and say,

01:16:25.100 --> 01:16:26.600
"Hey, you got women's
menstrual cycles?

01:16:26.600 --> 01:16:28.530
We want all that stuff."

01:16:28.530 --> 01:16:30.830
Why are they doing that?

01:16:30.830 --> 01:16:34.430
They're doing that because
behavioral prediction

01:16:34.430 --> 01:16:38.270
is about taking uncertainty
out of life.

01:16:38.270 --> 01:16:40.670
Advertising and marketing
are all about uncertainty--

01:16:40.670 --> 01:16:43.530
you never really know who's
going to buy your product.

01:16:43.530 --> 01:16:45.500
Until now.

01:16:45.500 --> 01:16:49.870
We have to recognize that we
gave technology a place

01:16:49.870 --> 01:16:55.700
in our lives
that it had not earned.

01:16:55.700 --> 01:17:00.530
That essentially, because
technology always made things

01:17:00.530 --> 01:17:03.530
better in the '50s, '60s, '70s,
'80s, and '90s,

01:17:03.530 --> 01:17:07.030
we developed a sense of
inevitability

01:17:07.030 --> 01:17:10.000
that it will always make things
better.

01:17:10.000 --> 01:17:13.630
We developed a trust, and the
industry earned good will

01:17:13.630 --> 01:17:20.330
that Facebook and Google have
cashed in.

01:17:20.330 --> 01:17:23.500
&gt;&gt; NARRATOR: The model is simply
this: provide a free service--

01:17:23.500 --> 01:17:26.630
like Facebook-- and in exchange,
you collect the data

01:17:26.630 --> 01:17:28.870
of the millions who use it.

01:17:28.870 --> 01:17:31.800
♪ ♪

01:17:31.800 --> 01:17:37.470
And every sliver of information
is valuable.

01:17:37.470 --> 01:17:41.370
&gt;&gt; It's not just what you post,
it's that you post.

01:17:41.370 --> 01:17:44.800
It's not just that you make
plans to see your friends later.

01:17:44.800 --> 01:17:47.470
It's whether you say,
"I'll see you later,"

01:17:47.470 --> 01:17:51.000
or, "I'll see you at 6:45."

01:17:51.000 --> 01:17:54.030
It's not just that you talk
about the things

01:17:54.030 --> 01:17:56.330
that you have to do today.

01:17:56.330 --> 01:17:59.230
It's whether you simply rattle
them on in a,

01:17:59.230 --> 01:18:04.500
in a rambling paragraph,
or list them as bullet points.

01:18:04.500 --> 01:18:09.030
All of these tiny signals are
the behavioral surplus

01:18:09.030 --> 01:18:13.630
that turns out to have immense
predictive value.

01:18:13.630 --> 01:18:16.300
&gt;&gt; NARRATOR: In 2010, Facebook
experimented

01:18:16.300 --> 01:18:19.070
with A.I.'s predictive powers
in what they called

01:18:19.070 --> 01:18:21.830
a "social contagion" experiment.

01:18:21.830 --> 01:18:25.470
They wanted to see if, through
online messaging,

01:18:25.470 --> 01:18:30.070
they could influence real-world
behavior.

01:18:30.070 --> 01:18:32.670
The aim was to get more people
to the polls

01:18:32.670 --> 01:18:34.530
in the 2010 midterm elections.

01:18:34.530 --> 01:18:38.000
&gt;&gt; Cleveland, I need you to keep
on fighting.

01:18:38.000 --> 01:18:41.030
I need you to keep on believing.

01:18:41.030 --> 01:18:42.530
&gt;&gt; NARRATOR: They offered
61 million users

01:18:42.530 --> 01:18:45.470
an "I voted" button together
with faces of friends

01:18:45.470 --> 01:18:47.330
who had voted.

01:18:47.330 --> 01:18:52.000
A subset of users received
just the button.

01:18:52.000 --> 01:18:56.130
In the end, they claimed to have
nudged 340,000 people to vote.

01:19:00.270 --> 01:19:03.170
They would conduct other
"massive contagion" experiments.

01:19:03.170 --> 01:19:06.600
Among them, one showing that by
adjusting their feeds,

01:19:06.600 --> 01:19:12.000
they could make users
happy or sad.

01:19:12.000 --> 01:19:13.300
&gt;&gt; When they went to write up
these findings,

01:19:13.300 --> 01:19:16.270
they boasted about two things.

01:19:16.270 --> 01:19:19.770
One was, "Oh, my goodness.

01:19:19.770 --> 01:19:24.770
Now we know that we can use cues
in the online environment

01:19:24.770 --> 01:19:28.830
to change real-world behavior.

01:19:28.830 --> 01:19:31.770
That's big news."

01:19:31.770 --> 01:19:35.600
The second thing that they
understood, and they celebrated,

01:19:35.600 --> 01:19:39.030
was that, "We can do this in a
way that bypasses

01:19:39.030 --> 01:19:43.370
the users' awareness."

01:19:43.370 --> 01:19:47.500
&gt;&gt; Private corporations have
built a corporate surveillance

01:19:47.500 --> 01:19:52.370
state without our awareness
or permission.

01:19:52.370 --> 01:19:55.230
And the systems necessary to
make it work

01:19:55.230 --> 01:19:58.430
are getting a lot better,
specifically with what are known

01:19:58.430 --> 01:20:01.500
as internet of things,
smart appliances, you know,

01:20:01.500 --> 01:20:04.430
powered by the Alexa voice
recognition system,

01:20:04.430 --> 01:20:06.870
or the Google Home system.

01:20:06.870 --> 01:20:09.700
&gt;&gt; Okay, Google,
play the morning playlist.

01:20:09.700 --> 01:20:12.200
&gt;&gt; Okay, playing morning
playlist.

01:20:12.200 --> 01:20:14.300
♪ ♪

01:20:14.300 --> 01:20:16.370
&gt;&gt; Okay, Google,
play music in all rooms.

01:20:16.370 --> 01:20:18.030
♪ ♪

01:20:18.030 --> 01:20:21.000
&gt;&gt; And those will put the
surveillance in places

01:20:21.000 --> 01:20:22.270
we've never had it before--

01:20:22.270 --> 01:20:24.800
living rooms, kitchens,
bedrooms.

01:20:24.800 --> 01:20:27.300
And I find all of that
terrifying.

01:20:27.300 --> 01:20:29.630
&gt;&gt; Okay, Google, I'm listening.

01:20:29.630 --> 01:20:31.400
&gt;&gt; NARRATOR: The companies say
they're not using the data

01:20:31.400 --> 01:20:36.770
to target ads, but helping A.I.
improve the user experience.

01:20:36.770 --> 01:20:40.030
&gt;&gt; Alexa, turn on the fan.

01:20:40.030 --> 01:20:41.500
(fan clicks on)

01:20:41.500 --> 01:20:42.670
&gt;&gt; Okay.

01:20:42.670 --> 01:20:43.830
&gt;&gt; NARRATOR: Meanwhile, they are
researching

01:20:43.830 --> 01:20:45.930
and applying for patents

01:20:45.930 --> 01:20:48.900
to expand their reach
into homes and lives.

01:20:48.900 --> 01:20:51.230
&gt;&gt; Alexa, take a video.

01:20:51.230 --> 01:20:52.670
(camera chirps)

01:20:52.670 --> 01:20:54.570
&gt;&gt; The more and more that you
use spoken interfaces--

01:20:54.570 --> 01:20:57.800
so smart speakers-- they're
being trained

01:20:57.800 --> 01:21:00.770
not just to recognize
who you are,

01:21:00.770 --> 01:21:03.970
but they're starting to take
baselines

01:21:03.970 --> 01:21:09.770
and comparing changes over time.

01:21:09.770 --> 01:21:12.970
So does your cadence increase
or decrease?

01:21:12.970 --> 01:21:15.600
Are you sneezing
while you're talking?

01:21:15.600 --> 01:21:18.730
Is your voice a little wobbly?

01:21:18.730 --> 01:21:21.570
The purpose of doing this is
to understand

01:21:21.570 --> 01:21:24.430
more about you in real time.

01:21:24.430 --> 01:21:27.830
So that a system could make
inferences, perhaps,

01:21:27.830 --> 01:21:30.600
like, do you have a cold?

01:21:30.600 --> 01:21:33.370
Are you in a manic phase?

01:21:33.370 --> 01:21:35.100
Are you feeling depressed?

01:21:35.100 --> 01:21:38.700
So that is an extraordinary
amount of information

01:21:38.700 --> 01:21:41.670
that can be gleaned by you
simply waking up

01:21:41.670 --> 01:21:45.330
and asking your smart speaker,
"What's the weather today?"

01:21:45.330 --> 01:21:47.430
&gt;&gt; Alexa, what's the weather
for tonight?

01:21:47.430 --> 01:21:50.630
&gt;&gt; Currently, in Pasadena, it's
58 degrees with cloudy skies.

01:21:50.630 --> 01:21:52.900
&gt;&gt; Inside it is, then.

01:21:52.900 --> 01:21:54.700
Dinner!

01:21:54.700 --> 01:21:57.630
&gt;&gt; The point is that this
is the same

01:21:57.630 --> 01:22:01.800
micro-behavioral targeting that
is directed

01:22:01.800 --> 01:22:08.670
toward individuals based on
intimate, detailed understanding

01:22:08.670 --> 01:22:11.000
of personalities.

01:22:11.000 --> 01:22:15.600
So this is precisely what
Cambridge Analytica did,

01:22:15.600 --> 01:22:19.300
simply pivoting from
the advertisers

01:22:19.300 --> 01:22:23.500
to the political outcomes.

01:22:23.500 --> 01:22:26.170
&gt;&gt; NARRATOR: The Cambridge
Analytica scandal of 2018

01:22:26.170 --> 01:22:29.970
engulfed Facebook, forcing
Mark Zuckerberg to appear

01:22:29.970 --> 01:22:32.900
before Congress to explain how
the data

01:22:32.900 --> 01:22:36.300
of up to 87 million Facebook
users had been harvested

01:22:36.300 --> 01:22:42.870
by a political consulting
company based in the U.K.

01:22:42.870 --> 01:22:45.500
The purpose was to target
and manipulate voters

01:22:45.500 --> 01:22:48.170
in the 2016 presidential
campaign,

01:22:48.170 --> 01:22:51.800
as well as the Brexit
referendum.

01:22:51.800 --> 01:22:53.870
Cambridge Analytica had been
largely funded

01:22:53.870 --> 01:22:58.830
by conservative hedge fund
billionaire Robert Mercer.

01:22:58.830 --> 01:23:02.370
&gt;&gt; And now we know that any
billionaire with enough money,

01:23:02.370 --> 01:23:04.200
who can buy the data,

01:23:04.200 --> 01:23:07.230
buy the machine intelligence
capabilities,

01:23:07.230 --> 01:23:10.700
buy the skilled data scientists,

01:23:10.700 --> 01:23:16.330
you know, they too can
commandeer the public,

01:23:16.330 --> 01:23:23.370
and infect and infiltrate and
upend our democracy

01:23:23.370 --> 01:23:27.770
with the same methodologies that
surveillance capitalism

01:23:27.770 --> 01:23:32.270
uses every single day.

01:23:32.270 --> 01:23:35.070
&gt;&gt; We didn't take a broad enough
view of our responsibility,

01:23:35.070 --> 01:23:37.230
and that was a big mistake.

01:23:37.230 --> 01:23:40.830
And it was my mistake,
and I'm sorry.

01:23:40.830 --> 01:23:41.770
&gt;&gt; NARRATOR:
Zuckerberg has apologized

01:23:41.770 --> 01:23:44.370
for numerous violations of
privacy,

01:23:44.370 --> 01:23:47.130
and his company was recently
fined $5 billion

01:23:47.130 --> 01:23:50.300
by the Federal Trade Commission.

01:23:50.300 --> 01:23:53.230
He has said Facebook will now
make data protection a priority,

01:23:53.230 --> 01:23:56.800
and the company has suspended
tens of thousands

01:23:56.800 --> 01:23:59.400
of third-party apps from its
platform

01:23:59.400 --> 01:24:02.930
as a result of an internal
investigation.

01:24:02.930 --> 01:24:06.870
&gt;&gt; You know, I wish I could say
that after Cambridge Analytica,

01:24:06.870 --> 01:24:09.030
we've learned our lesson and
that everything will be much

01:24:09.030 --> 01:24:12.930
better after that, but I'm
afraid the opposite is true.

01:24:12.930 --> 01:24:14.900
In some ways, Cambridge
Analytica was using tools

01:24:14.900 --> 01:24:16.830
that were ten years old.

01:24:16.830 --> 01:24:18.570
It was really, in some ways,
old-school,

01:24:18.570 --> 01:24:20.670
first-wave data science.

01:24:20.670 --> 01:24:22.270
What we're looking at now,
with current tools

01:24:22.270 --> 01:24:26.270
and machine learning, is that
the ability for manipulation,

01:24:26.270 --> 01:24:28.930
both in terms of elections
and opinions,

01:24:28.930 --> 01:24:31.700
but more broadly,
just how information travels,

01:24:31.700 --> 01:24:34.630
That is a much bigger problem,

01:24:34.630 --> 01:24:36.200
and certainly much more serious
than what we faced

01:24:36.200 --> 01:24:40.070
with Cambridge Analytica.

01:24:40.070 --> 01:24:43.670
&gt;&gt; NARRATOR: A.I. pioneer Yoshua
Bengio also has concerns

01:24:43.670 --> 01:24:48.470
about how his algorithms
are being used.

01:24:48.470 --> 01:24:51.600
&gt;&gt; So the A.I.s are tools.

01:24:51.600 --> 01:24:56.330
And they will serve the people
who control those tools.

01:24:56.330 --> 01:25:01.700
If those people's interests go
against the, the values

01:25:01.700 --> 01:25:04.670
of democracy, then democracy is
in danger.

01:25:04.670 --> 01:25:10.430
So I believe that scientists
who contribute to science,

01:25:10.430 --> 01:25:14.670
when that science can or will
have an impact on society,

01:25:14.670 --> 01:25:17.670
those scientists have a
responsibility.

01:25:17.670 --> 01:25:19.700
It's a little bit like the
physicists of,

01:25:19.700 --> 01:25:21.800
around the Second World War,

01:25:21.800 --> 01:25:25.100
who rose up to tell
the governments,

01:25:25.100 --> 01:25:29.130
"Wait, nuclear power
can be dangerous

01:25:29.130 --> 01:25:31.930
and nuclear war can be really,
really destructive."

01:25:31.930 --> 01:25:36.400
And today, the equivalent of a
physicist of the '40s and '50s

01:25:36.400 --> 01:25:38.730
and '60s are,
are the computer scientists

01:25:38.730 --> 01:25:41.430
who are doing machine learning
and A.I.

01:25:41.430 --> 01:25:45.000
♪ ♪

01:25:45.000 --> 01:25:46.300
&gt;&gt; NARRATOR: One person who
wanted to do something

01:25:46.300 --> 01:25:49.330
about the dangers was not
a computer scientist,

01:25:49.330 --> 01:25:53.330
but an ordinary citizen.

01:25:53.330 --> 01:25:55.600
Alastair Mactaggart was alarmed.

01:25:55.600 --> 01:25:58.800
&gt;&gt; Voting is, for me,
the most alarming one.

01:25:58.800 --> 01:26:00.570
If less than 100,000 votes
separated

01:26:00.570 --> 01:26:03.330
the last two candidates in the
last presidential election,

01:26:03.330 --> 01:26:06.900
in three states...

01:26:06.900 --> 01:26:10.430
&gt;&gt; NARRATOR: He began a solitary
campaign.

01:26:10.430 --> 01:26:12.100
&gt;&gt; We're talking about
convincing a relatively tiny

01:26:12.100 --> 01:26:14.900
fraction of the voters
in a very...

01:26:14.900 --> 01:26:17.700
in a handful of states
to either come out and vote

01:26:17.700 --> 01:26:18.970
or stay home.

01:26:18.970 --> 01:26:21.200
And remember, these companies
know everybody intimately.

01:26:21.200 --> 01:26:24.470
They know who's a racist,
who's a misogynist,

01:26:24.470 --> 01:26:26.770
who's a homophobe,
who's a conspiracy theorist.

01:26:26.770 --> 01:26:28.770
They know the lazy people and
the gullible people.

01:26:28.770 --> 01:26:31.130
They have access to the greatest
trove of personal information

01:26:31.130 --> 01:26:32.670
that's ever been assembled.

01:26:32.670 --> 01:26:35.370
They have the world's best data
scientists.

01:26:35.370 --> 01:26:37.430
And they have essentially
a frictionless way

01:26:37.430 --> 01:26:39.670
of communicating with you.

01:26:39.670 --> 01:26:43.070
This is power.

01:26:43.070 --> 01:26:44.800
&gt;&gt; NARRATOR: Mactaggart started
a signature drive

01:26:44.800 --> 01:26:47.000
for a California ballot
initiative,

01:26:47.000 --> 01:26:51.230
for a law to give consumers
control of their digital data.

01:26:51.230 --> 01:26:54.670
In all, he would spend
$4 million of his own money

01:26:54.670 --> 01:26:58.600
in an effort to rein in the
goliaths of Silicon Valley.

01:26:58.600 --> 01:27:02.570
Google, Facebook, AT&amp;T,
and Comcast

01:27:02.570 --> 01:27:06.170
all opposed his initiative.

01:27:06.170 --> 01:27:09.200
&gt;&gt; I'll tell you, I was scared.
Fear.

01:27:09.200 --> 01:27:12.000
Fear of looking like
a world-class idiot.

01:27:12.000 --> 01:27:14.930
The market cap of all the firms
arrayed against me were,

01:27:14.930 --> 01:27:19.800
was over $6 trillion.

01:27:19.800 --> 01:27:21.600
&gt;&gt; NARRATOR: He needed 500,000
signatures

01:27:21.600 --> 01:27:25.070
to get his initiative
on the ballot.

01:27:25.070 --> 01:27:27.370
He got well over 600,000.

01:27:27.370 --> 01:27:33.530
Polls showed 80% approval
for a privacy law.

01:27:33.530 --> 01:27:37.670
That made the politicians in
Sacramento pay attention.

01:27:37.670 --> 01:27:40.030
So Mactaggart decided that
because he was holding

01:27:40.030 --> 01:27:44.100
a strong hand, it was worth
negotiating with them.

01:27:44.100 --> 01:27:46.530
&gt;&gt; And if AB-375 passes
by tomorrow

01:27:46.530 --> 01:27:48.230
and is signed into law
by the governor,

01:27:48.230 --> 01:27:49.770
we will withdraw the initiative.

01:27:49.770 --> 01:27:51.270
Our deadline to do so is
tomorrow at 5:00.

01:27:51.270 --> 01:27:53.470
&gt;&gt; NARRATOR:
At the very last moment,

01:27:53.470 --> 01:27:55.600
a new law was rushed to the
floor of the state house.

01:27:55.600 --> 01:27:57.470
&gt;&gt; Everyone take their seats,
please.

01:27:57.470 --> 01:28:01.800
Mr. Secretary,
please call the roll.

01:28:01.800 --> 01:28:05.900
&gt;&gt; The voting starts.
&gt;&gt; Alan, aye.

01:28:05.900 --> 01:28:07.570
&gt;&gt; And the first guy,
I think, was a Republican,

01:28:07.570 --> 01:28:08.870
and he voted for it.

01:28:08.870 --> 01:28:10.600
And everybody had said the
Republicans won't vote for it

01:28:10.600 --> 01:28:11.670
because it has this private
right of action,

01:28:11.670 --> 01:28:13.830
where consumers can sue.

01:28:13.830 --> 01:28:15.530
And the guy in the Senate,
he calls the name.

01:28:15.530 --> 01:28:16.770
&gt;&gt; Aye, Roth.

01:28:16.770 --> 01:28:17.830
Aye, Skinner.

01:28:17.830 --> 01:28:19.000
Aye, Stern.

01:28:19.000 --> 01:28:20.800
Aye, Stone.

01:28:20.800 --> 01:28:23.600
&gt;&gt; You can see down below,
and everyone went green,

01:28:23.600 --> 01:28:26.270
and then it passed unanimously.

01:28:26.270 --> 01:28:29.770
&gt;&gt; Ayes 36; No zero,
the measure passes.

01:28:29.770 --> 01:28:32.200
Immediate transmittal to the...

01:28:32.200 --> 01:28:34.530
&gt;&gt; So I was blown away.

01:28:34.530 --> 01:28:36.500
It was, it was a day I will
never forget.

01:28:41.770 --> 01:28:43.630
So in January, next year,
you as a California resident

01:28:43.630 --> 01:28:45.900
will have the right to go to any
company and say,

01:28:45.900 --> 01:28:47.270
"What have you collected on me
in the last 12 years...

01:28:47.270 --> 01:28:48.700
12 months?

01:28:48.700 --> 01:28:51.370
What of my personal information
do you have?"

01:28:51.370 --> 01:28:52.300
So that's the first right.

01:28:52.300 --> 01:28:54.200
It's right of... we call that
the right to know.

01:28:54.200 --> 01:28:56.230
The second is the right
to say no.

01:28:56.230 --> 01:28:59.030
And that's the right to go to
any company and click a button,

01:28:59.030 --> 01:29:00.930
on any page where they're
collecting your information,

01:29:00.930 --> 01:29:03.200
and say, "Do not sell
my information."

01:29:03.200 --> 01:29:06.430
More importantly, we require
that they honor

01:29:06.430 --> 01:29:09.370
what's called a third-party
opt-out.

01:29:09.370 --> 01:29:11.130
You will click once
in your browser,

01:29:11.130 --> 01:29:13.830
"Don't sell my information,"

01:29:13.830 --> 01:29:18.230
and it will then send the signal
to every single website

01:29:18.230 --> 01:29:21.400
that you visit: "Don't sell
this person's information."

01:29:21.400 --> 01:29:22.970
And that's gonna have a huge
impact on the spread

01:29:22.970 --> 01:29:25.530
of your information
across the internet.

01:29:25.530 --> 01:29:27.900
&gt;&gt; NARRATOR: The tech companies
had been publicly cautious,

01:29:27.900 --> 01:29:31.500
but privately alarmed
about regulation.

01:29:31.500 --> 01:29:34.400
Then one tech giant came on
board in support

01:29:34.400 --> 01:29:37.000
of Mactaggart's efforts.

01:29:37.000 --> 01:29:39.670
&gt;&gt; I find the reaction among
other tech companies to,

01:29:39.670 --> 01:29:42.730
at this point, be pretty much
all over the place.

01:29:42.730 --> 01:29:45.970
Some people are saying,
"You're right to raise this.

01:29:45.970 --> 01:29:47.430
These are good ideas."

01:29:47.430 --> 01:29:49.100
Some people say, "We're not sure
these are good ideas,

01:29:49.100 --> 01:29:50.870
but you're right to raise it,"

01:29:50.870 --> 01:29:54.300
and some people are saying,
"We don't want regulation."

01:29:54.300 --> 01:29:56.970
And so, you know, we have
conversations with people

01:29:56.970 --> 01:30:00.030
where we point out that the auto
industry is better

01:30:00.030 --> 01:30:03.130
because there are
safety standards.

01:30:03.130 --> 01:30:05.430
Pharmaceuticals,
even food products,

01:30:05.430 --> 01:30:08.200
all of these industries are
better because the public

01:30:08.200 --> 01:30:11.000
has confidence in the products,

01:30:11.000 --> 01:30:14.930
in part because of a mixture
of responsible companies

01:30:14.930 --> 01:30:19.030
and responsible regulation.

01:30:19.030 --> 01:30:21.400
&gt;&gt; NARRATOR: But the lobbyists
for big tech have been working

01:30:21.400 --> 01:30:24.270
the corridors in Washington.

01:30:24.270 --> 01:30:26.300
They're looking for
a more lenient

01:30:26.300 --> 01:30:29.670
national privacy standard,
one that could perhaps override

01:30:29.670 --> 01:30:33.300
the California law
and others like it.

01:30:33.300 --> 01:30:34.570
But while hearings are held,

01:30:34.570 --> 01:30:37.170
and anti-trust legislation
threatened,

01:30:37.170 --> 01:30:40.530
the problem is that A.I.
has already spread so far

01:30:40.530 --> 01:30:43.630
into our lives and work.

01:30:43.630 --> 01:30:46.100
&gt;&gt; Well, it's in healthcare,
it's in education,

01:30:46.100 --> 01:30:48.670
it's in criminal justice,
it's in the experience

01:30:48.670 --> 01:30:51.230
of shopping as you walk down
the street.

01:30:51.230 --> 01:30:54.700
It has pervaded so many elements
of everyday life,

01:30:54.700 --> 01:30:57.370
and in a way that, in many
cases, is completely opaque

01:30:57.370 --> 01:30:59.230
to people.

01:30:59.230 --> 01:31:00.830
While we can see a phone and
look at it and we know that

01:31:00.830 --> 01:31:02.970
there's some A.I. technology
behind it,

01:31:02.970 --> 01:31:05.200
many of us don't know that when
we go for a job interview

01:31:05.200 --> 01:31:07.030
and we sit down
and we have a conversation,

01:31:07.030 --> 01:31:09.970
that we're being filmed, and
that our micro expressions

01:31:09.970 --> 01:31:12.670
are being analyzed
by hiring companies.

01:31:12.670 --> 01:31:14.700
Or that if you're in the
criminal justice system,

01:31:14.700 --> 01:31:16.670
that there are risk assessment
algorithms

01:31:16.670 --> 01:31:18.830
that are deciding
your "risk number,"

01:31:18.830 --> 01:31:22.530
which could determine whether
or not you receive bail or not.

01:31:22.530 --> 01:31:24.770
These are systems which, in many
cases, are hidden

01:31:24.770 --> 01:31:28.400
in the back end of our sort
of social institutions.

01:31:28.400 --> 01:31:29.400
And so, one of the big
challenges we have is,

01:31:29.400 --> 01:31:31.970
how do we make that more
apparent?

01:31:31.970 --> 01:31:32.830
How do we make it transparent?

01:31:32.830 --> 01:31:36.700
And how do we make it
accountable?

01:31:36.700 --> 01:31:39.830
&gt;&gt; For a very long time,
we have felt like as humans,

01:31:39.830 --> 01:31:43.130
as Americans,
we have full agency

01:31:43.130 --> 01:31:48.830
in determining our own futures--
what we read, what we see,

01:31:48.830 --> 01:31:50.330
we're in charge.

01:31:50.330 --> 01:31:53.070
What Cambridge Analytica taught
us,

01:31:53.070 --> 01:31:55.770
and what Facebook continues
to teach us,

01:31:55.770 --> 01:31:58.830
is that we don't have agency.

01:31:58.830 --> 01:32:00.470
We're not in charge.

01:32:00.470 --> 01:32:05.330
This is machines that are
automating some of our skills,

01:32:05.330 --> 01:32:09.330
but have made decisions about
who...

01:32:09.330 --> 01:32:12.630
Who we are.

01:32:12.630 --> 01:32:16.300
And they're using that
information to tell others

01:32:16.300 --> 01:32:19.570
the story of us.

01:32:19.570 --> 01:32:22.130
♪ ♪

01:32:32.970 --> 01:32:35.470
&gt;&gt; NARRATOR: In China,
in the age of A.I.,

01:32:35.470 --> 01:32:38.130
there's no doubt
about who is in charge.

01:32:38.130 --> 01:32:41.370
In an authoritarian state,
social stability

01:32:41.370 --> 01:32:43.770
is the watchword
of the government.

01:32:43.770 --> 01:32:47.930
(whistle blowing)

01:32:47.930 --> 01:32:51.070
And artificial intelligence has
increased its ability to scan

01:32:51.070 --> 01:32:54.430
the country for signs of unrest.

01:32:54.430 --> 01:32:57.470
(whistle blowing)

01:32:57.470 --> 01:33:00.430
It's been projected that over
600 million cameras

01:33:00.430 --> 01:33:04.700
will be deployed by 2020.

01:33:04.700 --> 01:33:07.730
Here, they may be used to
discourage jaywalking.

01:33:07.730 --> 01:33:10.400
But they also serve to remind
people

01:33:10.400 --> 01:33:14.830
that the state is watching.

01:33:14.830 --> 01:33:18.030
&gt;&gt; And now, there is a project
called Sharp Eyes,

01:33:18.030 --> 01:33:22.670
which is putting camera
on every major street

01:33:22.670 --> 01:33:29.730
and the corner of every village
in China-- meaning everywhere.

01:33:29.730 --> 01:33:33.530
Matching with the most advanced
artificial intelligence

01:33:33.530 --> 01:33:36.830
algorithm, which they can
actually use this data,

01:33:36.830 --> 01:33:39.470
real-time data, to pick up
a face or pick up a action.

01:33:39.470 --> 01:33:42.330
♪ ♪

01:33:42.330 --> 01:33:44.370
&gt;&gt; NARRATOR: Frequent security
expos feature companies

01:33:44.370 --> 01:33:48.530
like Megvii and its facial-
recognition technology.

01:33:48.530 --> 01:33:51.970
They show off cameras with A.I.
that can track cars,

01:33:51.970 --> 01:33:54.800
and identify individuals
by face,

01:33:54.800 --> 01:33:58.270
or just by the way they walk.

01:33:58.270 --> 01:34:02.130
&gt;&gt; The place is just filled with
these screens where you can see

01:34:02.130 --> 01:34:04.530
the computers are actually
reading people's faces

01:34:04.530 --> 01:34:07.630
and trying to digest that data,
and basically track

01:34:07.630 --> 01:34:09.700
and identify who each person is.

01:34:09.700 --> 01:34:11.600
And it's incredible to see so
many,

01:34:11.600 --> 01:34:12.870
because just two
or three years ago,

01:34:12.870 --> 01:34:14.700
we hardly saw
that kind of thing.

01:34:14.700 --> 01:34:16.700
So, a big part of it is
government spending.

01:34:16.700 --> 01:34:18.370
And so the technology's really
taken off,

01:34:18.370 --> 01:34:21.530
and a lot of companies have
started to sort of glom onto

01:34:21.530 --> 01:34:25.700
this idea that this
is the future.

01:34:25.700 --> 01:34:29.470
&gt;&gt; China is on its way
to building

01:34:29.470 --> 01:34:32.330
a total surveillance state.

01:34:32.330 --> 01:34:33.830
&gt;&gt; NARRATOR: And this is the
test lab

01:34:33.830 --> 01:34:36.370
for the surveillance state.

01:34:36.370 --> 01:34:40.170
Here, in the far northwest of
China,

01:34:40.170 --> 01:34:41.830
is the autonomous region
of Xinjiang.

01:34:41.830 --> 01:34:45.170
Of the 25 million people
who live here,

01:34:45.170 --> 01:34:48.170
almost half are a Muslim Turkic
speaking people

01:34:48.170 --> 01:34:52.400
called the Uighurs.

01:34:52.400 --> 01:34:53.900
(people shouting)

01:34:53.900 --> 01:34:57.670
In 2009, tensions with local
Han Chinese led to protests

01:34:57.670 --> 01:35:01.300
and then riots in the capital,
Urumqi.

01:35:01.300 --> 01:35:04.200
(people shouting, guns firing)

01:35:04.200 --> 01:35:05.670
(people shouting)

01:35:08.200 --> 01:35:11.200
As the conflict has grown,
the authorities have brought in

01:35:11.200 --> 01:35:13.670
more police,
and deployed extensive

01:35:13.670 --> 01:35:17.530
surveillance technology.

01:35:17.530 --> 01:35:20.700
That data feeds an A.I. system
that the government claims

01:35:20.700 --> 01:35:24.300
can predict individuals prone
to "terrorism"

01:35:24.300 --> 01:35:27.570
and detect those in need of
"re-education"

01:35:27.570 --> 01:35:30.730
in scores of recently
built camps.

01:35:30.730 --> 01:35:35.700
It is a campaign that has
alarmed human rights groups.

01:35:35.700 --> 01:35:39.100
&gt;&gt; Chinese authorities are,
without any legal basis,

01:35:39.100 --> 01:35:42.770
arbitrarily detaining up
to a million Turkic Muslims

01:35:42.770 --> 01:35:44.800
simply on the basis
of their identity.

01:35:44.800 --> 01:35:49.230
But even outside the facilities
in which these people

01:35:49.230 --> 01:35:51.300
are being held, most of the
population there

01:35:51.300 --> 01:35:53.470
is being subjected to
extraordinary levels

01:35:53.470 --> 01:35:58.470
of high-tech surveillance such
that almost no aspect of life

01:35:58.470 --> 01:36:01.100
anymore, you know, takes place
outside

01:36:01.100 --> 01:36:02.600
the state's line of sight.

01:36:02.600 --> 01:36:06.230
And so the kinds of behavior
that's now being monitored--

01:36:06.230 --> 01:36:07.770
you know, which language do you
speak at home,

01:36:07.770 --> 01:36:09.530
whether you're talking to your
relatives

01:36:09.530 --> 01:36:13.330
in other countries,
how often you pray--

01:36:13.330 --> 01:36:16.230
that information is now being
hoovered up

01:36:16.230 --> 01:36:19.230
and used to decide whether
people should be subjected

01:36:19.230 --> 01:36:21.800
to political re-education
in these camps.

01:36:21.800 --> 01:36:24.570
&gt;&gt; NARRATOR: There have been
reports of torture

01:36:24.570 --> 01:36:27.000
and deaths in the camps.

01:36:27.000 --> 01:36:28.800
And for Uighurs on the outside,

01:36:28.800 --> 01:36:31.670
Xinjiang has already been
described

01:36:31.670 --> 01:36:34.770
as an "open-air prison."

01:36:34.770 --> 01:36:36.930
&gt;&gt; Trying to have a normal life
as a Uighur

01:36:36.930 --> 01:36:40.430
is impossible both inside
and outside of China.

01:36:40.430 --> 01:36:43.530
Just imagine, while you're on
your way to work,

01:36:43.530 --> 01:36:47.600
police subject you to scan
your I.D.,

01:36:47.600 --> 01:36:51.770
forcing you to lift your chin,
while machines take your photo

01:36:51.770 --> 01:36:54.970
and wait... you wait until you
find out if you can go.

01:36:54.970 --> 01:36:59.100
Imagine police take your phone
and run data scan,

01:36:59.100 --> 01:37:02.500
and force you to install
compulsory software

01:37:02.500 --> 01:37:07.630
allowing your phone calls and
messages to be monitored.

01:37:07.630 --> 01:37:09.730
&gt;&gt; NARRATOR: Nury Turkel, a
lawyer and a prominent

01:37:09.730 --> 01:37:14.470
Uighur activist, addresses a
demonstration in Washington, DC.

01:37:14.470 --> 01:37:18.700
Many among the Uighur diaspora
have lost all contact

01:37:18.700 --> 01:37:21.030
with their families back home.

01:37:21.030 --> 01:37:26.100
Turkel warns that this dystopian
deployment of new technology

01:37:26.100 --> 01:37:29.330
is a demonstration project
for authoritarian regimes

01:37:29.330 --> 01:37:31.430
around the world.

01:37:31.430 --> 01:37:35.430
&gt;&gt; They have a bar codes in
somebody's home doors

01:37:35.430 --> 01:37:39.730
to identify what kind of citizen
that he is.

01:37:39.730 --> 01:37:42.670
What we're talking about is a
collective punishment

01:37:42.670 --> 01:37:45.100
of an ethnic group.

01:37:45.100 --> 01:37:48.200
Not only that, the Chinese
government has been promoting

01:37:48.200 --> 01:37:53.100
its methods, its technology,
it is...

01:37:53.100 --> 01:37:58.630
to other countries, namely
Pakistan, Venezuela, Sudan,

01:37:58.630 --> 01:38:04.400
and others to utilize, to
squelch political resentment

01:38:04.400 --> 01:38:07.970
or prevent a political upheaval
in their various societies.

01:38:07.970 --> 01:38:10.430
♪ ♪

01:38:10.430 --> 01:38:13.500
&gt;&gt; NARRATOR: China has a grand
scheme to spread its technology

01:38:13.500 --> 01:38:15.470
and influence around the world.

01:38:15.470 --> 01:38:19.770
Launched in 2013, it started
along the old Silk Road

01:38:19.770 --> 01:38:23.270
out of Xinjiang,
and now goes far beyond.

01:38:23.270 --> 01:38:29.570
It's called "the Belt and Road
Initiative."

01:38:29.570 --> 01:38:31.370
&gt;&gt; So effectively
what the Belt and Road

01:38:31.370 --> 01:38:35.630
is is China's attempt to,
via spending and investment,

01:38:35.630 --> 01:38:37.700
project its influence
all over the world.

01:38:37.700 --> 01:38:39.830
And we've seen, you know,
massive infrastructure projects

01:38:39.830 --> 01:38:43.300
going in in places like
Pakistan, in, in Venezuela,

01:38:43.300 --> 01:38:45.330
in Ecuador, in Bolivia--

01:38:45.330 --> 01:38:47.400
you know, all over the world,
Argentina,

01:38:47.400 --> 01:38:49.630
in America's backyard,
in Africa.

01:38:49.630 --> 01:38:51.530
Africa's been a huge place.

01:38:51.530 --> 01:38:54.230
And what the Belt and Road
ultimately does is, it attempts

01:38:54.230 --> 01:38:56.400
to kind of create a political
leverage

01:38:56.400 --> 01:39:00.070
for the Chinese spending
campaign all over the globe.

01:39:00.070 --> 01:39:03.600
&gt;&gt; NARRATOR: Like Xi Jinping's
2018 visit to Senegal,

01:39:03.600 --> 01:39:06.700
where Chinese contractors had
just built a new stadium,

01:39:06.700 --> 01:39:10.970
arranged loans for a new
infrastructure development,

01:39:10.970 --> 01:39:13.270
and, said the Foreign Ministry,

01:39:13.270 --> 01:39:16.370
there would be help
"maintaining social stability."

01:39:16.370 --> 01:39:19.470
&gt;&gt; As China comes into these
countries and provides

01:39:19.470 --> 01:39:21.770
these loans, what you end up
with is Chinese technology

01:39:21.770 --> 01:39:24.430
being sold and built out by,
you know, by Chinese companies

01:39:24.430 --> 01:39:26.370
in these countries.

01:39:26.370 --> 01:39:27.500
We've started to see it already
in terms

01:39:27.500 --> 01:39:29.170
of surveillance systems.

01:39:29.170 --> 01:39:31.170
Not the kind of high-level A.I.
stuff yet, but, you know,

01:39:31.170 --> 01:39:32.600
lower-level, camera-based,
you know,

01:39:32.600 --> 01:39:36.670
manual sort of observation-type
things all over.

01:39:36.670 --> 01:39:38.200
You know, you see it in
Cambodia, you see it in Ecuador,

01:39:38.200 --> 01:39:39.770
you see it in Venezuela.

01:39:39.770 --> 01:39:42.570
And what they do is, they sell
a dam, sell some other stuff,

01:39:42.570 --> 01:39:44.100
and they say, "You know,
by the way, we can give you

01:39:44.100 --> 01:39:46.730
these camera systems and,
for your emergency response.

01:39:46.730 --> 01:39:49.000
And it'll cost you $300 million,

01:39:49.000 --> 01:39:50.500
and we'll build a ton of
cameras,

01:39:50.500 --> 01:39:52.800
and we'll build you a kind of,
you know, a main center

01:39:52.800 --> 01:39:55.100
where you have police who can
watch these cameras."

01:39:55.100 --> 01:39:57.870
And that's going in all over
the world already.

01:39:57.870 --> 01:40:03.570
♪ ♪

01:40:03.570 --> 01:40:06.600
&gt;&gt; There are 58 countries that
are starting to plug in

01:40:06.600 --> 01:40:10.230
to China's vision of artificial
intelligence.

01:40:10.230 --> 01:40:15.300
Which means effectively that
China is in the process

01:40:15.300 --> 01:40:17.770
of raising a bamboo curtain.

01:40:17.770 --> 01:40:20.770
One that does not need to...

01:40:20.770 --> 01:40:24.130
One that is sort of
all-encompassing,

01:40:24.130 --> 01:40:26.700
that has shared resources,

01:40:26.700 --> 01:40:28.600
shared telecommunications
systems,

01:40:28.600 --> 01:40:31.970
shared infrastructure,
shared digital systems--

01:40:31.970 --> 01:40:35.400
even shared mobile-phone
technologies--

01:40:35.400 --> 01:40:38.700
that is, that is quickly going
up all around the world

01:40:38.700 --> 01:40:41.700
to the exclusion of us
in the West.

01:40:41.700 --> 01:40:43.170
&gt;&gt; Well, one of the things
I worry about the most

01:40:43.170 --> 01:40:45.130
is that the world
is gonna split in two,

01:40:45.130 --> 01:40:47.230
and that there will be
a Chinese tech sector

01:40:47.230 --> 01:40:48.970
and there will be an
American tech sector.

01:40:48.970 --> 01:40:51.830
And countries will effectively
get to choose

01:40:51.830 --> 01:40:53.170
which one they want.

01:40:53.170 --> 01:40:55.770
It'll be kind of like the Cold
War, where you decide,

01:40:55.770 --> 01:40:57.970
"Oh, are we gonna align
with the Soviet Union

01:40:57.970 --> 01:40:59.570
or are we gonna align
with the United States?"

01:40:59.570 --> 01:41:02.200
And the Third World gets to
choose this or that.

01:41:02.200 --> 01:41:06.000
And that's not a world that's
good for anybody.

01:41:06.000 --> 01:41:09.130
&gt;&gt; The markets in Asia and the
U.S. falling sharply

01:41:09.130 --> 01:41:11.470
on news that a top Chinese
executive

01:41:11.470 --> 01:41:13.100
has been arrested in Canada.

01:41:13.100 --> 01:41:14.300
Her name is Sabrina Meng.

01:41:14.300 --> 01:41:19.530
She is the CFO of the Chinese
telecom Huawei.

01:41:19.530 --> 01:41:21.270
&gt;&gt; NARRATOR: News of the
dramatic arrest of an important

01:41:21.270 --> 01:41:24.530
Huawei executive was ostensibly
about the company

01:41:24.530 --> 01:41:26.430
doing business with Iran.

01:41:26.430 --> 01:41:29.600
But it seemed to be more about
American distrust

01:41:29.600 --> 01:41:32.600
of the company's technology.

01:41:32.600 --> 01:41:33.900
From its headquarters
in southern China--

01:41:33.900 --> 01:41:38.930
designed to look like fanciful
European capitals--

01:41:38.930 --> 01:41:41.570
Huawei is the second-biggest
seller of smartphones,

01:41:41.570 --> 01:41:45.630
and the world leader
in building 5G networks,

01:41:45.630 --> 01:41:50.970
the high-speed backbone
for the age of A.I.

01:41:50.970 --> 01:41:53.070
Huawei's C.E.O.,
a former officer

01:41:53.070 --> 01:41:54.970
in the People's Liberation Army,

01:41:54.970 --> 01:41:57.830
was defiant about
the American actions.

01:41:57.830 --> 01:41:59.470
&gt;&gt; (speaking Mandarin)

01:41:59.470 --> 01:42:02.600
(translated): There's no way
the U.S. can crush us.

01:42:02.600 --> 01:42:08.500
The world needs Huawei because
we are more advanced.

01:42:08.500 --> 01:42:12.900
If the lights go out in the
West, the East will still shine.

01:42:12.900 --> 01:42:16.270
And if the North goes dark,
then there is still the South.

01:42:16.270 --> 01:42:19.730
America doesn't represent
the world.

01:42:19.730 --> 01:42:22.270
&gt;&gt; NARRATOR: The U.S. government
fears that as Huawei supplies

01:42:22.270 --> 01:42:26.400
countries around the world
with 5G,

01:42:26.400 --> 01:42:28.670
the Chinese government could
have back-door access

01:42:28.670 --> 01:42:30.700
to their equipment.

01:42:30.700 --> 01:42:34.400
Recently, the C.E.O. promised
complete transparency

01:42:34.400 --> 01:42:36.700
into the company's software,

01:42:36.700 --> 01:42:39.470
but U.S. authorities
are not convinced.

01:42:39.470 --> 01:42:44.530
&gt;&gt; Nothing in China exists free
and clear of the party-state.

01:42:44.530 --> 01:42:48.730
Those companies can only exist
and prosper

01:42:48.730 --> 01:42:51.030
at the sufferance of the party.

01:42:51.030 --> 01:42:55.030
And it's made very explicit that
when the party needs them,

01:42:55.030 --> 01:42:58.900
they either have to respond
or they will be dethroned.

01:42:58.900 --> 01:43:03.770
So this is the challenge with a
company like Huawei.

01:43:03.770 --> 01:43:08.900
So Huawei, Ren Zhengfei, the
head of Huawei, he can say,

01:43:08.900 --> 01:43:12.000
"Well, we... we're just a
private company and we just...

01:43:12.000 --> 01:43:15.470
We don't take orders
from the Communist Party."

01:43:15.470 --> 01:43:18.370
Well, maybe they haven't yet.

01:43:18.370 --> 01:43:20.870
But what the Pentagon sees,

01:43:20.870 --> 01:43:23.100
the National Intelligence
Council sees,

01:43:23.100 --> 01:43:27.070
and what the FBI sees is,
"Well, maybe not yet."

01:43:27.070 --> 01:43:30.200
But when the call comes,

01:43:30.200 --> 01:43:35.430
everybody knows what the
company's response will be.

01:43:35.430 --> 01:43:37.000
&gt;&gt; NARRATOR: The U.S. Commerce
Department

01:43:37.000 --> 01:43:39.400
has recently blacklisted
eight companies

01:43:39.400 --> 01:43:42.870
for doing business with
government agencies in Xinjiang,

01:43:42.870 --> 01:43:45.370
claiming they are aiding
in the "repression"

01:43:45.370 --> 01:43:49.300
of the Muslim minority.

01:43:49.300 --> 01:43:52.270
Among the companies is Megvii.

01:43:52.270 --> 01:43:55.170
They have strongly objected
to the blacklist,

01:43:55.170 --> 01:43:57.630
saying that it's "a
misunderstanding of our company

01:43:57.630 --> 01:44:01.500
and our technology."

01:44:01.500 --> 01:44:04.430
♪ ♪

01:44:04.430 --> 01:44:07.370
President Xi has increased his
authoritarian grip

01:44:07.370 --> 01:44:11.070
on the country.

01:44:11.070 --> 01:44:14.530
In 2018, he had the Chinese
constitution changed

01:44:14.530 --> 01:44:20.070
so that he could be president
for life.

01:44:20.070 --> 01:44:21.370
&gt;&gt; If you had asked me
20 years ago,

01:44:21.370 --> 01:44:23.230
"What will happen to China?",
I would've said,

01:44:23.230 --> 01:44:27.170
"Well, over time, the Great
Firewall will break down.

01:44:27.170 --> 01:44:29.770
Of course, people will get
access to social media,

01:44:29.770 --> 01:44:31.800
they'll get access to Google...

01:44:31.800 --> 01:44:35.700
Eventually, it'll become a much
more democratic place,

01:44:35.700 --> 01:44:38.370
with free expression
and lots of Western values."

01:44:38.370 --> 01:44:41.870
And the last time I checked,
that has not happened.

01:44:41.870 --> 01:44:46.600
In fact, technology's become
a tool of control.

01:44:46.600 --> 01:44:48.570
And as China has gone through
this amazing period of growth

01:44:48.570 --> 01:44:51.870
and wealth and openness in
certain ways,

01:44:51.870 --> 01:44:53.430
there has not been the
democratic transformation

01:44:53.430 --> 01:44:55.330
that I thought.

01:44:55.330 --> 01:44:57.570
And it may turn out that,
in fact,

01:44:57.570 --> 01:45:00.600
technology is a better tool for
authoritarian governments

01:45:00.600 --> 01:45:02.570
than it is for democratic
governments.

01:45:02.570 --> 01:45:04.900
&gt;&gt; NARRATOR: To dominate
the world in A.I.,

01:45:04.900 --> 01:45:08.000
President Xi is depending on
Chinese tech

01:45:08.000 --> 01:45:11.330
to lead the way.

01:45:11.330 --> 01:45:13.030
While companies like
Baidu, Alibaba,

01:45:13.030 --> 01:45:17.870
and Tencent are growing more
powerful and competitive,

01:45:17.870 --> 01:45:20.500
they're also beginning to have
difficulty accessing

01:45:20.500 --> 01:45:24.930
American technology, and are
racing to develop their own.

01:45:27.600 --> 01:45:31.200
With a continuing trade war
and growing distrust,

01:45:31.200 --> 01:45:33.630
the longtime argument for
engagement

01:45:33.630 --> 01:45:38.230
between the two countries
has been losing ground.

01:45:38.230 --> 01:45:42.100
&gt;&gt; I've seen more and more
of my colleagues move

01:45:42.100 --> 01:45:44.270
from a position when they
thought,

01:45:44.270 --> 01:45:47.330
"Well, if we just keep engaging
China,

01:45:47.330 --> 01:45:50.800
the lines between
the two countries

01:45:50.800 --> 01:45:52.800
will slowly converge."

01:45:52.800 --> 01:45:56.570
You know, whether it's in
economics, technology, politics.

01:45:56.570 --> 01:45:58.370
And the transformation,

01:45:58.370 --> 01:46:01.230
where they now think
they're diverging.

01:46:01.230 --> 01:46:05.000
So, in other words, the whole
idea of engagement

01:46:05.000 --> 01:46:07.430
is coming under question.

01:46:07.430 --> 01:46:15.600
And that's cast an entirely
different light on technology,

01:46:15.600 --> 01:46:18.800
because if you're diverging and
you're heading into a world

01:46:18.800 --> 01:46:23.900
of antagonism-- you know,
conflict, possibly,

01:46:23.900 --> 01:46:25.930
then suddenly, technology is
something

01:46:25.930 --> 01:46:27.930
that you don't want to share.

01:46:27.930 --> 01:46:30.870
You want to sequester,

01:46:30.870 --> 01:46:34.300
to protect your own national
interest.

01:46:34.300 --> 01:46:38.130
And I think the tipping-point
moment we are at now,

01:46:38.130 --> 01:46:41.130
which is what is casting
the whole question of things

01:46:41.130 --> 01:46:45.130
like artificial intelligence
and technological innovation

01:46:45.130 --> 01:46:47.470
into a completely different
framework,

01:46:47.470 --> 01:46:51.700
is that if in fact China
and the U.S. are in some way

01:46:51.700 --> 01:46:54.730
fundamentally antagonistic
to each other,

01:46:54.730 --> 01:46:59.900
then we're in a completely
different world.

01:46:59.900 --> 01:47:05.670
&gt;&gt; NARRATOR: In the age of A.I.,
a new reality is emerging.

01:47:05.670 --> 01:47:07.600
That with so much accumulated
investment

01:47:07.600 --> 01:47:11.800
and intellectual power, the
world is already dominated

01:47:11.800 --> 01:47:16.200
by just two A.I. superpowers.

01:47:16.200 --> 01:47:22.130
That's the premise of a new book
written by Kai-Fu Lee.

01:47:22.130 --> 01:47:23.500
&gt;&gt; Hi, I'm Kai-Fu.

01:47:23.500 --> 01:47:25.600
&gt;&gt; Hi, Dr. Lee, so
nice to meet you.

01:47:25.600 --> 01:47:26.670
&gt;&gt; Really nice to meet you.

01:47:26.670 --> 01:47:28.230
Look at all these dog ears.

01:47:28.230 --> 01:47:30.000
I love, I love that.
&gt;&gt; You like that?

01:47:30.000 --> 01:47:31.970
&gt;&gt; But I... but I don't like you
didn't buy the book,

01:47:31.970 --> 01:47:33.470
you... you borrowed it.

01:47:33.470 --> 01:47:35.570
&gt;&gt; I couldn't find it!
&gt;&gt; Oh, really?

01:47:35.570 --> 01:47:36.900
&gt;&gt; Yeah!
&gt;&gt; And, and you...

01:47:36.900 --> 01:47:39.130
you're coming to my talk?
&gt;&gt; Of course!

01:47:39.130 --> 01:47:40.500
&gt;&gt; Oh, hi.
&gt;&gt; I did my homework,

01:47:40.500 --> 01:47:41.600
I'm telling you.

01:47:41.600 --> 01:47:42.600
&gt;&gt; Oh, my goodness, thank you.

01:47:42.600 --> 01:47:44.670
Laurie, can you get this
gentleman a book?

01:47:44.670 --> 01:47:46.430
(people talking in background)

01:47:46.430 --> 01:47:47.730
&gt;&gt; NARRATOR: In his book
and in life,

01:47:47.730 --> 01:47:50.830
the computer
scientist-cum-venture capitalist

01:47:50.830 --> 01:47:52.230
walks a careful path.

01:47:52.230 --> 01:47:56.370
Criticism of the Chinese
government is avoided,

01:47:56.370 --> 01:47:58.700
while capitalist success
is celebrated.

01:47:58.700 --> 01:48:00.800
&gt;&gt; I'm studying electrical
engineering.

01:48:00.800 --> 01:48:03.230
&gt;&gt; Sure, send me a resume.
&gt;&gt; Okay, thanks.

01:48:03.230 --> 01:48:06.230
&gt;&gt; NARRATOR: Now, with the rise
of the two superpowers,

01:48:06.230 --> 01:48:09.400
he wants to warn the world
of what's coming.

01:48:09.400 --> 01:48:11.600
&gt;&gt; Are you the new leaders?

01:48:11.600 --> 01:48:14.230
&gt;&gt; If we're not the new leaders,
we're pretty close.

01:48:14.230 --> 01:48:15.800
(laughs)

01:48:15.800 --> 01:48:18.030
Thank you very much.
&gt;&gt; Thanks.

01:48:18.030 --> 01:48:20.630
&gt;&gt; NARRATOR: "Never," he writes,
"has the potential

01:48:20.630 --> 01:48:22.800
for human flourishing been
higher

01:48:22.800 --> 01:48:26.230
or the stakes of failure
greater."

01:48:26.230 --> 01:48:27.700
♪ ♪

01:48:27.700 --> 01:48:32.070
&gt;&gt; So if one has to say who's
ahead, I would say today,

01:48:32.070 --> 01:48:34.670
China is quickly catching up.

01:48:34.670 --> 01:48:38.970
China actually began
its big push

01:48:38.970 --> 01:48:42.370
in A.I. only two-and-a-half
years ago,

01:48:42.370 --> 01:48:46.700
when the AlphaGo-Lee Sedol match
became the Sputnik moment.

01:48:46.700 --> 01:48:49.870
&gt;&gt; NARRATOR: He says he believes
that the two A.I. superpowers

01:48:49.870 --> 01:48:52.700
should lead the way and work
together

01:48:52.700 --> 01:48:55.270
to make A.I. a force for good.

01:48:55.270 --> 01:48:58.400
If we do, we may have a chance
of getting it right.

01:48:58.400 --> 01:49:00.730
&gt;&gt; If we do a very good job
in the next 20 years,

01:49:00.730 --> 01:49:04.100
A.I. will be viewed as an age of
enlightenment.

01:49:04.100 --> 01:49:08.370
Our children and their children
will see A.I. as serendipity.

01:49:08.370 --> 01:49:13.800
That A.I. is here to liberate us
from having to do routine jobs,

01:49:13.800 --> 01:49:15.830
and push us to do what we love,

01:49:15.830 --> 01:49:19.530
and push us to think what it
means to be human.

01:49:19.530 --> 01:49:23.600
&gt;&gt; NARRATOR: But what if humans
mishandle this new power?

01:49:23.600 --> 01:49:25.930
Kai-Fu Lee understands
the stakes.

01:49:25.930 --> 01:49:28.270
After all, he invested early
in Megvii,

01:49:28.270 --> 01:49:33.030
which is now on the U.S.
blacklist.

01:49:33.030 --> 01:49:35.630
He says he's reduced his stake
and doesn't speak

01:49:35.630 --> 01:49:38.070
for the company.

01:49:38.070 --> 01:49:40.370
Asked about the government
using A.I.

01:49:40.370 --> 01:49:44.570
for social control,
he chose his words carefully.

01:49:44.570 --> 01:49:49.630
&gt;&gt; Um... A.I. is a technology
that can be used

01:49:49.630 --> 01:49:52.230
for good and for evil.

01:49:52.230 --> 01:50:00.700
So how... how do governments
limit themselves in,

01:50:00.700 --> 01:50:04.670
on the one hand,
using this A.I. technology

01:50:04.670 --> 01:50:07.970
and the database to maintain
a safe environment

01:50:07.970 --> 01:50:11.400
for its citizens, but,
but not encroach

01:50:11.400 --> 01:50:14.370
on a individual's rights
and privacies?

01:50:14.370 --> 01:50:17.530
That, I think, is also a tricky
issue, I think,

01:50:17.530 --> 01:50:19.200
for, for every country.

01:50:19.200 --> 01:50:22.170
I think for... I think every
country will be tempted

01:50:22.170 --> 01:50:26.030
to use A.I. probably
beyond the limits

01:50:26.030 --> 01:50:29.930
to which that you and I would
like the government to use.

01:50:35.370 --> 01:50:40.970
♪ ♪

01:50:40.970 --> 01:50:43.030
&gt;&gt; NARRATOR: Emperor Yao devised
the game of Go

01:50:43.030 --> 01:50:48.770
to teach his son discipline,
concentration, and balance.

01:50:48.770 --> 01:50:52.630
Over 4,000 years later,
in the age of A.I.,

01:50:52.630 --> 01:50:56.230
those words still resonate with
one of its architects.

01:50:56.230 --> 01:50:58.330
♪ ♪

01:50:58.330 --> 01:51:02.270
&gt;&gt; So A.I. can be used in many
ways that are very beneficial

01:51:02.270 --> 01:51:03.700
for society.

01:51:03.700 --> 01:51:08.230
But the current use of A.I.
isn't necessarily aligned

01:51:08.230 --> 01:51:11.630
with the goals of building
a better society,

01:51:11.630 --> 01:51:12.900
unfortunately.

01:51:12.900 --> 01:51:16.570
But, but we could change that.

01:51:16.570 --> 01:51:19.570
&gt;&gt; NARRATOR: In 2016, a game of
Go gave us a glimpse

01:51:19.570 --> 01:51:24.970
of the future of artificial
intelligence.

01:51:24.970 --> 01:51:27.500
Since then, it has become clear
that we will need

01:51:27.500 --> 01:51:32.900
a careful strategy to harness
this new and awesome power.

01:51:35.800 --> 01:51:38.600
&gt;&gt; I, I do think that democracy
is threatened by the progress

01:51:38.600 --> 01:51:42.300
of these tools unless we improve
our social norms

01:51:42.300 --> 01:51:46.430
and we increase
the collective wisdom

01:51:46.430 --> 01:51:51.830
at the planet level to, to deal
with that increased power.

01:51:51.830 --> 01:51:57.600
I'm hoping that my concerns are
not founded,

01:51:57.600 --> 01:51:59.900
but the stakes are so high

01:51:59.900 --> 01:52:06.600
that I don't think we should
take these concerns lightly.

01:52:06.600 --> 01:52:11.930
I don't think we can play with
those possibilities and just...

01:52:11.930 --> 01:52:17.030
race ahead without thinking
about the potential outcomes.

01:52:17.030 --> 01:52:20.870
♪ ♪

01:52:27.330 --> 01:52:31.100
&gt;&gt; Go to pbs.org/frontline for
more of the impact

01:52:31.100 --> 01:52:32.870
of A.I. on jobs.

01:52:32.870 --> 01:52:37.730
&gt;&gt; I believe about fifty percent
of jobs will be somewhat

01:52:37.730 --> 01:52:41.230
or extremely threatened by A.I.
in the next 15 years or so.

01:52:41.230 --> 01:52:43.530
&gt;&gt; And a look at the potential
for racial bias

01:52:43.530 --> 01:52:45.200
in this technology.

01:52:45.200 --> 01:52:47.000
&gt;&gt; We've had issues with bias,
with discrimination,

01:52:47.000 --> 01:52:48.670
with poor system design,
with errors.

01:52:48.670 --> 01:52:51.500
&gt;&gt; Connect to the "Frontline"
community on Facebook

01:52:51.500 --> 01:52:54.570
and Twitter, and watch anytime
on the PBS Video app

01:52:54.570 --> 01:52:56.500
or pbs.org/frontline.

01:52:58.130 --> 01:53:02.070
♪ ♪

01:53:25.000 --> 01:53:26.800
&gt;&gt; For more on this and
other "Frontline" programs,

01:53:26.800 --> 01:53:30.100
visit our website
at pbs.org/frontline.

01:53:34.930 --> 01:53:37.400
♪ ♪

01:53:40.300 --> 01:53:43.530
To order "Frontline's"
"In the Age of A.I." on DVD,

01:53:43.530 --> 01:53:48.830
visit ShopPBS or call
1-800-PLAY-PBS.

01:53:48.830 --> 01:53:52.570
This program is also available
on Amazon Prime Video.

01:53:57.870 --> 01:54:01.170
♪ ♪

