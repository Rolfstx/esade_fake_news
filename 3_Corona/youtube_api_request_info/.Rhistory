#Load libraries
library(RCurl)
library(jsonlite)
library(tuber)
library(data.table)
#Import only unique video IDs
allids <- read.table("GitHub/esade_fake_news/3_Corona/youtube_recommendation_scrapper/data/csv/id_only_corona_s2_b7_d7.csv", header=TRUE)
#Import only unique video IDs
allids <- read.table("GitHub/esade_fake_news/3_Corona/youtube_recommendation_scrapper/data/csv/id_only_corona_s2_b7_d7.csv", header=TRUE)
#Import only unique video IDs
allids <- read.table("GitHub/esade_fake_news/3_Corona/youtube_recommendation_scrapper/data/csv/id_only_corona_s2_b7_d7.csv", header=TRUE)
#Import only unique video IDs
allids <- read.table("GitHub/esade_fake_news/3_Corona/youtube_recommendation_scrapper/data/csv/id_only_corona_s2_b7_d7.csv", header=TRUE)
#Import only unique video IDs
allids <- read.table("GitHub/esade_fake_news/3_Corona/youtube_recommendation_scrapper/data/csv/id_only_corona.csv", header=TRUE)
#Import only unique video IDs
allids <- read.table("C:/Users/asf/Documents/GitHub/esade_fake_news/3_Corona/youtube_recommendation_scrapper/data/csv/id_only_corona.csv", header=TRUE)
#Build a URL to call the API
URL_base='https://www.googleapis.com/youtube/v3/videos?id=' #this is the base URL
URL_details='&part=contentDetails&key='                     #getting contentDetail for technical metadata
URL_key='AIzaSyCJ3DxCeehOBhM5mCqK4gxJnFj3s4vebC8'
allids2 <- base::as.list(allids)
#Loop through URLS to retrieve basic info (duration, format)
alldata = data.frame()
ptm <- proc.time()                                          #Time responses to the server
# General info - Youtube API part: contentDetails
for(i in 1:nrow(allids)){
cat('Iteracio', i, '/', nrow(allids), '\n')
url = paste(URL_base, allids[i, ], URL_details, URL_key, sep = "")
dd <- getURL(url)
result <- fromJSON(dd)
id = result$items$id[[1]]
duration = result$items$contentDetails$duration
caption = result$items$contentDetails$caption
definition = result$items$contentDetails$definition
alldata = rbind(alldata, data.frame(id, duration, caption, definition))
}
#Key 2:
URL_key2='AIzaSyDSqoxxClx-HBj1WchUjFXReg_ei0aqo4I'
# Video info (title, description, etc.) - Youtube API part: snippet
alldata2 = data.frame()
URL_details2='&part=snippet&key='                     #getting snippet for general metadata
for(i in 1:nrow(allids)){
tryCatch({
cat('Iteracio', i, '/', nrow(allids), '\n')
url2 = paste(URL_base, allids[i, ], URL_details2, URL_key2, sep = "")
dd2 <- getURL(url2)
result2 <- fromJSON(dd2)
id2 = result2$items$id[[1]]
publishedAt = result2$items$snippet$publishedAt
channelid = result2$items$snippet$channelId
channeltitle = result2$items$snippet$channelTitle
title = result2$items$snippet$title
description = result2$items$snippet$description
tag = result2$items$snippet$tags
category = result2$items$snippet$categoryId
alldata2 = rbind(alldata2, data.frame(id2, title, description, publishedAt, channelid, channeltitle, category))
}, error=function(e){cat("Error at row:", i, "\n")}
)
}
#Key 2:
URL_key='AIzaSyDSqoxxClx-HBj1WchUjFXReg_ei0aqo4I'
# Video info (title, description, etc.) - Youtube API part: snippet
alldata2 = data.frame()
URL_details2='&part=snippet&key='                     #getting snippet for general metadata
for(i in 1:nrow(allids)){
tryCatch({
cat('Iteracio', i, '/', nrow(allids), '\n')
url2 = paste(URL_base, allids[i, ], URL_details2, URL_key, sep = "")
dd2 <- getURL(url2)
result2 <- fromJSON(dd2)
id2 = result2$items$id[[1]]
publishedAt = result2$items$snippet$publishedAt
channelid = result2$items$snippet$channelId
channeltitle = result2$items$snippet$channelTitle
title = result2$items$snippet$title
description = result2$items$snippet$description
tag = result2$items$snippet$tags
category = result2$items$snippet$categoryId
alldata2 = rbind(alldata2, data.frame(id2, title, description, publishedAt, channelid, channeltitle, category))
}, error=function(e){cat("Error at row:", i, "\n")}
)
}
#Key 2:
URL_key='AIzaSyB61wkkzvcAraGmnT1dRz0l-BN6ctDwX1c'
# Video info (title, description, etc.) - Youtube API part: snippet
alldata2 = data.frame()
URL_details2='&part=snippet&key='                     #getting snippet for general metadata
for(i in 1:nrow(allids)){
tryCatch({
cat('Iteracio', i, '/', nrow(allids), '\n')
url2 = paste(URL_base, allids[i, ], URL_details2, URL_key, sep = "")
dd2 <- getURL(url2)
result2 <- fromJSON(dd2)
id2 = result2$items$id[[1]]
publishedAt = result2$items$snippet$publishedAt
channelid = result2$items$snippet$channelId
channeltitle = result2$items$snippet$channelTitle
title = result2$items$snippet$title
description = result2$items$snippet$description
tag = result2$items$snippet$tags
category = result2$items$snippet$categoryId
alldata2 = rbind(alldata2, data.frame(id2, title, description, publishedAt, channelid, channeltitle, category))
}, error=function(e){cat("Error at row:", i, "\n")}
)
}
View(alldata2)
library(data.table)
library(dplyr)
#Import File:
data <- read.csv('~/GitHub/esade_fake_news/3_Corona/youtube_recommendation_scrapper/data/csv/covid_20200429_unique_search2_depth7_branch7.csv')
#write to csv in case something goes wrong.
write.csv(alldata, "GitHub/esade_fake_news/3_Corona/youtube_recommendation_scrapper/contentDetails_covid_v2.csv")
#write to csv in case something goes wrong.
write.csv(alldata, "~/GitHub/esade_fake_news/3_Corona/youtube_recommendation_scrapper/contentDetails_covid_v2.csv")
#Import File:
data <- read.csv('~/GitHub/esade_fake_news/3_Corona/youtube_recommendation_scrapper/data/csv/covid_20200429_unique_search2_depth7_branch7.csv')
View(data)
setDT(data)
unique_channel <- unique(data[,channel])
setDT(unique_channel)
data_filtered <- data[channel=unique_channel,]
unique_channel <- unique(data, by = 'channel')
View(unique_channel)
unique_channel2 <- unique_channel[!apply(unique_channel, 1, function(x) any(x=="")),]
View(unique_channel2)
