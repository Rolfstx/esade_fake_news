{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "collapsed": true,
    "id": "aFtC2PHN-rW6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/Rolf/anaconda3/lib/python3.7/runpy.py\", line 193, in _run_module_as_main\n",
      "    \"__main__\", mod_spec)\n",
      "  File \"/Users/Rolf/anaconda3/lib/python3.7/runpy.py\", line 85, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/Rolf/anaconda3/lib/python3.7/site-packages/pip/__main__.py\", line 23, in <module>\n",
      "    from pip._internal.cli.main import main as _main  # isort:skip # noqa\n",
      "  File \"/Users/Rolf/anaconda3/lib/python3.7/site-packages/pip/_internal/cli/main.py\", line 10, in <module>\n",
      "    from pip._internal.cli.autocompletion import autocomplete\n",
      "  File \"/Users/Rolf/anaconda3/lib/python3.7/site-packages/pip/_internal/cli/autocompletion.py\", line 9, in <module>\n",
      "    from pip._internal.cli.main_parser import create_main_parser\n",
      "  File \"/Users/Rolf/anaconda3/lib/python3.7/site-packages/pip/_internal/cli/main_parser.py\", line 7, in <module>\n",
      "    from pip._internal.cli import cmdoptions\n",
      "  File \"/Users/Rolf/anaconda3/lib/python3.7/site-packages/pip/_internal/cli/cmdoptions.py\", line 24, in <module>\n",
      "    from pip._internal.cli.progress_bars import BAR_TYPES\n",
      "  File \"/Users/Rolf/anaconda3/lib/python3.7/site-packages/pip/_internal/cli/progress_bars.py\", line 12, in <module>\n",
      "    from pip._internal.utils.logging import get_indentation\n",
      "  File \"/Users/Rolf/anaconda3/lib/python3.7/site-packages/pip/_internal/utils/logging.py\", line 18, in <module>\n",
      "    from pip._internal.utils.misc import ensure_dir\n",
      "  File \"/Users/Rolf/anaconda3/lib/python3.7/site-packages/pip/_internal/utils/misc.py\", line 20, in <module>\n",
      "    from pip._vendor import pkg_resources\n",
      "  File \"/Users/Rolf/anaconda3/lib/python3.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3251, in <module>\n",
      "    @_call_aside\n",
      "  File \"/Users/Rolf/anaconda3/lib/python3.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3235, in _call_aside\n",
      "    f(*args, **kwargs)\n",
      "  File \"/Users/Rolf/anaconda3/lib/python3.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 3264, in _initialize_master_working_set\n",
      "    working_set = WorkingSet._build_master()\n",
      "  File \"/Users/Rolf/anaconda3/lib/python3.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 574, in _build_master\n",
      "    ws = cls()\n",
      "  File \"/Users/Rolf/anaconda3/lib/python3.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 567, in __init__\n",
      "    self.add_entry(entry)\n",
      "  File \"/Users/Rolf/anaconda3/lib/python3.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 623, in add_entry\n",
      "    for dist in find_distributions(entry, True):\n",
      "  File \"/Users/Rolf/anaconda3/lib/python3.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2065, in find_on_path\n",
      "    for dist in factory(fullpath):\n",
      "  File \"/Users/Rolf/anaconda3/lib/python3.7/site-packages/pip/_vendor/pkg_resources/__init__.py\", line 2134, in distributions_from_metadata\n",
      "    yield Distribution.from_location(\n",
      "KeyboardInterrupt\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#library to extract subtitles\n",
    "#%pip install youtube_transcript_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 39782,
     "status": "ok",
     "timestamp": 1591989790220,
     "user": {
      "displayName": "Rolf Stirnimann",
      "photoUrl": "",
      "userId": "14933580794895575475"
     },
     "user_tz": -120
    },
    "id": "GA36Kd_O-3z7",
    "outputId": "f55dc4fc-eaf7-4693-f5e9-f1baf0bdf558"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-0f35642cec2f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#code needed to use colab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#if you're using juptyer notebook then skip this line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "#code needed to use colab\n",
    "#if you're using juptyer notebook then skip this line\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1814,
     "status": "ok",
     "timestamp": 1591989807751,
     "user": {
      "displayName": "Rolf Stirnimann",
      "photoUrl": "",
      "userId": "14933580794895575475"
     },
     "user_tz": -120
    },
    "id": "5OZBQ2GD_JKc",
    "outputId": "48aea90e-931f-4d63-b50b-d05a84a316fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Pierre/Documents/GitHub/esade_fake_news/1_Climate_Change/youtube_query/data/nlp\n",
      "00000000-000000_climate_change_bias.csv\r\n",
      "00000000-000000_climate_change_nlp.csv\r\n",
      "00000000-000000_climate_change_nlp_subs.csv\r\n",
      "00000000-000000_climate_change_nlp_subs_clean.csv\r\n",
      "00000000-000000_climate_change_nlp_subs_clean_expanded.csv\r\n",
      "20200507-033205_climate_change_bias.csv\r\n",
      "20200507-033205_climate_change_nlp.csv\r\n",
      "20200507-033205_climate_change_nlp_subs.csv\r\n",
      "20200507-033205_climate_change_nlp_subs2.csv\r\n",
      "20200507-033205_climate_change_nlp_subs_clean.csv\r\n",
      "20200507-033205_climate_change_nlp_subs_clean_expanded.csv\r\n",
      "\u001b[34mnotebooks\u001b[m\u001b[m\r\n"
     ]
    }
   ],
   "source": [
    "#cd used in colab\n",
    "#%cd \"gdrive/My Drive/Colab Notebooks\"\n",
    "%cd ~/Documents/GitHub/esade_fake_news/1_Climate_Change/youtube_query/data/nlp\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Yd5L-3b-ffc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 862,
     "status": "ok",
     "timestamp": 1591989810492,
     "user": {
      "displayName": "Rolf Stirnimann",
      "photoUrl": "",
      "userId": "14933580794895575475"
     },
     "user_tz": -120
    },
    "id": "53AFr164-ffi",
    "outputId": "0d03c492-e433-412c-c38b-18b09449b841"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bias_num</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>channel</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Watch the Latest News Headlines and Live Event...</td>\n",
       "      <td>#coronavirus #covid19 #news #LiveNews #Streami...</td>\n",
       "      <td>ABC News</td>\n",
       "      <td>w_Ma8oQLmSM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>An Inconvenient Border: Where China Meets Nort...</td>\n",
       "      <td>Bob Woodruffâ€™s daring 880-mile journey along t...</td>\n",
       "      <td>ABC News</td>\n",
       "      <td>8wRLQ8MVi0Q</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Bias_num                                              title  \\\n",
       "1       0.0  Watch the Latest News Headlines and Live Event...   \n",
       "2       0.0  An Inconvenient Border: Where China Meets Nort...   \n",
       "\n",
       "                                         description   channel           id  \n",
       "1  #coronavirus #covid19 #news #LiveNews #Streami...  ABC News  w_Ma8oQLmSM  \n",
       "2  Bob Woodruffâ€™s daring 880-mile journey along t...  ABC News  8wRLQ8MVi0Q  "
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import csv\n",
    "df = pd.read_csv('00000000-000000_climate_change_nlp.csv', index_col=0)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset is: 664\n",
      "Percentage of videos that are right: 18.54 %\n"
     ]
    }
   ],
   "source": [
    "#length of dataset\n",
    "print(f'Length of dataset is: {len(df)}')\n",
    "\n",
    "#percentage of videos that are right\n",
    "pr_right = round(df['Bias_num'].mean()*100,2)\n",
    "print(f'Percentage of videos that are right: {pr_right} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 402904,
     "status": "ok",
     "timestamp": 1591990216291,
     "user": {
      "displayName": "Rolf Stirnimann",
      "photoUrl": "",
      "userId": "14933580794895575475"
     },
     "user_tz": -120
    },
    "id": "xRA0-t5l-ffm",
    "outputId": "bb87540c-e1cf-447b-d323-666d3deea9d6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 309/664 [05:17<04:32,  1.30it/s]"
     ]
    }
   ],
   "source": [
    "#loop to extract subtitles\n",
    "\n",
    "tttt = []\n",
    "\n",
    "with tqdm(total=len(df['id'])) as pbar:  \n",
    "    for idx in df['id']:\n",
    "        try:\n",
    "            t = YouTubeTranscriptApi.get_transcript(idx)\n",
    "        except:\n",
    "            tttt.append([idx, np.nan]) #add NaN to videos that do not have subtitles\n",
    "            continue\n",
    "        tt = [i[\"text\"] for i in t]\n",
    "        ttt = [' '.join(tt)]\n",
    "        tttt.append([idx, ttt])\n",
    "        pbar.update(1)\n",
    "\n",
    "df_sub = pd.DataFrame(tttt, columns=['video_id', 'subtitles'])\n",
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge subtitles with dataframe containing video id, channel, etc. \n",
    "df2 = df.merge(df_sub, left_on='id', right_on='video_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i checked a few nan subtitles and they don't have subtitles. \n",
    "df2[df2['subtitles'].isna()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop column\n",
    "df2.drop('video_id', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export csv\n",
    "df2.to_csv('00000000-000000_climate_change_nlp_subs.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtitles cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train_cleaned.csv\n",
    "filename = '00000000-000000_climate_change_nlp_subs.csv'\n",
    "df = pd.read_csv(filename, usecols=['Bias_num', 'subtitles','channel'])\n",
    "\n",
    "#copy to not destroy original dataset\n",
    "df_channel = df.copy()\n",
    "\n",
    "#create new column text. easier in the analysis\n",
    "df['text'] = df['subtitles']\n",
    "\n",
    "#drop subtitles and channel\n",
    "df.drop(['subtitles'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#which videos have NaN as subtitle\n",
    "df_nan = df[df['text'].isna()]\n",
    "\n",
    "#number of videos with no subtitles\n",
    "num_nan = len(df_nan)\n",
    "print(f'Number of videos that do NOT have subtitles: {num_nan}')\n",
    "\n",
    "print()\n",
    "\n",
    "#displays channels with missing subtitles\n",
    "channel = df_nan.groupby([\"channel\"])['Bias_num'].count().sort_values(ascending=False)\n",
    "print(channel[:5])\n",
    "\n",
    "print()\n",
    "\n",
    "#display distribution of bias with missing subtitles\n",
    "bias = df_nan.groupby('Bias_num')['channel'].count()\n",
    "print(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop channel\n",
    "df.drop(['channel'], axis=1, inplace=True)\n",
    "\n",
    "#drop NaNs\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "#remove brackets [] and quotations marks\n",
    "df['text'] = df['text'].str[2:-2]\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#average right wing videos\n",
    "df['Bias_num'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split channel string to remove each element from subtitles\n",
    "\n",
    "#remove (in English) since the brackets cause problems with replacing values\n",
    "df_channel['channel'] = df_channel['channel'].replace(\"(in English)\", '', regex=True)\n",
    "\n",
    "#list all unique channels\n",
    "unique_channel = df_channel['channel'].unique()\n",
    "\n",
    "#split channel name by whitespace\n",
    "unique_channel_split = [i.split(' ') for i in unique_channel]\n",
    "\n",
    "#display first row\n",
    "unique_channel_split[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop to remove each single word in channel name\n",
    "#doing this to remove any possibility that the NLP model learns channel to predict bias\n",
    "for channel in unique_channel_split:\n",
    "    for element in channel:\n",
    "        df['text'].replace(element,\n",
    "                           '',\n",
    "                           inplace=True,\n",
    "                           regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the text [Music] appears in subtitles when music is playing. \n",
    "#these values mess up the analysis in the end with 3-gram, 2-gram\n",
    "to_replace = ['Music', 'music', 'Applause', 'applause', 'Laughter', 'laughter']\n",
    "\n",
    "df['text'].replace(to_replace, '', inplace=True, regex=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('00000000-000000_climate_change_nlp_subs_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split subtitles after 1'000 characters\n",
    "\n",
    "We split at 1'000 characters to make our dataset larger. I (Rolf) thinks that having more rows is more important for the model training than few but long rows. I think that 1'000 characters is a good cut-off. This is an arbitrary number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train_cleaned.csv\n",
    "filename = '00000000-000000_climate_change_nlp_subs_clean.csv'\n",
    "df_clean = pd.read_csv(filename, index_col=0)\n",
    "\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_clean has this many rows\n",
    "\n",
    "print('df_clean has this many rows: ' + str(len(df_clean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to split text into columns after 1'000 characters\n",
    "def chunks(s, n):\n",
    "    \"\"\"Produce `n`-character chunks from `s`.\"\"\"\n",
    "    for start in range(0, len(s), n):\n",
    "        yield s[start:start+n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty lists to keep info\n",
    "sub = []\n",
    "bias = []\n",
    "\n",
    "#loop to split at 1'000 characters\n",
    "for index, row in df.iterrows():\n",
    "    for chunk in chunks(row['text'], 1000):\n",
    "        sub.append(chunk)\n",
    "        bias.append(row['Bias_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a dataframe from sub and bias list\n",
    "df_sub = pd.DataFrame(list(zip(bias, sub)), columns=['bias_num', 'text'])\n",
    "\n",
    "length = len(df_sub)\n",
    "print(f\"Number of rows after splitting at 1'000 characters: {length}\")\n",
    "\n",
    "percentage = round(df_sub['bias_num'].mean()*100, 2)\n",
    "print(f\"Percentage of rows that are right wing: {percentage}%\")\n",
    "\n",
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop to calculate average length of text per row\n",
    "\n",
    "length = []\n",
    "\n",
    "for index, row in df_sub.iterrows():\n",
    "    length.append(len(row['text']))\n",
    "    \n",
    "avg_length = round(sum(length)/len(length),2)\n",
    "print(f\"Average character length of text column per row: {avg_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.to_csv('00000000-000000_climate_change_nlp_subs_clean_expanded.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Extract_subtitles.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
