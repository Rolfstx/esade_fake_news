{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aFtC2PHN-rW6",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting youtube_transcript_api\n",
      "  Downloading https://files.pythonhosted.org/packages/21/81/c4ae5534b113f4938b482f360babbbe6fda550441a4af8e1007dba518586/youtube_transcript_api-0.3.1-py3-none-any.whl\n",
      "Requirement already satisfied: requests in c:\\users\\atryf\\anaconda3\\lib\\site-packages (from youtube_transcript_api) (2.22.0)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in c:\\users\\atryf\\anaconda3\\lib\\site-packages (from requests->youtube_transcript_api) (2.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in c:\\users\\atryf\\anaconda3\\lib\\site-packages (from requests->youtube_transcript_api) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\atryf\\anaconda3\\lib\\site-packages (from requests->youtube_transcript_api) (2019.6.16)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in c:\\users\\atryf\\anaconda3\\lib\\site-packages (from requests->youtube_transcript_api) (1.24.2)\n",
      "Installing collected packages: youtube-transcript-api\n",
      "Successfully installed youtube-transcript-api-0.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#library to extract subtitles\n",
    "%pip install youtube_transcript_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 39782,
     "status": "ok",
     "timestamp": 1591989790220,
     "user": {
      "displayName": "Rolf Stirnimann",
      "photoUrl": "",
      "userId": "14933580794895575475"
     },
     "user_tz": -120
    },
    "id": "GA36Kd_O-3z7",
    "outputId": "f55dc4fc-eaf7-4693-f5e9-f1baf0bdf558",
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-164-4996ee3d8d09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "#code needed to use colab\n",
    "#if you're using juptyer notebook then skip this line\n",
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1814,
     "status": "ok",
     "timestamp": 1591989807751,
     "user": {
      "displayName": "Rolf Stirnimann",
      "photoUrl": "",
      "userId": "14933580794895575475"
     },
     "user_tz": -120
    },
    "id": "5OZBQ2GD_JKc",
    "outputId": "48aea90e-931f-4d63-b50b-d05a84a316fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/Rolf/Documents/GitHub/esade_fake_news/4_Politics/python3_script/data/nlp\n",
      "20200504-193926_joe_biden_bias.csv\r\n",
      "20200504-193926_joe_biden_nlp.csv\r\n",
      "20200504-193926_joe_biden_nlp_subs.csv\r\n",
      "20200504-193926_joe_biden_nlp_subs_clean.csv\r\n",
      "20200504-193926_joe_biden_nlp_subs_clean_expanded.csv\r\n",
      "\u001b[34mnotebooks\u001b[m\u001b[m\r\n",
      "\u001b[34mresults\u001b[m\u001b[m\r\n",
      "trigrams_joe_biden.png\r\n"
     ]
    }
   ],
   "source": [
    "#cd used in colab\n",
    "#%cd \"gdrive/My Drive/Colab Notebooks\"\n",
    "%cd ..\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Yd5L-3b-ffc"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Subtitles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 862,
     "status": "ok",
     "timestamp": 1591989810492,
     "user": {
      "displayName": "Rolf Stirnimann",
      "photoUrl": "",
      "userId": "14933580794895575475"
     },
     "user_tz": -120
    },
    "id": "53AFr164-ffi",
    "outputId": "0d03c492-e433-412c-c38b-18b09449b841"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bias_num</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>channel</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Elon Musk says Australia’s energy emergency is...</td>\n",
       "      <td>It’s been a miserable few weeks for Malcolm Tu...</td>\n",
       "      <td>60 Minutes Australia</td>\n",
       "      <td>w7BMaG3zyVo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Real life Catch Me If You Can con artist revea...</td>\n",
       "      <td>Catch Me If You Can stars Leonardo DiCaprio as...</td>\n",
       "      <td>60 Minutes Australia</td>\n",
       "      <td>3UmcxQto7UU</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Bias_num                                              title  \\\n",
       "1         0  Elon Musk says Australia’s energy emergency is...   \n",
       "2         0  Real life Catch Me If You Can con artist revea...   \n",
       "\n",
       "                                         description               channel  \\\n",
       "1  It’s been a miserable few weeks for Malcolm Tu...  60 Minutes Australia   \n",
       "2  Catch Me If You Can stars Leonardo DiCaprio as...  60 Minutes Australia   \n",
       "\n",
       "            id  \n",
       "1  w7BMaG3zyVo  \n",
       "2  3UmcxQto7UU  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import csv\n",
    "df = pd.read_csv('20200504-193926_joe_biden_nlp.csv', index_col=0)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of dataset is: 849\n",
      "Percentage of videos that are right: 32.51 %\n"
     ]
    }
   ],
   "source": [
    "#length of dataset\n",
    "print(f'Length of dataset is: {len(df)}')\n",
    "\n",
    "#percentage of videos that are right\n",
    "pr_right = round(df['Bias_num'].mean()*100,2)\n",
    "print(f'Percentage of videos that are right: {pr_right} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 402904,
     "status": "ok",
     "timestamp": 1591990216291,
     "user": {
      "displayName": "Rolf Stirnimann",
      "photoUrl": "",
      "userId": "14933580794895575475"
     },
     "user_tz": -120
    },
    "id": "xRA0-t5l-ffm",
    "outputId": "bb87540c-e1cf-447b-d323-666d3deea9d6"
   },
   "outputs": [],
   "source": [
    "#loop to extract subtitles\n",
    "\n",
    "tttt = []\n",
    "\n",
    "with tqdm(total=len(df['id'])) as pbar:  \n",
    "    for idx in df['id']:\n",
    "        try:\n",
    "            t = YouTubeTranscriptApi.get_transcript(idx)\n",
    "        except:\n",
    "            tttt.append([idx, np.nan]) #add NaN to videos that do not have subtitles\n",
    "            continue\n",
    "        tt = [i[\"text\"] for i in t]\n",
    "        ttt = [' '.join(tt)]\n",
    "        tttt.append([idx, ttt])\n",
    "        pbar.update(1)\n",
    "\n",
    "df_sub = pd.DataFrame(tttt, columns=['video_id', 'subtitles'])\n",
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#merge subtitles with dataframe containing video id, channel, etc. \n",
    "df2 = df.merge(df_sub, left_on='id', right_on='video_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i checked a few nan subtitles and they don't have subtitles. \n",
    "df2[df2['subtitles'].isna()].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop column\n",
    "df2.drop('video_id', inplace=True, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export csv\n",
    "df2.to_csv(\"20200504-193926_joe_biden_nlp_subs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtitles cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train_cleaned.csv\n",
    "filename = \"20200504-193926_joe_biden_nlp_subs.csv\"\n",
    "df = pd.read_csv(filename, usecols=['Bias_num', 'subtitles','channel'])\n",
    "\n",
    "#copy to not destroy original dataset\n",
    "df_channel = df.copy()\n",
    "\n",
    "#create new column text. easier in the analysis\n",
    "df['text'] = df['subtitles']\n",
    "\n",
    "#drop subtitles and channel\n",
    "df.drop(['subtitles'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of videos that do NOT have subtitles: 112\n",
      "\n",
      "channel\n",
      "CNN               15\n",
      "Reagan Library    14\n",
      "PBS NewsHour      12\n",
      "The Hill           7\n",
      "CBS News           7\n",
      "Name: Bias_num, dtype: int64\n",
      "\n",
      "Bias_num\n",
      "0    78\n",
      "1    34\n",
      "Name: channel, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#which videos have NaN as subtitle\n",
    "df_nan = df[df['text'].isna()]\n",
    "\n",
    "#number of videos with no subtitles\n",
    "num_nan = len(df_nan)\n",
    "print(f'Number of videos that do NOT have subtitles: {num_nan}')\n",
    "\n",
    "print()\n",
    "\n",
    "#displays channels with missing subtitles\n",
    "channel = df_nan.groupby([\"channel\"])['Bias_num'].count().sort_values(ascending=False)\n",
    "print(channel[:5])\n",
    "\n",
    "print()\n",
    "\n",
    "#display distribution of bias with missing subtitles\n",
    "bias = df_nan.groupby('Bias_num')['channel'].count()\n",
    "print(bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bias_num</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>who hasn't been shocked by a recent electricit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>you're one of the greatest con men of all time...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Bias_num                                               text\n",
       "0         0  who hasn't been shocked by a recent electricit...\n",
       "1         0  you're one of the greatest con men of all time..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#drop channel\n",
    "df.drop(['channel'], axis=1, inplace=True)\n",
    "\n",
    "#drop NaNs\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "#remove brackets [] and quotations marks\n",
    "df['text'] = df['text'].str[2:-2]\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32570659488559894"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#average right wing videos\n",
    "df['Bias_num'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['60', 'Minutes', 'Australia']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#split channel string to remove each element from subtitles\n",
    "\n",
    "#remove (in English) since the brackets cause problems with replacing values\n",
    "df_channel['channel'] = df_channel['channel'].replace(\"(in English)\", '', regex=True)\n",
    "\n",
    "#list all unique channels\n",
    "unique_channel = df_channel['channel'].unique()\n",
    "\n",
    "#split channel name by whitespace\n",
    "unique_channel_split = [i.split(' ') for i in unique_channel]\n",
    "\n",
    "#display first row\n",
    "unique_channel_split[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loop to remove each single word in channel name\n",
    "#doing this to remove any possibility that the NLP model learns channel to predict bias\n",
    "for channel in unique_channel_split:\n",
    "    for element in channel:\n",
    "        df['text'].replace(element,\n",
    "                           '',\n",
    "                           inplace=True,\n",
    "                           regex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#the text [Music] appears in subtitles when music is playing. \n",
    "#these values mess up the analysis in the end with 3-gram, 2-gram\n",
    "to_replace = ['Music', 'music', 'Applause', 'applause', 'Laughter', 'laughter', '♪', \"'\", \"\\n\", \"\\\\\\92\", \"\\\\\\96\", \n",
    "              \"\\\\\\\\\", 'AUDIENCE LAUGHING', 'AUDIENCE APPLAUDING', 'AUDIENCE GASPS', \"NARRATOR\"]\n",
    "\n",
    "df['text'].replace(to_replace, '', inplace=True, regex=True)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"20200504-193926_joe_biden_nlp_subs_clean.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split subtitles after 1'000 characters\n",
    "\n",
    "We split at 1'000 characters to make our dataset larger. I (Rolf) thinks that having more rows is more important for the model training than few but long rows. I think that 1'000 characters is a good cut-off. This is an arbitrary number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Bias_num</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>who hasnt been shocked by a recent electricity...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>youre one of the greatest con men of all time ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>if you thought the insults hold between North ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>bravery courage defiance heartbreak theyre not...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>Jeffrey Epstein was a billionaire   businessma...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Bias_num                                               text\n",
       "0         0  who hasnt been shocked by a recent electricity...\n",
       "1         0  youre one of the greatest con men of all time ...\n",
       "2         0  if you thought the insults hold between North ...\n",
       "3         0  bravery courage defiance heartbreak theyre not...\n",
       "5         0  Jeffrey Epstein was a billionaire   businessma..."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load train_cleaned.csv\n",
    "filename = \"20200504-193926_joe_biden_nlp_subs_clean.csv\"\n",
    "df_clean = pd.read_csv(filename, index_col=0)\n",
    "\n",
    "df_clean.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_clean has this many rows: 743\n"
     ]
    }
   ],
   "source": [
    "#df_clean has this many rows\n",
    "\n",
    "print('df_clean has this many rows: ' + str(len(df_clean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to split text into columns after 1'000 characters\n",
    "def chunks(s, n):\n",
    "    \"\"\"Produce `n`-character chunks from `s`.\"\"\"\n",
    "    for start in range(0, len(s), n):\n",
    "        yield s[start:start+n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#empty lists to keep info\n",
    "sub = []\n",
    "bias = []\n",
    "\n",
    "#loop to split at 1'000 characters\n",
    "for index, row in df.iterrows():\n",
    "    for chunk in chunks(row['text'], 1000):\n",
    "        sub.append(chunk)\n",
    "        bias.append(row['Bias_num'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows after splitting at 1'000 characters: 18179\n",
      "Percentage of rows that are right wing: 42.09%\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bias_num</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>who hasnt been shocked by a recent electricity...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ave it denote when it gets dark [] but keeping...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>e energy its Rayleigh you could actually expor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>eted headfirst into a national political brawl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>energy  he quickly realized there were plenty ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bias_num                                               text\n",
       "0         0  who hasnt been shocked by a recent electricity...\n",
       "1         0  ave it denote when it gets dark [] but keeping...\n",
       "2         0  e energy its Rayleigh you could actually expor...\n",
       "3         0  eted headfirst into a national political brawl...\n",
       "4         0  energy  he quickly realized there were plenty ..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create a dataframe from sub and bias list\n",
    "df_sub = pd.DataFrame(list(zip(bias, sub)), columns=['bias_num', 'text'])\n",
    "\n",
    "length = len(df_sub)\n",
    "print(f\"Number of rows after splitting at 1'000 characters: {length}\")\n",
    "\n",
    "percentage = round(df_sub['bias_num'].mean()*100, 2)\n",
    "print(f\"Percentage of rows that are right wing: {percentage}%\")\n",
    "\n",
    "df_sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average character length of text column per row: 979.15\n"
     ]
    }
   ],
   "source": [
    "#loop to calculate average length of text per row\n",
    "\n",
    "length = []\n",
    "\n",
    "for index, row in df_sub.iterrows():\n",
    "    length.append(len(row['text']))\n",
    "    \n",
    "avg_length = round(sum(length)/len(length),2)\n",
    "print(f\"Average character length of text column per row: {avg_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sub.to_csv(\"20200504-193926_joe_biden_nlp_subs_clean_expanded.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Extract_subtitles.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
